{"meta":{"title":"小余哥|猿无忧","subtitle":"","description":"小余哥个人博客，分享一些技术教程以及日常踩坑记录。","author":"小余哥|猿无忧","url":"https://xiaoyuge5201.github.io","root":"/"},"pages":[{"title":"categories","date":"2021-02-26T08:36:55.000Z","updated":"2021-02-26T08:36:55.000Z","comments":true,"path":"categories/index.html","permalink":"https://xiaoyuge5201.github.io/categories/index.html","excerpt":"","text":""},{"title":"书单","date":"2022-05-28T09:20:39.996Z","updated":"2022-05-28T09:20:39.996Z","comments":true,"path":"books/index.html","permalink":"https://xiaoyuge5201.github.io/books/index.html","excerpt":"","text":""},{"title":"404 Not Found：该页无法显示","date":"2022-03-27T14:06:03.328Z","updated":"2022-03-27T14:06:03.328Z","comments":true,"path":"/404.html","permalink":"https://xiaoyuge5201.github.io/404.html","excerpt":"","text":""},{"title":"个人项目","date":"2022-05-28T09:20:26.265Z","updated":"2022-05-28T09:20:26.265Z","comments":true,"path":"repository/index.html","permalink":"https://xiaoyuge5201.github.io/repository/index.html","excerpt":"","text":""},{"title":"简单的聊下自己","date":"2022-05-28T09:17:44.296Z","updated":"2022-05-28T09:17:44.296Z","comments":true,"path":"about/index.html","permalink":"https://xiaoyuge5201.github.io/about/index.html","excerpt":"","text":"现在的年纪还没有到写自传的时候，只能是在这里简简单单的聊下自己。 我呢，一个普普通通的程序猿。没错就是大家眼中那种熬夜猝死、脱发、肥胖、死宅、一个书包走天下的一类人。 平时无非就是加加班、打打游戏、看看博客、偶尔追剧，和大部分普通人也没多大区别； 长相呢，也就那样(请勿舔屏，谢谢) 和下面这四位以及屏幕前的你霸占江湖颜值榜多年 不过岁月是把杀猪刀，谁也逃不过；哎，不说了，都是泪！！！ 弄这个博客网呢，也就是在学习、工作中遇到了一些问题或者学习到一些东西，在这上面记录一下，当然博客都是一搜一大把，而且内容也大差不差的，这无可厚非； 暂时写到这里（其实也没啥写的），在下面贴一些鸡汤请大家喝，毕竟看到这里也累了，喝一碗再走。 1.你真正喜欢想要的，没有一样是可以轻易得到的。 2.愿我走过的苦难你不必经历，愿我已有的幸福你正在触及。 3.打你脸的时候别问我为什么，因为我给你糖的时候你从来不会说谢谢。 4.习惯这个东西很可怕，特别是你不得不面对改变的时候。 5.这个世界就是这样，总有一大群人和你一起欢笑，却只有一个人陪你黯然神伤。 6.喜欢就争取，得到就珍惜，错过就忘记。 7.不要总去顾及别人的目光，做好自己，青春无悔。"},{"title":"tags","date":"2021-02-26T08:36:55.000Z","updated":"2021-02-26T08:36:55.000Z","comments":true,"path":"tags/index.html","permalink":"https://xiaoyuge5201.github.io/tags/index.html","excerpt":"","text":""},{"title":"友链","date":"2022-05-28T09:21:27.578Z","updated":"2022-05-28T09:21:27.578Z","comments":true,"path":"links/index.html","permalink":"https://xiaoyuge5201.github.io/links/index.html","excerpt":"","text":""}],"posts":[{"title":"实现异步的8种方式","slug":"async","date":"2023-03-28T13:58:15.000Z","updated":"2023-03-28T13:58:15.000Z","comments":false,"path":"async/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/async/index.html","excerpt":"","text":"1. 前言 异步执行对于开发者来说并不陌生，在实际的开发过程中，很多场景多会使用到异步，相比同步执行，异步可以大大缩短请求链路耗时时间，比如：「发送短信、邮件、异步更新等」，这些都是典型的可以通过异步实现的场景 首先我们先看一个常见的用户下单的场景： 在同步操作中，我们执行到 「发送短信」 的时候，我们必须等待 「赠送积分」 这个操作彻底执行完才能执行，如果 「赠送积分」 这个动作执行时间较长，发送短信需要等待，这就是典型的同步场景。 实际上，发送短信和赠送积分没有任何的依赖关系，通过异步，我们可以实现赠送积分和发送短信这两个操作能够同时进行，比如 2. 异步的8种实现方式 线程Thread Future 异步框架CompletableFuture Spring注解@Async Spring ApplicationEvent事件 消息队列 第三方异步框架(比如 Hutool的ThreadUtil) Guava异步 3. 异步案例代码 3.1 线程异步 123456789101112131415/** * @author xiaoyuge */public class AsyncThread extends Thread&#123; @Override public void run() &#123; System.out.println(&quot;current thread name:&quot;+Thread.currentThread().getName()); &#125; public static void main(String[] args) &#123; AsyncThread asyncThread = new AsyncThread(); asyncThread.start(); &#125;&#125; 当然如果每次都创建一个Thread线程，频繁的创建、销毁，浪费系统资源，可以采用线程池： 1234567public class AsyncThreadPool &#123; public static void main(String[] args) &#123; ExecutorService executorService = Executors.newCachedThreadPool(); executorService.submit(() -&gt; System.out.println(&quot;执行业务逻辑......&quot;)); &#125;&#125; 可以将业务逻辑封装到Runnable或Callable中，交由线程池来执行。 3.2 Future异步 1234567891011121314151617181920212223242526/** * @author xiaoyuge */public class FutureDemo &#123; public String executeDemo() throws ExecutionException, InterruptedException &#123; ExecutorService executorService = Executors.newFixedThreadPool(1); Future&lt;String&gt; future = executorService.submit(() -&gt; &#123; System.out.println(&quot; --- task start --- &quot;); Thread.sleep(3000); System.out.println(&quot; --- task finish ---&quot;); return &quot;this is future execute final result!!!&quot;; &#125;); //这里需要返回值时会阻塞主线程 return future.get(); &#125; public static void main(String[] args) &#123; FutureDemo demo = new FutureDemo(); try &#123; System.out.println(demo.executeDemo()); &#125; catch (ExecutionException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果： 123 --- task start --- --- task finish ---this is future execute final result!!! Future的不足之处: 无法被动接收异步任务的计算结果： 虽然我们可以主动将异步任务提交给线程池中的线程来执行，但是待异步任务执行结束之后，主线程无法得到任务完成与否的通知，它需要通过get方法主动获取任务执行的结果 Future间彼此孤立： 有时某一个耗时很长的异步任务执行结束之后，你想利用它返回的结果再做进一步的运算，该运算也会是一个异步任务，两者之间的关系需要程序开发人员手动进行绑定赋予，Future并不能将其形成一个任务流（pipeline），每一个Future都是彼此之间都是孤立的，所以才有了后面的CompletableFuture，CompletableFuture就可以将多个Future串联起来形成任务流。 Future没有很好的错误处理机制： 截止目前，如果某个异步任务在执行发的过程中发生了异常，调用者无法被动感知，必须通过捕获get方法的异常才知晓异步任务执行是否出现了错误，从而在做进一步的判断处理 3.3 CompletableFuture实现异步 123456789101112131415161718192021222324252627/** * @author xiaoyuge */public class CompletableFutureDemo &#123; public static void thenRunAsync() throws ExecutionException, InterruptedException &#123; CompletableFuture&lt;Integer&gt; cf1 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; cf1 do something&quot;); return 1; &#125;); CompletableFuture&lt;Integer&gt; cf2 = CompletableFuture.supplyAsync(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot; cf2 do something&quot;); return 2; &#125;); System.out.println(&quot;cf1执行结果 -&gt;&quot; + cf1.get()); System.out.println(&quot;cf2执行结果 -&gt;&quot; + cf2.get()); &#125; public static void main(String[] args) &#123; try &#123; thenRunAsync(); &#125; catch (ExecutionException | InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果： 1234ForkJoinPool.commonPool-worker-1 cf1 do somethingForkJoinPool.commonPool-worker-1 cf2 do somethingcf1执行结果 -&gt;1cf2执行结果 -&gt;2 我们不需要显式使用ExecutorService，CompletableFuture 内部使用了ForkJoinPool来处理异步任务，如果在某些业务场景我们想自定义自己的异步线程池也是可以的。 3.4 Spring的@Async异步 自定义线程池 123456789101112131415161718192021222324@EnableAsync //异步调用@Async注解生效。@Configurationpublic class TaskPoolConfig &#123; @Bean(&quot;taskExcutor&quot;) public Executor takExecutor()&#123; //返回可用处理器的Java虚拟机的数量 int i = Runtime.getRuntime().availableProcessors(); System.out.println(&quot;系统最大线程数：&quot;+i); ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setCorePoolSize(16); //核心线程池大小 executor.setMaxPoolSize(20); //最大线程数 executor.setKeepAliveSeconds(60);//活跃时间 executor.setQueueCapacity(9999); //配置缓存队列容量，默认值为：Integer.MAX_VALUE executor.setThreadNamePrefix(&quot;async Excutor -&quot;);//线程名前缀 executor.setAwaitTerminationSeconds(60); //等待的时间 //调度器shutdown被调用时等待当前被调度的任务完成, 就是等待所有的任务结束后再关闭线程池 executor.setWaitForTasksToCompleteOnShutdown(true); // 线程池对拒绝任务（无线程可用）的处理策略，目前只支持AbortPolicy、CallerRunsPolicy；默认为后者 executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); executor.initialize(); return executor; &#125;&#125; AsyncService 12345678910111213141516171819202122232425262728293031323334public interface AsyncService &#123; MessageResult sendSms(String callPrefix, String mobile, String actionType, String content); MessageResult sendEmail(String email, String subject, String content);&#125;@Slf4j@Servicepublic class AsyncServiceImpl implements AsyncService &#123; @Autowired private IMessageHandler mesageHandler; @Override @Async(&quot;taskExecutor&quot;) public MessageResult sendSms(String callPrefix, String mobile, String actionType, String content) &#123; try &#123; Thread.sleep(1000); mesageHandler.sendSms(callPrefix, mobile, actionType, content); &#125; catch (Exception e) &#123; log.error(&quot;发送短信异常 -&gt; &quot;, e); &#125; &#125; @Override @Async(&quot;taskExecutor&quot;) public sendEmail(String email, String subject, String content) &#123; try &#123; Thread.sleep(1000); mesageHandler.sendsendEmail(email, subject, content); &#125; catch (Exception e) &#123; log.error(&quot;发送email异常 -&gt; &quot;, e); &#125; &#125;&#125; 在实际项目中，使用@Async调用线程池，推荐的方式是使用自定义线程池的模式，不推荐直接使用@Async实现异步 3.5 Spring ApplicationEvent事件实现异步 定义事件 123456789101112131415161718192021public class AsyncSendEmailEvent extends ApplicationEvent &#123; /** * 邮箱 **/ private String email; /** * 主题 **/ private String subject; /** * 内容 **/ private String content; /** * 接收者 **/ private String targetUserId; //------省略getter/setter方法&#125; 定义事件处理器 1234567891011121314151617181920@Slf4j@Componentpublic class AsyncSendEmailEventHandler implements ApplicationListener&lt;AsyncSendEmailEvent&gt; &#123; @Autowired private IMessageHandler messageHandler; @Async(&quot;taskExecutor&quot;) @Override public void onApplicationEvent(AsyncSendEmailEvent event) &#123; if (event == null) &#123; return; &#125; String email = event.getEmail(); String subject = event.getSubject(); String content = event.getContent(); String targetUserId = event.getTargetUserId(); messageHandler.sendsendEmailSms(email, subject, content, targerUserId); &#125;&#125; 另外，可能有些时候采用ApplicationEvent实现异步的使用，当程序出现异常错误的时候，需要考虑补偿机制，那么这时候可以结合Spring Retry重试来帮助我们避免这种异常造成数据不一致问题 3.6 消息队列 配置 123456789101112spring: rabbitmq: ####连接地址 host: 192.168.0.92 ####端口号 port: 5672 ####账号 username: xiaoyuge ####密码 password: 123456 ### 地址 virtual-host: /mytest1205 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Componentpublic class RabbitMQConfig &#123; /** * 定义交换机 */ private String EXCHANGE_SPRINGBOOT_NAME = &quot;springboot_topic_exchange&quot;; /** * 短信队列 */ private String FANOUT_SMS_QUEUE = &quot;springboot_topic_sms_queue&quot;; /** * 邮件队列 */ private String FANOUT_SMS_EMAIL = &quot;springboot_topic_email_queue&quot;; /** * 创建短信队列 */ @Bean public Queue smsQueue() &#123; return new Queue(FANOUT_SMS_QUEUE); &#125; /** * 创建邮件队列 */ @Bean public Queue emailQueue() &#123; return new Queue(FANOUT_SMS_EMAIL); &#125; /** * 创建交换机 * * @return */ @Bean public TopicExchange topicExchange() &#123; return new TopicExchange(EXCHANGE_SPRINGBOOT_NAME); &#125; /** * 定义短信队列绑定交换机 */ @Bean public Binding smsBindingExchange(Queue smsQueue, TopicExchange topicExchange) &#123; return BindingBuilder.bind(smsQueue).to(topicExchange).with(&quot;my.sms&quot;); &#125; /** * 定义邮件队列绑定交换机 */ @Bean public Binding emailBindingExchange(Queue emailQueue, TopicExchange topicExchange) &#123; return BindingBuilder.bind(emailQueue).to(topicExchange).with(&quot;my.#&quot;); &#125;&#125; 回调事件消息生产者 12345678910111213141516//&lt;dependency&gt;// &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;// &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;//&lt;/dependency&gt;@Slf4j@Componentpublic class CallbackProducer &#123; @Autowired private AmqpTemplate amqpTemplate; public void send(CallbackDTO callbackDTO, final long delay)&#123; log.info(&quot;生产者发送消息，callbackDTO,&#123;&#125;&quot;, callbackDTO); // 参数1:交换机名称 、参数2: 路由key 参数3:消息 amqpTemplate.convertAndSend(&quot;springboot_topic_exchange&quot;,&quot;my.time&quot;,&quot;this is a message&quot;); &#125;&#125; 回调事件消息消费者 123456789101112131415161718@Component@RabbitListener(queues = &quot;springboot_topic_email_queue&quot;)public class CallbackConsumer &#123; @RabbitHandler public void consumer(String mesg)&#123; System.out.println(&quot;邮件消费者接收到消息:&quot;+mesg); &#125;&#125;@Component@RabbitListener(queues = &quot;springboot_topic_sms_queue&quot;)class FanoutSmsConsumer &#123; @RabbitHandler public void consumer(String msg)&#123; System.out.println(&quot;短信消费者收到消息：&quot;+msg); &#125;&#125; 3.7 ThreadUtil异步工具类 ThreadUtil.execAsync 123456789101112131415161718192021@Slf4jpublic class ThreadUtils &#123; /** * &lt;dependency&gt; * &lt;groupId&gt;cn.hutool&lt;/groupId&gt; * &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; * &lt;version&gt;5.7.4&lt;/version&gt; * &lt;/dependency&gt; */ public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; ThreadUtil.execAsync(() -&gt; &#123; ThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current(); int number = threadLocalRandom.nextInt(20) + 1; System.out.println(number); &#125;); System.out.println(&quot;当前第：&quot; + i + &quot;个线程&quot;); &#125; System.out.println(&quot;task finish!&quot;); &#125;&#125; 输出结果： 1234567当前第：0个线程当前第：1个线程当前第：2个线程task finish!31110 ThreadUtil.execute 12345678910111213141516171819202122public static void main(String[] args) throws InterruptedException &#123; CountDownLatch countDownLatch = ThreadUtil.newCountDownLatch(5); for (int i = 0; i &lt; countDownLatch.getCount(); i++) &#123; ThreadUtil.execute(()-&gt;&#123; try &#123; Thread.sleep(2000); //休眠2秒 ThreadLocalRandom threadLocalRandom = ThreadLocalRandom.current(); int number = threadLocalRandom.nextInt(20) + 1; System.out.println(number); &#125;catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;); System.out.println(&quot;第&quot;+ i +&quot;个线程&quot; ); //调用线程计数器 - 1 countDownLatch.countDown(); &#125; //唤醒主线程 countDownLatch.await(); System.out.println(&quot; finish &quot;); &#125;&#125; 输出结果： 1234567第0个线程第1个线程第2个线程//这里等了2s18135 3.8 Guava异步 Guava的ListenableFuture顾名思义就是可以监听的Future，是对java原生Future的扩展增强。我们知道Future表示一个异步计算任务，当任务完成时可以得到计算结果。如果我们希望一旦计算完成就拿到结果展示给用户或者做另外的计算，就必须使用另一个线程不断的查询计算状态。这样做，代码复杂，而且效率低下。使用**「Guava ListenableFuture」**可以帮我们检测Future是否完成了，不需要再通过get()方法苦苦等待异步的计算结果，如果完成就自动调用回调函数，这样可以减少并发程序的复杂度 ListenableFuture是一个接口，它从jdk的Future接口继承，添加了void addListener(Runnable listener, Executor executor)方法。 看下如何使用ListenableFuture。首先需要定义ListenableFuture的实例: 123456789ListeningExecutorService executorService = MoreExecutors.listeningDecorator(Executors.newCachedThreadPool()); final ListenableFuture&lt;Integer&gt; listenableFuture = executorService.submit(new Callable&lt;Integer&gt;() &#123; @Override public Integer call() throws Exception &#123; log.info(&quot;callable execute...&quot;) TimeUnit.SECONDS.sleep(1); return 1; &#125; &#125;); 首先通过MoreExecutors类的静态方法listeningDecorator方法初始化一个ListeningExecutorService的方法，然后使用此实例的submit方法即可初始化ListenableFuture对象。 ListenableFuture要做的工作，在Callable接口的实现类中定义，这里只是休眠了1秒钟然后返回一个数字1，有了ListenableFuture实例，可以执行此Future并执行Future完成之后的回调函数。 12345678910111213Futures.addCallback(listenableFuture, new FutureCallback&lt;Integer&gt;() &#123; @Override public void onSuccess(Integer result) &#123; //成功执行... System.out.println(&quot;Get listenable future&#x27;s result with callback &quot; + result); &#125; @Override public void onFailure(Throwable t) &#123; //异常情况处理... t.printStackTrace(); &#125;&#125;); 本博文摘录自：austin流川枫的实现异步的8种方式","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"分布式系统 - 全局唯一ID实现方案","slug":"global-unique-id","date":"2023-03-26T06:29:57.000Z","updated":"2023-03-26T06:29:57.000Z","comments":false,"path":"global-unique-id/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/global-unique-id/index.html","excerpt":"","text":"常见的分布式ID生成方式，大致分类的话可以分为两类：一种是类DB型的，根据设置不同起始值和步长来实现趋势递增，需要考虑服务的容错性和可用性; 另一种是类snowflake型，这种就是将64位划分为不同的段，每段代表不同的涵义，基本就是时间戳、机器ID和序列数。这种方案就是需要考虑时钟回拨的问题以及做一些 buffer的缓冲设计提高性能。@pdai 1. 为什么需要全局唯一ID 传统的单体架构时，我们基本上是单库然后业务单表的结构，每个业务表的ID一般我们都是从1增，通过AUTO_INCREMENT=1设置自增起始值，但是在分布式服务架构模式下分库分表的设计，使得多个库或者多个表存储相同的业务数据，这种情况根据数据库的自增ID就会产生相同的ID，不能保证主键的唯一性。 2. UUID UUID(Universally Unique Identifier)，通用唯一识别码的缩写，UUID是由一组32为的16进制的数字组成的，所以理论上UUID的总数为：16^32 = 2^128，如果每纳秒产生1兆个UUID，要花完全部UUID需要100亿年。 生成的UUID是由8-4-4-4-12格式的数据组成，其中32个字符和4个连接符’-’,一般使用的时候会将连接符删除uuid.toString().replaceAll(&quot;-&quot;,&quot;&quot;)。 目前UUID创建方式有5个版本，每个版本的算法不同，应用范围也不同。 版本1-基于时间的UUID： 这个一般是通过当前时间，随机数，和本地Mac地址来计算出来，可以通过 org.apache.logging.log4j.core.util包中的 UuidUtil.getTimeBasedUuid()来使用或者其他包中工具。由于使用了MAC地址，因此能够确保唯一性，但是同时也暴露了MAC地址，私密性不够好 版本2-DCE安全的UUID： DCE（Distributed Computing Environment）安全的UUID和基于时间的UUID算法相同，但会把时间戳的前4位置换为POSIX的UID或GID。这个版本的UUID在实际中较少用到 版本3-基于名字的UUID（MD5）： 基于名字的UUID通过计算名字和名字空间的MD5散列值得到。这个版本的UUID保证了：相同名字空间中不同名字生成的UUID的唯一性；不同名字空间中的UUID的唯一性；相同名字空间中相同名字的UUID重复生成是相同的 版本4-随机UUID： 根据随机数，或者伪随机数生成UUID。这种UUID产生重复的概率是可以计算出来的，但是重复的可能性可以忽略不计，因此该版本也是被经常使用的版本。JDK中使用的就是这个版本 版本5-基于名字的UUID（SHA1）： 和基于名字的UUID算法类似，只是散列值计算使用SHA1（Secure Hash Algorithm 1）算法。 Java中JDK自带的UUID产生方式就是版本4根据随机数生成的UUID和版本3基于名字的UUID 1234567891011public static void main(String[] args) &#123; //获取一个版本4根据随机字节数组的UUID。 UUID uuid = UUID.randomUUID(); System.out.println(uuid.toString().replaceAll(&quot;-&quot;,&quot;&quot;)); //获取一个版本3(基于名称)根据指定的字节数组的UUID。 byte[] nbyte = &#123;10, 20, 30&#125;; UUID uuidFromBytes = UUID.nameUUIDFromBytes(nbyte); System.out.println(uuidFromBytes.toString().replaceAll(&quot;-&quot;,&quot;&quot;));&#125; 输出结果： 1259f51e7ea5ca453bbfaf2c1579f09f1d7f49b84d0bbc38e9a493718013baace6 虽然UUID生成方便，本地也没有网络消耗，但是使用起来有一些缺点： 不易于存储：UUID太长，16字节128位，通常以36长度的字符串表示，很多场景不适用 信息不安全：基于MAC地址生成UUID的算法可能会造成MAC地址泄露，暴露使用者的位置 对Mysql索引不利：如果作为数据库主键，在InnoDB引擎下，UUID的无序性可能会引起数据位置频繁变动，严重影响性能 3. 数据库生成 由于分布式数据库的起始自增值一样所以才会有冲突的情况发生，那么我们将分布式系统中数据库的同一个业务表的自增ID设计成不一样的起始值，然后设置固定的步长，步长的值即为分库的数量或分表的数量。 以MySQL举例，利用给字段设置auto_increment_increment和auto_increment_offset来保证ID自增。 auto_increment_offset：表示自增长字段从哪个数开始，他的取值范围是1 … 65535 auto_increment_increment：表示自增长字段每次递增的量，其默认值是1，取值范围是1 … 65535。 假设有三台机器，则DB1中order表的起始ID值为1，DB2中order表的起始值为2，DB3中order表的起始值为3，它们自增的步长都为3，则它们的ID生成范围如下图所示： 通过这种方式明显的优势就是依赖于数据库自身不需要其他资源，并且ID号单调自增，可以实现一些对ID有特殊要求的业务 但是缺点也很明显，首先它强依赖DB，当DB异常时整个系统不可用。虽然配置主从复制可以尽可能地增加可用性，但是数据一致性在特殊情况下难以保证。主从切换时的不一致可能会导致重复发号。还有就是ID发号性能瓶颈限制在单台MySQL的读写性能 4. 基于Redis Redis实现分布式唯一ID主要是通过INCR和INCRBY 这样的自增原子命令，由于Redis自身的单线程的特点所以能保证生成的ID可肯定是唯一有序的。 但是单机存在性能瓶颈，无法满足高并发的业务需求，所以可以采用集群的方式来实现。集群的方式又会涉及到和数据库集群同样的问题i，所以也需要设置分段和步长来实现。 为了避免长期自增后数字过大可以通过与当前时间戳组合使用，另外为了确保并发和业务多线程的问题可以采用Redis + LUA的方式进行编码保证安全。 Redis 实现分布式全局唯一ID，它的性能比较高，生成的数据是有序的，对排序业务有利，但是同样它依赖于redis，需要系统引进redis组件，增加了系统的配置复杂性。 当然现在Redis的使用性很普遍，所以如果其他业务已经引进了Redis集群，则可以资源利用考虑使用Redis来实现。 5. 雪花算法-Snowflake Snowflake雪花算法是由Twitter开源的分布式ID生成算法，以划分命名空间的方式将64-bit位分割成多个部分，每个部分代表不同的含义。而Java中64bit的整数是Long类型，所以Java中snowFlake算法生成的ID就是long来存储的。 一个SnowflakesID有64位： 第1位： Java中long的最高位是符号位，正数为0，负数是1，一般生成的ID都是正数，所以默认为0 第2-42位：时间戳，表示了自选定的时期以来的毫秒数 第43-52位：计算机ID，表示机器数，即2^10 = 1024台机器，防止冲突，如果我们对IDC（互联网数据中心）有需求，还可以将 10-bit 分 5-bit 给 IDC，分5-bit给工作机器 第53-64位：每台及其上生成ID的序列号，这允许在同一毫秒内创建多个Snowflake ID 这样的划分之后相当于在一毫秒一个数据中心的一台机器上可产生4096个有序的不重复的ID。但是我们 IDC 和机器数肯定不止一个，所以毫秒内能生成的有序ID数是翻倍的。 Snowflake 的Twitter官方原版是用Scala写的，对Scala语言有研究的同学可以去阅读下，以下是 Java 版本的写法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170package com.jajian.demo.distribute;/** * Twitter_Snowflake&lt;br&gt; * SnowFlake的结构如下(每部分用-分开):&lt;br&gt; * 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 &lt;br&gt; * 1位标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0&lt;br&gt; * 41位时间截(毫秒级)，注意，41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截) * 得到的值），这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的（如下下面程序IdWorker类的startTime属性）。41位的时间截，可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69&lt;br&gt; * 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId&lt;br&gt; * 12位序列，毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号&lt;br&gt; * 加起来刚好64位，为一个Long型。&lt;br&gt; * SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。 */public class SnowflakeDistributeId &#123; // ==============================Fields=========================================== /** * 开始时间截 (2015-01-01) */ private final long twepoch = 1420041600000L; /** * 机器id所占的位数 */ private final long workerIdBits = 5L; /** * 数据标识id所占的位数 */ private final long datacenterIdBits = 5L; /** * 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** * 支持的最大数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** * 序列在id中占的位数 */ private final long sequenceBits = 12L; /** * 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** * 数据标识id向左移17位(12+5) */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** * 时间截向左移22位(5+5+12) */ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** * 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** * 工作机器ID(0~31) */ private long workerId; /** * 数据中心ID(0~31) */ private long datacenterId; /** * 毫秒内序列(0~4095) */ private long sequence = 0L; /** * 上次生成ID的时间截 */ private long lastTimestamp = -1L; //==============================Constructors===================================== /** * 构造函数 * * @param workerId 工作ID (0~31) * @param datacenterId 数据中心ID (0~31) */ public SnowflakeDistributeId(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;worker Id can&#x27;t be greater than %d or less than 0&quot;, maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format(&quot;datacenter Id can&#x27;t be greater than %d or less than 0&quot;, maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; // ==============================Methods========================================== /** * 获得下一个ID (该方法是线程安全的) * * @return SnowflakeId */ public synchronized long nextId() &#123; long timestamp = timeGen(); //如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException( String.format(&quot;Clock moved backwards. Refusing to generate id for %d milliseconds&quot;, lastTimestamp - timestamp)); &#125; //如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; //毫秒内序列溢出 if (sequence == 0) &#123; //阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; //时间戳改变，毫秒内序列重置 else &#123; sequence = 0L; &#125; //上次生成ID的时间截 lastTimestamp = timestamp; //移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // | (datacenterId &lt;&lt; datacenterIdShift) // | (workerId &lt;&lt; workerIdShift) // | sequence; &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; /** * 返回以毫秒为单位的当前时间 * * @return 当前时间(毫秒) */ protected long timeGen() &#123; return System.currentTimeMillis(); &#125;&#125; 测试的代码如下： 12345678public static void main(String[] args) &#123; SnowflakeDistributeId idWorker = new SnowflakeDistributeId(0, 0); for (int i = 0; i &lt; 1000; i++) &#123; long id = idWorker.nextId();// System.out.println(Long.toBinaryString(id)); System.out.println(id); &#125;&#125; 雪花算法提供了一个很好的设计思想，雪花算法生成的ID是趋势递增，不依赖数据库等第三方系统，以服务的方式部署，稳定性更高，生成ID的性能也是非常高的，而且可以根据自身业务特性分配bit位，非常灵活。 但是雪花算法强依赖机器时钟，如果机器上时钟回拨，会导致发号重复或者服务会处于不可用状态。如果恰巧回退前生成过一些ID，而时间回退后，生成的ID就有可能重复。官方对于此并没有给出解决方案，而是简单的抛错处理，这样会造成在时间被追回之前的这段时间服务不可用。 6. 百度-UidGenerator 百度的 UidGenerator 是百度开源基于Java语言实现的唯一ID生成器，是在雪花算法 snowflake 的基础上做了一些改进。UidGenerator以组件形式工作在应用项目中, 支持自定义workerId位数和初始化策略，适用于docker等虚拟化环境下实例自动重启、漂移等场景。 在实现上，UidGenerator提供了两种生成唯一ID方式，分别是DefaultUidGenerator和CachedUidGenerator，官方建议如果有性能考虑的话使用CacheUidGeneraotr方式实现。 UidGenerator依赖是以划分命名空间的方式将64bit位分割成多个部分，只不过它的默认划分方式有别于雪花算法Snowflake。它默认是由1-28-22-13的格式进行划分的，可以根据不同的业务自己调整各个字段占用的位数。 第1位仍然占用1bit，其值始终是0。 第2开始的28位是时间戳，可以表示2^28个数，这里不再是以毫秒而是以秒作为单位。 第29开始的22位是workId（数据中心+工作机器，可以是其他组成方式），可表示 2^22 = 4194304个工作ID。 最后13bit构成自增序列 其中 workId（机器 id），最多可支持约420w次机器启动。内置实现为在启动时由数据库分配（表名为 WORKER_NODE），默认分配策略为用后即弃，后续可提供复用策略。 1234567891011ROP TABLE IF EXISTS WORKER_NODE;CREATE TABLE WORKER_NODE( ID BIGINT NOT NULL AUTO_INCREMENT COMMENT &#x27;auto increment id&#x27;, HOST_NAME VARCHAR(64) NOT NULL COMMENT &#x27;host name&#x27;, PORT VARCHAR(64) NOT NULL COMMENT &#x27;port&#x27;, TYPE INT NOT NULL COMMENT &#x27;node type: ACTUAL or CONTAINER&#x27;, LAUNCH_DATE DATE NOT NULL COMMENT &#x27;launch date&#x27;, MODIFIED TIMESTAMP NOT NULL COMMENT &#x27;modified time&#x27;, CREATED TIMESTAMP NOT NULL COMMENT &#x27;created time&#x27;, PRIMARY KEY(ID)) COMMENT=&#x27;DB WorkerID Assigner for UID Generator&#x27;,ENGINE = INNODB; 6.1 DefaultUidGenerator实现 DefaultUidGenerator就是正常的根据时间戳和机器位还有序列号的生成方式，和雪花算法很相似，对于时钟回拨也只是抛异常处理，仅有一些不同，如以秒位单位而不再是毫秒，且支持Docker等虚拟化环境。 123456789101112131415161718192021222324252627protected synchronized long nextId() &#123; long currentSecond = getCurrentSecond(); // Clock moved backwards, refuse to generate uid if (currentSecond &lt; lastSecond) &#123; long refusedSeconds = lastSecond - currentSecond; throw new UidGenerateException(&quot;Clock moved backwards. Refusing for %d seconds&quot;, refusedSeconds); &#125; // At the same second, increase sequence if (currentSecond == lastSecond) &#123; sequence = (sequence + 1) &amp; bitsAllocator.getMaxSequence(); // Exceed the max sequence, we wait the next second to generate uid if (sequence == 0) &#123; currentSecond = getNextSecond(lastSecond); &#125; // At the different second, sequence restart from zero &#125; else &#123; sequence = 0L; &#125; lastSecond = currentSecond; // Allocate bits for UID return bitsAllocator.allocate(currentSecond - epochSeconds, workerId, sequence);&#125; 如果你要使用 DefaultUidGenerator 的实现方式的话，以上划分的占用位数可通过 spring 进行参数配置。 123456789&lt;bean id=&quot;defaultUidGenerator&quot; class=&quot;com.baidu.fsg.uid.impl.DefaultUidGenerator&quot; lazy-init=&quot;false&quot;&gt; &lt;property name=&quot;workerIdAssigner&quot; ref=&quot;disposableWorkerIdAssigner&quot;/&gt; &lt;!-- Specified bits &amp; epoch as your demand. No specified the default value will be used --&gt; &lt;property name=&quot;timeBits&quot; value=&quot;29&quot;/&gt; &lt;property name=&quot;workerBits&quot; value=&quot;21&quot;/&gt; &lt;property name=&quot;seqBits&quot; value=&quot;13&quot;/&gt; &lt;property name=&quot;epochStr&quot; value=&quot;2016-09-20&quot;/&gt;&lt;/bean&gt; 6.2 CachedUidGenerator实现 官方建议的性能较高的 CachedUidGenerator 生成方式，是使用 RingBuffer 缓存生成的id。数组每个元素成为一个slot。RingBuffer容量，默认为Snowflake算法中sequence最大值（2^13 = 8192）。可通过 boostPower 配置进行扩容，以提高 RingBuffer 读写吞吐量。Tail指针、Cursor指针用于环形数组上读写slot Tail指针 表示Producer生产的最大序号(此序号从0开始，持续递增)。Tail不能超过Cursor，即生产者不能覆盖未消费的slot。当Tail已赶上curosr，此时可通过rejectedPutBufferHandler指定PutRejectPolicy Cursor指针 表示Consumer消费到的最小序号(序号序列与Producer序列相同)。Cursor不能超过Tail，即不能消费未生产的slot。当Cursor已赶上tail，此时可通过rejectedTakeBufferHandler指定TakeRejectPolicy CachedUidGenerator采用了双RingBuffer，Uid-RingBuffer用于存储Uid、Flag-RingBuffer用于存储Uid状态(是否可填充、是否可消费)。 由于数组元素在内存中是连续分配的，可最大程度利用CPU cache以提升性能。但同时会带来「伪共享」FalseSharing问题，为此在Tail、Cursor指针、Flag-RingBuffer中采用了CacheLine 补齐方式。 RingBuffer填充时机 初始化预填充: RingBuffer初始化时，预先填充满整个RingBuffer。 即时填充: Take消费时，即时检查剩余可用slot量(tail - cursor)，如小于设定阈值，则补全空闲slots。阈值可通过paddingFactor来进行配置，请参考Quick Start中CachedUidGenerator配置。 周期填充: 通过Schedule线程，定时补全空闲slots。可通过scheduleInterval配置，以应用定时填充功能，并指定Schedule时间间隔。# 7. 美团Leaf Leaf是美团基础研发平台推出的一个分布式ID生成服务，名字取自德国哲学家、数学家莱布尼茨的著名的一句话：“There are no two identical leaves in the world”，世间不可能存在两片相同的叶子。 Leaf 也提供了两种ID生成的方式，分别是 Leaf-segment 数据库方案和 Leaf-snowflake 方案。 7.1 Leaf-segment 数据库方案 Leaf-segment 数据库方案，是在上文描述的在使用数据库的方案上，做了如下改变： 原方案每次获取ID都得读写一次数据库，造成数据库压力大。改为利用proxy server批量获取，每次获取一个segment(step决定大小)号段的值。用完之后再去数据库获取新的号段，可以大大的减轻数据库的压力。 各个业务不同的发号需求用 biz_tag字段来区分，每个biz-tag的ID获取相互隔离，互不影响。如果以后有性能需求需要对数据库扩容，不需要上述描述的复杂的扩容操作，只需要对biz_tag分库分表就行。 数据库表设计如下： 12345678CREATE TABLE `leaf_alloc` ( `biz_tag` varchar(128) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;业务key&#x27;, `max_id` bigint(20) NOT NULL DEFAULT &#x27;1&#x27; COMMENT &#x27;当前已经分配了的最大id&#x27;, `step` int(11) NOT NULL COMMENT &#x27;初始步长，也是动态调整的最小步长&#x27;, `description` varchar(256) DEFAULT NULL COMMENT &#x27;业务key的描述&#x27;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, PRIMARY KEY (`biz_tag`)) ENGINE=InnoDB; 7.2 Leaf-snowflake方案 Leaf-snowflake方案完全沿用 snowflake 方案的bit位设计，对于workerID的分配引入了Zookeeper持久顺序节点的特性自动对snowflake节点配置 wokerID。避免了服务规模较大时，动手配置成本太高的问题。 Leaf-snowflake是按照下面几个步骤启动的： 启动Leaf-snowflake服务，连接Zookeeper，在leaf_forever父节点下检查自己是否已经注册过（是否有该顺序子节点）。 如果有注册过直接取回自己的workerID（zk顺序节点生成的int类型ID号），启动服务。 如果没有注册过，就在该父节点下面创建一个持久顺序节点，创建成功后取回顺序号当做自己的workerID号，启动服务。 为了减少对 Zookeeper的依赖性，会在本机文件系统上缓存一个workerID文件。当ZooKeeper出现问题，恰好机器出现问题需要重启时，能保证服务能够正常启动。 上文阐述过在类 snowflake算法上都存在时钟回拨的问题，Leaf-snowflake在解决时钟回拨的问题上是通过校验自身系统时间与 leaf_forever/${self}节点记录时间做比较然后启动报警的措施。 美团官方建议是由于强依赖时钟，对时间的要求比较敏感，在机器工作时NTP同步也会造成秒级别的回退，建议可以直接关闭NTP同步。要么在时钟回拨的时候直接不提供服务直接返回ERROR_CODE，等时钟追上即可。或者做一层重试，然后上报报警系统，更或者是发现有时钟回拨之后自动摘除本身节点并报警。 在性能上官方提供的数据目前 Leaf 的性能在4C8G 的机器上QPS能压测到近5w/s，TP999 1ms 8. Mist薄雾算法","categories":[{"name":"05 分布式","slug":"05-分布式","permalink":"https://xiaoyuge5201.github.io/categories/05-%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://xiaoyuge5201.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"分布式理论及一致性算法","slug":"distributed-01","date":"2023-03-26T03:51:24.000Z","updated":"2023-03-26T03:51:24.000Z","comments":false,"path":"distributed-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/distributed-01/index.html","excerpt":"","text":"什么是分布式系统 一个分布式系统就是一些独立的计算机集合，但是对这个系统的用户来说，系统就像一台计算机一样 分布式系统是一个硬件或软件组建分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统，简单来说就是一群独立计算机集合共同对外提供服务；分布式意味着可以采用更多的普通计算机组成分布式集群对外提供服务。 计算机越多，CPU、内存、存储资源等就越多，能够处理的并发访问量也就越大。 从分布式系统的概念中，可以知道各个主机之间通信和协调主要是通过网络进行的，所以分布式系统中的计算机在空间上没几乎没有限制，这些计算机可能被放在不同的机柜上，也可能在不同的机房或城市(地区/国家)。 分布式系统的主要特征 分布性 分布式系统中多台计算机之间在空间位置上可以随意分布，也可随时变动 对等性 分布式系统中的计算机没有主/从之分，即没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本(Replica)是分布式最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式，在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。 数据副本是指在不同节点上持久化同一份数据，当某一个节点上存储的数据丢失，可以从副本上读取该数据，这是解决分布式系统丢失问题最有效的方式。 另一类副本是服务副本，每个节点都有能力接收来自外部的请求并进行相应的处理 自治性 分布式系统中的各个节点都包含自己的处理机和内存，各自具有独立的处理数据的功能，通常，彼此在地位上平等的，无主次之分，既能自治地进行工作，又能利用共享的通信线路来传递消息，协调任务处理 并发性 在一个计算机网络中，程序运行过程的并发性操作是非常常见的行为。例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一 分布式系统面临的问题 缺乏全局时钟 在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。 机器宕机 机器宕机是最常见的异常之一。在大型集群中每日宕机发生的概率为千分之一左右，在实践中，一台宕机的机器恢复的时间通常认为是24 小时，一般需要人工介入重启机器 网络异常 消息丢失，两片节点之间彼此完全无法通信，即出现了“网络分化”；消息乱序，有一定的概率不是按照发送时的顺序依次到达目的节点，考虑使用序列号等机制处理网络消息的乱序问题，使得无效的、过期的网络消息不影响系统的正确性；数据错误；不可靠的TCP，TCP 协议为应用层提供了可靠的、面向连接的传输服务，但在分布式系统的协议设计中不能认为所有网络通信都基于TCP 协议则通信就是可靠的。TCP协议只能保证同一个TCP 链接内的网络消息不乱序，TCP 链接之间的网络消息顺序则无法保证。 分布式三态 如果某个节点向另一个节点发起RPC(Remote procedure call)调用，即某个节点A 向另一个节点B 发送一个消息，节点B 根据收到的消息内容完成某些操作，并将操作的结果通过另一个消息返回给节点A，那么这个RPC 执行的结果有三种状态：“成功”、“失败”、“超时（未知）”，称之为分布式系统的三态。 存储数据丢失 对于有状态节点来说，数据丢失意味着状态丢失，通常只能从其他节点读取、恢复存储的状态。 异常处理原则：被大量工程实践所检验过的异常处理黄金原则是：任何在设计阶段考虑到的异常情况一定会在系统实际运行中发生，但在系统实际运行遇到的异常却很有可能在设计时未能考虑，所以，除非需求指标允许，在系统设计时不能放过任何异常情况 衡量分布式系统的指标 性能 系统的吞吐能力，指系统在某一时间可以处理的数据总量，通常可以用系统每秒处理的总的数据量来衡量；系统的响应延迟，指系统完成某一功能需要使用的时间；系统的并发能力，指系统可以同时完成某一功能的能力，通常也用QPS(query per second)来衡量。上述三个性能指标往往会相互制约，追求高吞吐的系统，往往很难做到低延迟；系统平均响应时间较长时，也很难提高QPS 可用性 系统的可用性(availability)指系统在面对各种异常时可以正确提供服务的能力。系统的可用性可以用系统停服务的时间与正常服务的时间的比例来衡量，也可以用某功能的失败次数与成功次数的比例来衡量。可用性是分布式的重要指标，衡量了系统的鲁棒性，是系统容错能力的体现 可扩展性 系统的可扩展性(scalability)指分布式系统通过扩展集群机器规模提高系统性能（吞吐、延迟、并发）、存储容量、计算能力的特性。好的分布式系统总在追求“线性扩展性”，也就是使得系统的某一指标可以随着集群中的机器数量线性增长。 一致性 分布式系统为了提高可用性，总是不可避免的使用副本的机制，从而引发副本一致性的问题。越是强的一致的性模型，对于用户使用来说使用起来越简单 分布式理论基础 主要包括CAP理论和BASE理论 CAP理论 CAP理论是分布式系统（特别是分布是存储领域）中被讨论最多的理论，其中 C：一致性(Consistency) A：可用性(Availability) P：分区容错性(Partition tolerance) CAP理论告诉我们C、A、P三者不能同时满足，最多只能满足其中两个 BASE 理论 BASE是&quot;Basically Available, Soft state, Eventually consistent(基本可用、软状态、最终一致性)&quot;的首字母。其中的软状态和最终一致性这两种技巧擅于对付存在分区的场合，并因此提高类可用性 分布式一致性算法 一致性算法的目的是保证在分布式系统中，多数据副本节点数据一致性，主要包含一致性hash算法、Paxos算法、Raft算法、ZAB算法等 一致性Hash算法 一致性Hash算法是个经典算法，Hash环的引入是为了解决**单调性(Monotonicity)的问题，虚拟节点的引入是为了解决平衡性(Balance)**的问题 Paxos算法 Paxos算法是Lamport宗师提出的一种基于消息传递的分布式一致性算法 Raft算法 Paxos是出了名的难懂，而Raft正式为了探索一种更易于理解的一致性算法而产生的。它的设计目的就是易于理解，所以在选主的冲突处理等方式上它都选择了非常简单明了的解决方案 ZAB算法 ZAB 协议全称：Zookeeper Atomic Broadcast（Zookeeper 原子广播协议）, 它应该是所有一致性协议中生产环境中应用最多的了。为什么呢？因为他是为 Zookeeper 设计的分布式一致性协议！# 摘录 分布式系统 - 理论基础,理论及一致性算法","categories":[{"name":"05 分布式","slug":"05-分布式","permalink":"https://xiaoyuge5201.github.io/categories/05-%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://xiaoyuge5201.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"Mybatis执行一条SQL命令的过程","slug":"mybatis-sql-excute-process","date":"2023-03-25T13:07:32.000Z","updated":"2023-03-25T13:07:32.000Z","comments":false,"path":"mybatis-sql-excute-process/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mybatis-sql-excute-process/index.html","excerpt":"","text":"Mybatis中的sql命令，在枚举类SqlCommandType中定义的： 123public enum SqlCommandType &#123; UNKNOWN, INSERT, UPDATE, DELETE, SELECT, FLUSH;&#125; 下面以Mapper接口中一个方法为例子，看下sql命令的执行完整流程: 1234public interface StudentMapper &#123; //参数RowBounds和ResultSetHandler是可选参数，表示分页对象和自定义结果集处理器，一般不需要。 List&lt;Student&gt; findAllStudents(Map&lt;String, Object&gt; map, RowBounds rowBounds, ResultSetHandler rh); &#125; 一条完整的sql命令，其执行的完整流程如下图所示： MapperProxy的功能 因为Mapper接口不能直接实例化，MapperProxy的作用就是使用JDK动态代理功能，间接实例化Mapper的proxy对象 缓存MapperMethod对象 123456789101112131415161718192021222324private final Map&lt;Method, MapperMethod&gt; methodCache;@Overridepublic Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; if (Object.class.equals(method.getDeclaringClass())) &#123; try &#123; return method.invoke(this, args); &#125; catch (Throwable t) &#123; throw ExceptionUtil.unwrapThrowable(t); &#125; &#125; // 投鞭断流 final MapperMethod mapperMethod = cachedMapperMethod(method); return mapperMethod.execute(sqlSession, args);&#125; // 缓存MapperMethodprivate MapperMethod cachedMapperMethod(Method method) &#123; MapperMethod mapperMethod = methodCache.get(method); if (mapperMethod == null) &#123; mapperMethod = new MapperMethod(mapperInterface, method, sqlSession.getConfiguration()); methodCache.put(method, mapperMethod); &#125; return mapperMethod;&#125; MapperMethod的功能 解析Mapper接口的方法，并且封装成MapperMethod对象 将sql命令正确路由到恰当的SqlSession的方法上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class MapperMethod &#123; // 保存了Sql命令的类型和键id private final SqlCommand command; // 保存了Mapper接口方法的解析信息 private final MethodSignature method; public MapperMethod(Class&lt;?&gt; mapperInterface, Method method, Configuration config) &#123; this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, method); &#125; // 根据解析结果，路由到恰当的SqlSession方法上 public Object execute(SqlSession sqlSession, Object[] args) &#123; Object result; if (SqlCommandType.INSERT == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); &#125; else if (SqlCommandType.UPDATE == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); &#125; else if (SqlCommandType.DELETE == command.getType()) &#123; Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); &#125; else if (SqlCommandType.SELECT == command.getType()) &#123; if (method.returnsVoid() &amp;&amp; method.hasResultHandler()) &#123; executeWithResultHandler(sqlSession, args); result = null; &#125; else if (method.returnsMany()) &#123; result = executeForMany(sqlSession, args); &#125; else if (method.returnsMap()) &#123; result = executeForMap(sqlSession, args); &#125; else &#123; Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); &#125; &#125; else if (SqlCommandType.FLUSH == command.getType()) &#123; result = sqlSession.flushStatements(); &#125; else &#123; throw new BindingException(&quot;Unknown execution method for: &quot; + command.getName()); &#125; if (result == null &amp;&amp; method.getReturnType().isPrimitive() &amp;&amp; !method.returnsVoid()) &#123; throw new BindingException(&quot;Mapper method &#x27;&quot; + command.getName() + &quot; attempted to return null from a method with a primitive return type (&quot; + method.getReturnType() + &quot;).&quot;); &#125; return result; &#125; // ...","categories":[{"name":"04 Mybatis","slug":"04-Mybatis","permalink":"https://xiaoyuge5201.github.io/categories/04-Mybatis/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Mybatis 总体框架设计","slug":"mybatis-01","date":"2023-03-24T14:11:59.000Z","updated":"2023-03-24T14:11:59.000Z","comments":false,"path":"mybatis-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mybatis-01/index.html","excerpt":"","text":"Mybatis 架构概览 Mybatis框架整体设计如下： 接口层与数据库交互方式 Mybatis和数据库交互有两种方式： 使用传统的Mybatis提供的API 使用Mapper接口 使用传统的Mybatis提供的API: 这是传统的传递Statement Id和查询参数给SqlSession对象，使用SqlSession对象完成和数据库的交互；Mybatis提供了非常方便和简单的API，供用户实现对数据库的增删改查操作以及对数据库连接信息和Mybatis自身配置信息的维护操作 上述使用MyBatis 的方法，是创建一个和数据库打交道的SqlSession对象，然后根据Statement Id 和参数来操作数据库，这种方式固然很简单和实用，但是它不符合面向对象语言的概念和面向接口编程的编程习惯。由于面向接口的编程是面向对象的大趋势，MyBatis 为了适应这一趋势，增加了第二种使用MyBatis 支持接口（Interface）调用方式 使用Mapper接口: Mybatis将配置文件中的每一个&lt;Mapper&gt;节点抽象为一个Mapper接口，而这个接口中声明的方法跟&lt;Mapper&gt;节点中的&lt;select|update|delete|insert&gt; 节点项对应，即&lt;select|update|delete|insert&gt; 节点的id值为Mapper接口中的方法名称，parameterType 值表示Mapper 对应方法的入参类型，而resultMap 值则对应了Mapper 接口表示的返回值类型或者返回结果集的元素类型 根据MyBatis 的配置规范配置好后，通过SqlSession.getMapper(XXXMapper.class)方法，MyBatis 会根据相应的接口声明的方法信息，通过动态代理机制生成一个Mapper 实例，我们使用Mapper 接口的某一个方法时，MyBatis 会根据这个方法的方法名和参数类型，确定Statement Id，底层还是通过SqlSession.select(“statementId”,parameterObject);或者SqlSession.update(“statementId”,parameterObject); 等等来实现对数据库的操作， MyBatis 引用Mapper 接口这种调用方式，纯粹是为了满足面向接口编程的需要。（其实还有一个原因是在于，面向接口的编程，使得用户在接口上可以使用注解来配置SQL语句，这样就可以脱离XML配置文件，实现“0配置”）。 数据处理层 数据处理层可以说是MyBatis 的核心，从大的方面上讲，它要完成两个功能： 通过传入参数构建动态SQL语句； SQL语句的执行以及封装查询结果集成List 参数映射和动态SQL语句生成: 动态语句生成可以说是MyBatis框架非常优雅的一个设计，MyBatis 通过传入的参数值，使用 Ognl 来动态地构造SQL语句，使得MyBatis 有很强的灵活性和扩展性。 参数映射指的是对于java 数据类型和jdbc数据类型之间的转换：这里有包括两个过程：查询阶段，我们要将java类型的数据，转换成jdbc类型的数据，通过 preparedStatement.setXXX() 来设值；另一个就是对resultset查询结果集的jdbcType 数据转换成java 数据类型。# SQL语句的执行以及封装查询结果集成List: 动态SQL语句生成之后，MyBatis 将执行SQL语句，并将可能返回的结果集转换成List 列表。MyBatis 在对结果集的处理中，支持结果集关系一对多和多对一的转换，并且有两种支持方式，一种为嵌套查询语句的查询，还有一种是嵌套结果集的查询。# 框架支撑层 事务管理机制 事务管理机制对于ORM框架而言是不可缺少的一部分，事务管理机制的质量也是考量一个ORM框架是否优秀的一个标准 连接吃管理机制 由于创建一个数据库连接所占用的资源比较大， 对于数据吞吐量大和访问量非常大的应用而言，连接池的设计就显得非常重要。 缓存机制 为了提高数据利用率和减小服务器和数据库的压力，MyBatis 会对于一些查询提供会话级别的数据缓存，会将对某一次查询，放置到SqlSession 中，在允许的时间间隔内，对于完全相同的查询，MyBatis 会直接将缓存结果返回给用户，而不用再到数据库中查找。 SQL语句的配置方式 传统的MyBatis 配置SQL 语句方式就是使用XML文件进行配置的，但是这种方式不能很好地支持面向接口编程的理念，为了支持面向接口的编程，MyBatis 引入了Mapper接口的概念，面向接口的引入，对使用注解来配置SQL 语句成为可能，用户只需要在接口上添加必要的注解即可，不用再去配置XML文件了，但是，目前的MyBatis 只是对注解配置SQL 语句提供了有限的支持，某些高级功能还是要依赖XML配置文件配置SQL 语句。# 引导层 引导层是配置和启动MyBatis配置信息的方式。MyBatis 提供两种方式来引导MyBatis ：基于XML配置文件的方式和基于Java API 的方式。 主要构建及其相互关系 从MyBatis代码实现的角度来看，主体构件和关系如下： 主要的核心部件解释如下： SqlSession 作为MyBatis工作的主要顶层API，表示和数据库交互的会话，完成必要数据库增删改查功能 Executor MyBatis执行器，是MyBatis 调度的核心，负责SQL语句的生成和查询缓存的维护 StatementHandler 封装了JDBC Statement操作，负责对JDBC statement 的操作，如设置参数、将Statement结果集转换成List集合。 ParameterHandler 负责对用户传递的参数转换成JDBC Statement 所需要的参数， ResultSetHandler 负责将JDBC返回的ResultSet结果集对象转换成List类型的集合； TypeHandler 负责java数据类型和jdbc数据类型之间的映射和转换 MappedStatement MappedStatement维护了一条&lt;select|update|delete|insert&gt;节点的封装 SqlSource 负责根据用户传递的parameterObject，动态地生成SQL语句，将信息封装到BoundSql对象中，并返回 BoundSql 表示动态生成的SQL语句以及相应的参数信息 Configuration MyBatis所有的配置信息都维持在Configuration对象之中。","categories":[{"name":"04 Mybatis","slug":"04-Mybatis","permalink":"https://xiaoyuge5201.github.io/categories/04-Mybatis/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Springboot集成Selenium","slug":"selenium","date":"2023-03-23T00:59:15.000Z","updated":"2023-03-23T00:59:15.000Z","comments":false,"path":"selenium/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/selenium/index.html","excerpt":"","text":"1. 介绍Selenium Selenium是一个用于Web应用程序测试的工具。Selenium测试直接运行在浏览器中，就像真正的用户在操作一样。支持的浏览器包括IE（7, 8, 9, 10, 11），Mozilla Firefox，Safari，Google Chrome，Opera，Edge等。这个工具的主要功能包括：测试与浏览器的兼容性——测试应用程序看是否能够很好得工作在不同浏览器和操作系统之上。测试系统功能——创建回归测试检验软件功能和用户需求。支持自动录制动作和自动生成.Net、Java、Perl等不同语言的测试脚本。 2. 简单Demo 下载驱动chromedriver request12-- 根据自己chrome的版本并下载相对应的驱动http://chromedriver.storage.googleapis.com/index.html 配置chromedriver Window配置：下载好后解压放到你喜欢的位置，放到D:\\Program Files\\ChromeDriver文件夹下，记好这个路径配置要用到。接着右键我的电脑==&gt;属性==&gt;高级系统设置==&gt;环境变量==&gt;选中系统变量中的Path，点击编辑，点击新建，把前面提到的文件路径添加进去，点击确定至窗口关闭。 Mac: 使用终端打开即可。 引入依赖，依赖包下载地址 12345678910111213141516171819&lt;dependency&gt; &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt; &lt;artifactId&gt;selenium-java&lt;/artifactId&gt; &lt;version&gt;4.8.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 用到了@WithMockUser注解 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.security&lt;/groupId&gt; &lt;artifactId&gt;spring-security-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; &lt;!-- 解决报错：com.google.common.util.concurrent.SimpleTimeLimiter.create(Ljava/util/concurrent/ExecutorService;)Lcom/google/common/util/concurrent/SimpleTimeLimiter; --&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;31.1-jre&lt;/version&gt;&lt;/dependency&gt; 编写测试类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@RunWith(SpringRunner.class)//设置测试端口号@SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.DEFINED_PORT, properties = &#123;&quot;server.port=1000&quot;&#125;)public class SeleniumTest &#123; @Autowired private TestRestTemplate restRestTemplate; private static ChromeDriver driver; @BeforeClass public static void openBrowser() &#123; //初始化参数，chromeDriver驱动包的路径 System.setProperty(&quot;webdriver.chrome.driver&quot;, &quot;/Users/xiaoyuge/Desktop/browser/chromedriver&quot;); driver = new ChromeDriver(); // 最大化浏览器 driver.manage().window().maximize(); //加载URL driver.get(&quot;http://localhost:8082/mpg/login/auth1&quot;); //等待加载完成 driver.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS); &#125; @AfterClass public static void closeBrowser() &#123; driver.quit(); &#125; @Test public void loginTest() &#123; //获取页面元素 WebElement username = driver.findElementById(&quot;username&quot;); WebElement password = driver.findElementById(&quot;password&quot;); WebElement submit = driver.findElementByClassName(&quot;btn&quot;); username.sendKeys(&quot;admin&quot;); password.sendKeys(&quot;admin1&quot;); //提交表单 submit.click(); //获取cookies Set&lt;org.openqa.selenium.Cookie&gt; cookies = driver.manage().getCookies(); System.out.println(&quot;Size: &quot; + cookies.size()); Iterator&lt;Cookie&gt; itr = cookies.iterator(); CookieStore cookieStore = new BasicCookieStore(); while (itr.hasNext()) &#123; Cookie cookie = itr.next(); BasicClientCookie basicClientCo = new BasicClientCookie(cookie.getName(), cookie.getValue()); basicClientCo.setDomain(cookie.getDomain()); basicClientCo.setPath(cookie.getPath()); cookieStore.addCookie(basicClientCo); &#125; //如此便能拿到登录后的cookie，后续需要访问该网站其他网页，只需将拿到的cookie放到请求中“骗过”服务器即可 &#125; @Test @WithMockUser(username = &quot;admin&quot;, password = &quot;admin1&quot;) public void getData() &#123; DataSources dataSources = restRestTemplate.getForObject(&quot;&quot;, DataSources.class); Assert.assertEquals(&quot;22&quot;, dataSources.getName()); &#125;&#125;","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"ContiPerf接口性能测试","slug":"contiperf","date":"2023-03-22T08:17:47.000Z","updated":"2023-03-22T08:17:47.000Z","comments":false,"path":"contiperf/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/contiperf/index.html","excerpt":"","text":"1. 简介 ContiPerf是一个轻量级的测试工具，基于JUnit4开发，可用于接口级的性能测试，可以设置线程数和执行次数，通过限制最大时间和平均执行时间来进行效率测试。 2. 使用方法 添加依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.databene&lt;/groupId&gt; &lt;artifactId&gt;contiperf&lt;/artifactId&gt; &lt;version&gt;2.3.4&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 创建Domain 123456789101112131415161718@Entity@Table(name = &quot;g_data_sources&quot;)@org.hibernate.annotations.Table(appliesTo = &quot;g_data_sources&quot;, comment = &quot;数据源信息表&quot;)public class DataSources extends BasePO &#123; @Column(name = &quot;name&quot;, columnDefinition = &quot;varchar(30) NOT NULL comment &#x27;数据库名&#x27;&quot;) private String name; @Column(name = &quot;url&quot;, columnDefinition = &quot;varchar(100) NOT NULL comment &#x27;数据库ip&#x27;&quot;) private String url; @Column(name = &quot;username&quot;, columnDefinition = &quot;varchar(30) NOT NULL comment &#x27;数据库连接用户名&#x27;&quot;) private String username; @Column(name = &quot;pwd&quot;, columnDefinition = &quot;varchar(30) default &#x27;&#x27; comment &#x27;数据库连接密码&#x27;&quot;) private String pwd; //-----------------省略getter/setter方法 ---------------&#125; 创建查询代码 123456789101112131415@Repository(&quot;dataSourcesDaoImpl&quot;)@Transactional(propagation = Propagation.REQUIRED, rollbackFor = Exception.class)public class DataSourcesDaoImpl &#123; @PersistenceContext private EntityManager entityManager; public DataSources findById(Integer id) &#123; List&lt;DataSources&gt; result = entityManager.createQuery(&quot;from DataSources e where e.id = &quot; + id).getResultList(); if (result != null &amp;&amp; !result.isEmpty()) &#123; return result.get(0); &#125; return null; &#125;&#125; 创建测试类 123456789101112131415161718192021@Slf4j@SpringBootTest@RunWith(SpringRunner.class)public class MybatisGeneratorApplicationTest &#123; @Autowired private DataSourcesDaoImpl dataSourcesDao; @Rule public ContiPerfRule rule = new ContiPerfRule(); //重点 @Test //10个线程 执行100次 @PerfTest(invocations = 100, threads = 10) //重点 //指定每次执行的最长时间/平均时间/总时间 // @Required(max = 1200, average = 250, totalTime = 60000) public void test() &#123; int id = (int) (Math.random() * 60); dataSourcesDao.findById(id); &#125;&#125; JUnit执行完毕，会在target/contiperf-report中有相关的执行结果 可以使用浏览器打开查看结果 Measured invocations: 请求次数 Thread Count: 线程数 Execution time：总执行时间 Throughput: 吞吐量，每秒效率 TPS Min. latency: 最短响应时间 Average latency: 平均响应时间 Median: TP50响应时间 90%: TP90响应时间，指在一个时间段内（如5分钟），统计该方法每次调用所消耗的时间，并将这些时间按从小到大的顺序进行排序，取第90%的那个值作为TP90 值；配置此监控指标对应的报警阀值后，需要保证在这个时间段内该方法所有调用的消耗时间至少有90%的值要小于此阀值，否则系统将会报警 Max latency: 最长响应时间 报告图片显示不出来，是源码中使用了google图表，需要在线！！并且源码中写死了cht=lxy，可以参考ContiPerf html报告 3. @PerfTest参数说明 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455@Documented@Target(&#123;ElementType.METHOD, ElementType.TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)public @interface PerfTest &#123; //常用的就是这个参数 执行次数 与线程无关 int invocations() default 1; /** * 间隔时间 可以暂时不用 因为性能测试主要是测试并发 * @PerfTest(invocations = 300, threads = 2, duration = 100)，如果执行方法300次的时候执行时间还没到100ms，则继续执行到满足执行时间等于100ms，如果执行到50次的时候已经100ms了，则会继续执行之100次。 * The number of milliseconds to run and repeat the test with the full number of configured threads - * use this alternatively to &#123;@link #invocations()&#125;. When using a &#123;@link #rampUp()&#125;, the ramp-up times * add to the duration. * @see #duration() */ int duration() default -1; /** * 线程数 * The number of threads which concurrently invoke the test. The default value is 1. */ int threads() default 1; /** * The number of milliseconds to wait before each thread is added to the currently active threads. * On &#123;@link #duration()&#125;-based tests, the total ramp-up time of rampUp * (threads - 1) is added to the * configured duration. */ int rampUp() default 0; /** * The number of milliseconds to wait before the actual measurement and requirements monitoring is activated. * Use this to exclude ramp-up times from measurement or wait some minutes before dynamic optimizations are * applied (like code optimization or cache population). */ int warmUp() default 0; /** * Set this to true, if execution should stop with a failure message as soon as a configured &#123;@link Required#max()&#125; * value is violated. Set it to false, if you are interested in performing a full measurement to get percentiles, * throughput and more. The default value is false. */ boolean cancelOnViolation() default false; /** * The class of a &#123;@link WaitTimer&#125; implementation by which a wait time can be incurred between test invocations */ Class&lt;? extends WaitTimer&gt; timer() default None.class; /** * The parameters to initialize the &#123;@link WaitTimer&#125;. * The meaning of the values is individual for the WaitTimer implementation. */ double[] timerParams() default &#123;&#125;; /** * One ore more &#123;@link Clock&#125; classes to use for time measurement. * The first one specified is the one relevant for requirements verification. */ Class&lt;? extends Clock&gt;[] clocks() default &#123;&#125;;&#125; 4. @Required参数说明 参数 说明 @Required(throughput = 20) 要求每秒至少执行20个测试 @Required(average = 50) 要求平均执行时间不超过50ms @Required(median = 45) 要求所有执行的50%不超过45ms @Required(max = 2000) 要求没有测试超过2s @Required(totalTime = 5000) 要求总的执行时间不超过5s @Required(percentile90 = 3000) 要求90%的测试不超过3s @Required(percentile95 = 5000) 要求95%的测试不超过5s @Required(percentile99 = 10000) 要求99%的测试不超过10s","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Springboot单元测试获取Resources文件的8个姿势","slug":"get-resources-file","date":"2023-03-22T03:14:46.000Z","updated":"2023-03-22T03:14:46.000Z","comments":false,"path":"get-resources-file/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/get-resources-file/index.html","excerpt":"","text":"引入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 测试代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143@RunWith(SpringRunner.class)@SpringBootTest(classes = &#123;IdempotentApplication.class&#125;)public class ResourceTest &#123; private final String resourceName = &quot;template/1.text&quot;; /** * 通过class.getClassLoader获取 */ @Test public void function1() &#123; try &#123; String path = this.getClass().getClassLoader().getResource(&quot;&quot;).getPath();// getResource(&quot;&quot;)里面是空字符串 String filepath = path + resourceName; getFileContent(filepath); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 通过文件名getPath来获取路径 */ @Test public void function2() &#123; try &#123; String path = this.getClass().getClassLoader().getResource(resourceName).getPath(); String filePath = URLDecoder.decode(path, &quot;UTF-8&quot;);//如果路径中带有中文会被URLEncoder,因此这里需要解码 getFileContent(filePath); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 直接通过文件名 + getFile()获取 */ @Test public void function3() &#123; try &#123; String path = this.getClass().getClassLoader().getResource(resourceName).getFile(); String filePath = URLDecoder.decode(path, &quot;UTF-8&quot;);//如果路径中带有中文会被URLEncoder,因此这里需要解码 getFileContent(filePath); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 直接使用getResourceAsStream方法获取流 * springboot项目中需要使用此种方法，因为jar包中没有一个实际的路径存放文件 */ @Test public void function4() &#123; try &#123; InputStream in = this.getClass().getClassLoader().getResourceAsStream(resourceName); getFileContent(in); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 直接使用getResourceAsStream方法获取流 * 如果不使用getClassLoader，可以使用getResourceAsStream(&quot;/配置测试.txt&quot;)直接从resources根路径下获取 */ @Test public void function5() &#123; try &#123; InputStream in = this.getClass().getResourceAsStream(&quot;/&quot;+resourceName); getFileContent(in); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 通过ClassPathResource类获取，建议SpringBoot中使用 * springboot项目中需要使用此种方法，因为jar包中没有一个实际的路径存放文件 */ @Test public void function6() &#123; try &#123; ClassPathResource cb = new ClassPathResource(resourceName); InputStream in = cb.getInputStream(); getFileContent(in); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 通过绝对路径获取项目中文件的位置（不能用于服务器） */ @Test public void function7() &#123; try &#123; String rootPath = System.getProperty(&quot;user.dir&quot;); String filePath = rootPath + &quot;/src/main/resources/&quot; + resourceName; getFileContent(filePath); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** * 通过绝对路径获取项目中文件的位置（不能用于服务器） */ @Test public void function8() &#123; try &#123; //参数为空 File directory = new File(&quot;&quot;); //规范路径：getCanonicalPath() 方法返回绝对路径，会把 ..\\ 、.\\ 这样的符号解析掉 String rootCanonicalPath = directory.getCanonicalPath(); //绝对路径：getAbsolutePath() 方法返回文件的绝对路径，如果构造的时候是全路径就直接返回全路径，如果构造时是相对路径，就返回当前目录的路径 + 构造 File 对象时的路径 String filePath = rootCanonicalPath + &quot;/src/main/resources/&quot; + resourceName; getFileContent(filePath); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 根据文件路径读取文件内容 * * @param fileInPath 文件路径 * @throws IOException 异常 */ public static void getFileContent(Object fileInPath) throws IOException &#123; BufferedReader br = null; if (fileInPath == null) &#123; return; &#125; if (fileInPath instanceof String) &#123; br = new BufferedReader(new FileReader((String) fileInPath)); &#125; else if (fileInPath instanceof InputStream) &#123; br = new BufferedReader(new InputStreamReader((InputStream) fileInPath)); &#125; String line; StringBuilder sb = new StringBuilder(); while ((line = br.readLine()) != null) &#123; sb.append(line); &#125; System.out.println(sb.toString()); br.close(); &#125;&#125;","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Java 反射机制详解","slug":"reflection","date":"2023-03-18T14:10:52.000Z","updated":"2023-03-18T14:10:52.000Z","comments":false,"path":"reflection/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/reflection/index.html","excerpt":"","text":"Java反射机制是在运行状态中，对于任意一个类都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性； 这种动态获取的信息以及动态调用对象的方法的功能成为Java语言的反射机制。 1. 反射基础 RTTI（Run-Time Type Identification）运行时类别识别，在《Thinking in java》中提到其作用是在运行时识别一个对象的类型和类的信息。 主要有两种方式： 1. 传统的RTTI，它嘉定我们在编译时已经知道了所有的类型； 2. &quot;反射&quot;机制，运行在运行时发现和使用类的信息 反射就是把Java类中的各种成分映射成一个个Java对象 例如：一个类有：成员变量、方法、构造方法、包等信息，利用反射技术可以对一个类进行解刨，把个个组成部分映射成一个个对象 1.1. Class类 Class类时一个实实在在的类，存在与JDK的java.lang包中，Class类的示例表示Java应用运行时的类（class ans enum）或接口（interface and annotation） （每个类运行时都在JVM里表现为一个class对象，可以通过类名.class、类型.getClass()、Class.forName(“类名”)等方法获取class对象）. 数组同样也被映射为class 对象的一个类，所有具有相同元素类型和维数的数组都共享该 Class 对象。基本类型boolean，byte，char，short，int，long，float，double和关键字void同样表现为 class 对象。 1234567891011121314151617181920212223public final class Class&lt;T&gt; implements java.io.Serializable, GenericDeclaration, Type, AnnotatedElement &#123; private static final int ANNOTATION = 0x00002000; private static final int ENUM = 0x00004000; private static final int SYNTHETIC = 0x00001000; private static native void registerNatives(); static &#123; registerNatives(); &#125; /* * Private constructor. Only the Java Virtual Machine creates Class objects. //私有构造器，只有JVM才能调用创建Class对象 * This constructor is not used and prevents the default constructor being * generated. */ private Class(ClassLoader loader) &#123; // Initialize final field for classLoader. The initialization value of non-null // prevents future JIT optimizations from assuming this final field is null. classLoader = loader; &#125; //..........&#125; 到这里我们可以得出以下几点信息： Class类也是类的一种，与class关键字时不一样的 手动编写的类被编译后会产生一个Class对象，其表示的是创建的类的类型信息，而且这个Class对象保存在同名.class文件中字节码文件（ 每个通过关键字class标识的类，在内存中有且只有一个与之对应的Class对象来描述其类型信息，无论创建多少个实例对象，其依据的都是用一个Class对象。 Class类只存私有构造函数，因此对应Class对象只能有JVM创建和加载 Class类的对象作用是运行时提供或获得某个对象的类型信息，这点对于反射技术很重要(关于反射稍后分析)。 1.2. 类加载 类加载机制和类字节码技术可以参考以下两篇文章： JVM基础 - 类字节码详解 源代码通过编译器编译为字节码，再通过类加载子系统进行加载到JVM中运行 JVM基础 - Java类加载机制 这里我们需要知道的是： 类加载机制流程 类的加载 2. 反射的使用 在Java中，Class类和java.lang.reflect类库一起对反射进行类权利的支持。在反射包中， 我们常用的类主要有： Constructor类表示Class对象所表示的类的构造方法，利用它可以在运行时动态创建对象 Field表示Class对象所表示的类的成员变量，通过他可以在运行时动态修改成员变量的属性值（包括private） Method表示Class对象所表示的类的成员方法，通过它可以动态调用对象的方法（包括private） 2.1. Class类对象的获取 在类加载的时候，JVM会创建一个class对象，class对象是反射中最常用的，获取class对象的方式主要有三种： 根据类名： 类名.class 根据对象： 对象.getClass() 根据全限定类名：Class.forName(“全限定类名”) 1234567891011121314151617181920212223242526272829303132333435363738394041 @Testpublic void classTest() throws Exception &#123; // 获取Class对象的三种方式 logger.info(&quot;根据类名: \\t&quot; + User.class); logger.info(&quot;根据对象: \\t&quot; + new User().getClass()); logger.info(&quot;根据全限定类名:\\t&quot; + Class.forName(&quot;com.test.User&quot;)); // 常用的方法 logger.info(&quot;获取全限定类名:\\t&quot; + userClass.getName()); logger.info(&quot;获取类名:\\t&quot; + userClass.getSimpleName()); logger.info(&quot;实例化:\\t&quot; + userClass.newInstance());&#125;// ...package com.test;public class User &#123; private String name = &quot;init&quot;; private int age; public User() &#123;&#125; public User(String name, int age) &#123; super(); this.name = name; this.age = age; &#125; private String getName() &#123; return name; &#125; private void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;User [name=&quot; + name + &quot;, age=&quot; + age + &quot;]&quot;; &#125;&#125; 输出结果： 123456根据类名: class com.test.User根据对象: class com.test.User根据全限定类名: class com.test.User获取全限定类名: com.test.User获取类名: User实例化: User [name=init, age=0] 再来看下Class类的方法： 方法名 说明 forName() 1. 获取Class对象的一个引用，但引用的类还没有加载(该类的第一个对象没有生成)就加载了这个类。 2.为了产生Class引用，forName()立即就进行了初始化。 Object-getClass() 获取Class对象的一个引用，返回表示该对象的实际类型的Class引用 getName() 取全限定的类名(包括包名)，即类的完整名字。 getSimpleName() 获取类名(不包括包名) getCanonicalName() 获取全限定的类名(包括包名) isInterface() 判断Class对象是否是表示一个接口 getInterfaces() 返回Class对象数组，表示Class对象所引用的类所实现的所有接口 getSupercalss() 返回Class对象，表示Class对象所引用的类所继承的直接基类。应用该方法可在运行时发现一个对象完整的继承结构 newInstance() 返回一个Oject对象，是实现“虚拟构造器”的一种途径。使用该方法创建的类，必须带有无参的构造器 getFields() 获得某个类的所有的公共（public）的字段，包括继承自父类的所有公共字段。 类似的还有getMethods和getConstructors。 getDeclaredFields 获得某个类的自己声明的字段，即包括public、private和proteced，默认但是不包括父类声明的任何字段。类似的还有getDeclaredMethods和getDeclaredConstructors。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.cry;import java.lang.reflect.Field;interface I1 &#123;&#125;interface I2 &#123;&#125;class Cell&#123; public int mCellPublic;&#125;class Animal extends Cell&#123; private int mAnimalPrivate; protected int mAnimalProtected; int mAnimalDefault; public int mAnimalPublic; private static int sAnimalPrivate; protected static int sAnimalProtected; static int sAnimalDefault; public static int sAnimalPublic;&#125;class Dog extends Animal implements I1, I2 &#123; private int mDogPrivate; public int mDogPublic; protected int mDogProtected; private int mDogDefault; private static int sDogPrivate; protected static int sDogProtected; static int sDogDefault; public static int sDogPublic;&#125;public class Test &#123; public static void main(String[] args) throws IllegalAccessException, InstantiationException &#123; Class&lt;Dog&gt; dog = Dog.class; //类名打印 System.out.println(dog.getName()); //com.cry.Dog System.out.println(dog.getSimpleName()); //Dog System.out.println(dog.getCanonicalName());//com.cry.Dog //接口 System.out.println(dog.isInterface()); //false for (Class iI : dog.getInterfaces()) &#123; System.out.println(iI); &#125; /* interface com.cry.I1 interface com.cry.I2 */ //父类 System.out.println(dog.getSuperclass());//class com.cry.Animal //创建对象 Dog d = dog.newInstance(); //字段 for (Field f : dog.getFields()) &#123; System.out.println(f.getName()); &#125; /* mDogPublic sDogPublic mAnimalPublic sAnimalPublic mCellPublic //父类的父类的公共字段也打印出来了 */ System.out.println(&quot;---------&quot;); for (Field f : dog.getDeclaredFields()) &#123; System.out.println(f.getName()); &#125; /** 只有自己类声明的字段 mDogPrivate mDogPublic mDogProtected mDogDefault sDogPrivate sDogProtected sDogDefault sDogPublic */ &#125;&#125; getName、getCanonicalName与getSimpleName的区别: getSimpleName: 只获取类名 getName：类的全限定名，JVM中class的表示，可以用于动态加载class对象； getCanonicalName： 主要用于输出(toString)或log打印，大多数情况和getName一样，但是在内部类、数组等类型的表示形式就不同了 123456789101112131415161718192021public class Test &#123; private class inner&#123; &#125; public static void main(String[] args) throws ClassNotFoundException &#123; //普通类 System.out.println(Test.class.getSimpleName()); //Test System.out.println(Test.class.getName()); //com.cry.Test System.out.println(Test.class.getCanonicalName()); //com.cry.Test //内部类 System.out.println(inner.class.getSimpleName()); //inner System.out.println(inner.class.getName()); //com.cry.Test$inner System.out.println(inner.class.getCanonicalName()); //com.cry.Test.inner //数组 System.out.println(args.getClass().getSimpleName()); //String[] System.out.println(args.getClass().getName()); //[Ljava.lang.String; System.out.println(args.getClass().getCanonicalName()); //java.lang.String[] //我们不能用getCanonicalName去加载类对象，必须用getName //Class.forName(inner.class.getCanonicalName()); 报错 Class.forName(inner.class.getName()); &#125;&#125; 2.2. Constructor类及其用法 Constructor类存在于反射包java.lang.reflect中，反射的是Class对象所表示的类的构造方法 获取Constructor对象是通过Class类中的方法获取的，Class类与Constructor相关的主要方法如下： 方法返回值 方法名 方法说明 static Class&lt;?&gt; forName(String className) 返回与带有给定字符串名的类或接口相关联的 Class 对象。 Constructor getConstructor(Class&lt;?&gt;… parameterTypes) 返回指定参数类型、具有public访问权限的构造函数对象 Constructor&lt;?&gt;[] getConstructors() 返回所有具有public访问权限的构造函数的Constructor对象数组 Constructor getDeclaredConstructor(Class&lt;?&gt;… parameterTypes) 返回指定参数类型、所有声明的（包括private）构造函数对象 Constructor&lt;?&gt;[] getDeclaredConstructors() 返回所有声明的（包括private）构造函数对象 T newInstance() 调用无参构造器创建此 Class 对象所表示的类的一个新实例。 下面看一个简单例子来了解Constructor对象的使用： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101public class ConstructionTest implements Serializable &#123; public static void main(String[] args) throws Exception &#123; Class&lt;?&gt; clazz = null; //获取Class对象的引用 clazz = Class.forName(&quot;com.example.javabase.User&quot;); //第一种方法，实例化默认构造方法，User必须无参构造函数,否则将抛异常 User user = (User) clazz.newInstance(); user.setAge(20); user.setName(&quot;Jack&quot;); System.out.println(user); System.out.println(&quot;--------------------------------------------&quot;); //获取带String参数的public构造函数 Constructor cs1 =clazz.getConstructor(String.class); //创建User User user1= (User) cs1.newInstance(&quot;hiway&quot;); user1.setAge(22); System.out.println(&quot;user1:&quot;+user1.toString()); System.out.println(&quot;--------------------------------------------&quot;); //取得指定带int和String参数构造函数,该方法是私有构造private Constructor cs2=clazz.getDeclaredConstructor(int.class,String.class); //由于是private必须设置可访问 cs2.setAccessible(true); //创建user对象 User user2= (User) cs2.newInstance(25,&quot;hiway2&quot;); System.out.println(&quot;user2:&quot;+user2.toString()); System.out.println(&quot;--------------------------------------------&quot;); //获取所有构造包含private Constructor&lt;?&gt; cons[] = clazz.getDeclaredConstructors(); // 查看每个构造方法需要的参数 for (int i = 0; i &lt; cons.length; i++) &#123; //获取构造函数参数类型 Class&lt;?&gt; clazzs[] = cons[i].getParameterTypes(); System.out.println(&quot;构造函数[&quot;+i+&quot;]:&quot;+cons[i].toString() ); System.out.print(&quot;参数类型[&quot;+i+&quot;]:(&quot;); for (int j = 0; j &lt; clazzs.length; j++) &#123; if (j == clazzs.length - 1) System.out.print(clazzs[j].getName()); else System.out.print(clazzs[j].getName() + &quot;,&quot;); &#125; System.out.println(&quot;)&quot;); &#125; &#125;&#125;class User &#123; private int age; private String name; public User() &#123; super(); &#125; public User(String name) &#123; super(); this.name = name; &#125; /** * 私有构造 * @param age * @param name */ private User(int age, String name) &#123; super(); this.age = age; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; @Override public String toString() &#123; return &quot;User&#123;&quot; + &quot;age=&quot; + age + &quot;, name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &#x27;&#125;&#x27;; &#125;&#125; 输出结果： 12345678910111213/* output User&#123;age=20, name=&#x27;Jack&#x27;&#125;--------------------------------------------user1:User&#123;age=22, name=&#x27;hiway&#x27;&#125;--------------------------------------------user2:User&#123;age=25, name=&#x27;hiway2&#x27;&#125;--------------------------------------------构造函数[0]:private com.example.javabase.User(int,java.lang.String)参数类型[0]:(int,java.lang.String)构造函数[1]:public com.example.javabase.User(java.lang.String)参数类型[1]:(java.lang.String)构造函数[2]:public com.example.javabase.User()参数类型[2]:() 2.3. Field类及其用法 Field提供有关类或接口的单个字段的信息，以及对的动态访问权限，反射的字段可能是一个类（静态）字段或实例字段 同样的道理，我们可以通过Class类提供的方法来获取代表字段信息的Field对象，Field对象相关的方法如下： 返回值 方法名称 方法说明 Field getDeclaredField(String name) 获取指定name名称的(包含private修饰的)字段，不包括继承的字段 Field[] getDeclaredFields() 获取Class对象所表示的类或接口的所有(包含private修饰的)字段,不包括继承的字段 Field getField(String name) 获取指定name名称、具有public修饰的字段，包含继承字段 Field[] getFields() 获取修饰符为public的字段，包含继承字段 下面的代码演示了上述方法的使用过程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class ReflectField &#123; public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException &#123; Class&lt;?&gt; clazz = Class.forName(&quot;reflect.Student&quot;); //获取指定字段名称的Field类,注意字段修饰符必须为public而且存在该字段, // 否则抛NoSuchFieldException Field field = clazz.getField(&quot;age&quot;); System.out.println(&quot;field:&quot;+field); //获取所有修饰符为public的字段,包含父类字段,注意修饰符为public才会获取 Field fields[] = clazz.getFields(); for (Field f:fields) &#123; System.out.println(&quot;f:&quot;+f.getDeclaringClass()); &#125; System.out.println(&quot;================getDeclaredFields====================&quot;); //获取当前类所字段(包含private字段),注意不包含父类的字段 Field fields2[] = clazz.getDeclaredFields(); for (Field f:fields2) &#123; System.out.println(&quot;f2:&quot;+f.getDeclaringClass()); &#125; //获取指定字段名称的Field类,可以是任意修饰符的自动,注意不包含父类的字段 Field field2 = clazz.getDeclaredField(&quot;desc&quot;); System.out.println(&quot;field2:&quot;+field2); &#125; /** 输出结果: field:public int reflect.Person.age f:public java.lang.String reflect.Student.desc f:public int reflect.Person.age f:public java.lang.String reflect.Person.name ================getDeclaredFields==================== f2:public java.lang.String reflect.Student.desc f2:private int reflect.Student.score field2:public java.lang.String reflect.Student.desc */&#125;class Person&#123; public int age; public String name; //省略set和get方法&#125;class Student extends Person&#123; public String desc; private int score; //省略set和get方法&#125; 上述的方法需要注意的是： 如果不需要获取其父类的字段，则使用Class类的getDeclaredField/getDeclaredFelds方法来获取字段 如果需要获取父类的字段，使用Class类的getField/getFields，但是只能会去到public修饰的字段，无法获取父类的私有字段 通过Field类本身的方法对指定类属性赋值： 123456789101112131415161718192021222324//获取Class对象引用Class&lt;?&gt; clazz = Class.forName(&quot;reflect.Student&quot;);Student st= (Student) clazz.newInstance();//获取父类public字段并赋值Field ageField = clazz.getField(&quot;age&quot;);ageField.set(st,18);Field nameField = clazz.getField(&quot;name&quot;);nameField.set(st,&quot;Lily&quot;);//只获取当前类的字段,不获取父类的字段Field descField = clazz.getDeclaredField(&quot;desc&quot;);descField.set(st,&quot;I am student&quot;);Field scoreField = clazz.getDeclaredField(&quot;score&quot;);//设置可访问，score是private的scoreField.setAccessible(true);scoreField.set(st,88);System.out.println(st.toString());//输出结果：Student&#123;age=18, name=&#x27;Lily ,desc=&#x27;I am student&#x27;, score=88&#125; //获取字段值System.out.println(scoreField.get(st));// 88 其中的set(Object obj, Object value)方法是Field类本身的方法，用于设置字段的值，而get(Object obj)则是获取字段的值, 当然关于Field类还有其他常用的方法如下： 返回值 方法名 方法说明 void set(Object obj, Object value) 将指定对象变量上此 Field 对象表示的字段设置为指定的新值。 Object get(Object obj) 返回指定对象上此 Field 表示的字段的值 Class&lt;?&gt; getType() 返回一个 Class 对象，它标识了此Field 对象所表示字段的声明类型 boolean isEnumConstant() 如果此字段表示枚举类型的元素则返回 true；否则返回 false String toGenericString() 返回一个描述此 Field（包括其一般类型）的字符串 String getName() 返回此 Field 对象表示的字段的名称 Class&lt;?&gt; getDeclaringClass() 返回表示类或接口的 Class 对象，该类或接口声明由此 Field 对象表示的字段 void setAccessible(boolean flag) 将此对象的 accessible 标志设置为指示的布尔值,即设置其可访问性 上述方法可能是较为常用的，事实上在设置值的方法上，Field类还提供了专门针对基本数据类型的方法，如setInt()/getInt()、setBoolean()/getBoolean、setChar()/getChar()等等方法 。 需要特别注意的是被final关键字修饰的Field字段是安全的，在运行时可以接收任何修改，但最终其实际值是不会发生改变的 2.4. Method类及其用法 Method 提供关于类或接口上单独某个方法（以及如何访问该方法）的信息，所反映的方法可能是类方法或实例方法（包括抽象方法）。 下面是Class类获取Method对相关的方法： 返回值 方法名称 方法说明 Method getDeclaredMethod(String name, Class&lt;?&gt;… parameterTypes) 返回一个指定参数的Method对象，该对象反映此 Class 对象所表示的类或接口的指定已声明方法。 Method[] getDeclaredMethods() 返回 Method 对象的一个数组，这些对象反映此 Class 对象表示的类或接口声明的所有方法，包括公共、保护、默认（包）访问和私有方法，但不包括继承的方法。 Method getMethod(String name, Class&lt;?&gt;… parameterTypes) 返回一个 Method 对象，它反映此 Class 对象所表示的类或接口的指定公共成员方法。 Method[] getMethods() 返回一个包含某些 Method 对象的数组，这些对象反映此 Class 对象所表示的类或接口（包括那些由该类或接口声明的以及从超类和超接口继承的那些的类或接口）的公共 member 方法。 同样通过案例演示上述方法： 12345678910111213141516171819202122232425262728293031323334353637383940public class ReflectMethod &#123; public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException &#123; Class clazz = Class.forName(&quot;reflect.Circle&quot;); //根据参数获取public的Method,包含继承自父类的方法 Method method = clazz.getMethod(&quot;draw&quot;,int.class,String.class); System.out.println(&quot;method:&quot;+method); //获取所有public的方法: Method[] methods =clazz.getMethods(); for (Method m:methods)&#123; System.out.println(&quot;m::&quot;+m); &#125; System.out.println(&quot;=========================================&quot;); //获取当前类的方法包含private,该方法无法获取继承自父类的method Method method1 = clazz.getDeclaredMethod(&quot;drawCircle&quot;); System.out.println(&quot;method1::&quot;+method1); //获取当前类的所有方法包含private,该方法无法获取继承自父类的method Method[] methods1=clazz.getDeclaredMethods(); for (Method m:methods1)&#123; System.out.println(&quot;m1::&quot;+m); &#125; &#125;&#125;class Shape &#123; public void draw()&#123; System.out.println(&quot;draw&quot;); &#125; public void draw(int count , String name)&#123; System.out.println(&quot;draw &quot;+ name +&quot;,count=&quot;+count); &#125;&#125;class Circle extends Shape&#123; private void drawCircle()&#123; System.out.println(&quot;drawCircle&quot;); &#125; public int getAllCount()&#123; return 100; &#125;&#125; 输出结果： 1234567891011121314151617181920method:public void reflect.Shape.draw(int,java.lang.String)m::public int reflect.Circle.getAllCount()m::public void reflect.Shape.draw()m::public void reflect.Shape.draw(int,java.lang.String)m::public final void java.lang.Object.wait(long,int) throws java.lang.InterruptedExceptionm::public final native void java.lang.Object.wait(long) throws java.lang.InterruptedExceptionm::public final void java.lang.Object.wait() throws java.lang.InterruptedExceptionm::public boolean java.lang.Object.equals(java.lang.Object)m::public java.lang.String java.lang.Object.toString()m::public native int java.lang.Object.hashCode()m::public final native java.lang.Class java.lang.Object.getClass()m::public final native void java.lang.Object.notify()m::public final native void java.lang.Object.notifyAll()=========================================method1::private void reflect.Circle.drawCircle()m1::public int reflect.Circle.getAllCount()m1::private void reflect.Circle.drawCircle() 在通过getMethods方法获取Method对象时，会把父类的方法也获取到，如上的输出结果，把Object类的方法都打印出来了。而getDeclaredMethod/getDeclaredMethods方法都只能获取当前类的方法。我们在使用时根据情况选择即可。下面将演示通过Method对象调用指定类的方法： 1234567891011121314151617181920Class clazz = Class.forName(&quot;reflect.Circle&quot;);//创建对象Circle circle = (Circle) clazz.newInstance();//获取指定参数的方法对象MethodMethod method = clazz.getMethod(&quot;draw&quot;,int.class,String.class);//通过Method对象的invoke(Object obj,Object... args)方法调用method.invoke(circle,15,&quot;圈圈&quot;);//对私有无参方法的操作Method method1 = clazz.getDeclaredMethod(&quot;drawCircle&quot;);//修改私有方法的访问标识method1.setAccessible(true);method1.invoke(circle);//对有返回值得方法操作Method method2 =clazz.getDeclaredMethod(&quot;getAllCount&quot;);Integer count = (Integer) method2.invoke(circle);System.out.println(&quot;count:&quot;+count); 输出结果: 123draw 圈圈,count=15drawCirclecount:100 3. 反射机制执行的流程 12345678910111213141516171819public class HelloReflect &#123; public static void main(String[] args) &#123; try &#123; // 1. 使用外部配置的实现，进行动态加载类 TempFunctionTest test = (TempFunctionTest)Class.forName(&quot;com.tester.HelloReflect&quot;).newInstance(); test.sayHello(&quot;call directly&quot;); // 2. 根据配置的函数名，进行方法调用（不需要通用的接口抽象） Object t2 = new TempFunctionTest(); Method method = t2.getClass().getDeclaredMethod(&quot;sayHello&quot;, String.class); method.invoke(test, &quot;method invoke&quot;); &#125; catch (ClassNotFoundException | InstantiationException | NoSuchMethodException | IllegalAccessException | InvocationTargetException e) &#123; e.printStackTrace(); &#125; &#125; public void sayHello(String word) &#123; System.out.println(&quot;hello,&quot; + word); &#125;&#125; 来看下执行流程 3.1. 反射获取类实例 首先调用了java.lang.Class的静态方法，获取类的信息 1234567@CallerSensitivepublic static Class&lt;?&gt; forName(String className) throws ClassNotFoundException &#123; // 先通过反射，获取调用进来的类信息，从而获取当前的 classLoader Class&lt;?&gt; caller = Reflection.getCallerClass(); // 调用native方法进行获取class信息 return forName0(className, true, ClassLoader.getClassLoader(caller), caller);&#125; forName()反射获取类的信息，并没有将实现留给Java，而是交给了jvm去加载。 首先是先获取ClassLoader，然后调用native方法，获取信息，加载类则是回调java.lang.ClassLoader，最后，jvm又会回调ClassLoader进行类加载 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException &#123; return loadClass(name, false); &#125;// sun.misc.Launcherpublic Class&lt;?&gt; loadClass(String var1, boolean var2) throws ClassNotFoundException &#123; int var3 = var1.lastIndexOf(46); if (var3 != -1) &#123; SecurityManager var4 = System.getSecurityManager(); if (var4 != null) &#123; var4.checkPackageAccess(var1.substring(0, var3)); &#125; &#125; if (this.ucp.knownToNotExist(var1)) &#123; Class var5 = this.findLoadedClass(var1); if (var5 != null) &#123; if (var2) &#123; this.resolveClass(var5); &#125; return var5; &#125; else &#123; throw new ClassNotFoundException(var1); &#125; &#125; else &#123; return super.loadClass(var1, var2); &#125;&#125;// java.lang.ClassLoaderprotected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; // 先获取锁 synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded // 如果已经加载了的话，就不用再加载了 Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; // 双亲委托加载 if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; // 父类没有加载到时，再自己加载 if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125;&#125;protected Object getClassLoadingLock(String className) &#123; Object lock = this; if (parallelLockMap != null) &#123; // 使用 ConcurrentHashMap来保存锁 Object newLock = new Object(); lock = parallelLockMap.putIfAbsent(className, newLock); if (lock == null) &#123; lock = newLock; &#125; &#125; return lock;&#125;protected final Class&lt;?&gt; findLoadedClass(String name) &#123; if (!checkName(name)) return null; return findLoadedClass0(name);&#125; 下面来看一下newInstance()的实现方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960// 首先肯定是 Class.newInstance@CallerSensitivepublic T newInstance() throws InstantiationException, IllegalAccessException&#123; if (System.getSecurityManager() != null) &#123; checkMemberAccess(Member.PUBLIC, Reflection.getCallerClass(), false); &#125; // NOTE: the following code may not be strictly correct under // the current Java memory model. // Constructor lookup // newInstance() 其实相当于调用类的无参构造函数，所以，首先要找到其无参构造器 if (cachedConstructor == null) &#123; if (this == Class.class) &#123; // 不允许调用 Class 的 newInstance() 方法 throw new IllegalAccessException( &quot;Can not call newInstance() on the Class for java.lang.Class&quot; ); &#125; try &#123; // 获取无参构造器 Class&lt;?&gt;[] empty = &#123;&#125;; final Constructor&lt;T&gt; c = getConstructor0(empty, Member.DECLARED); // Disable accessibility checks on the constructor // since we have to do the security check here anyway // (the stack depth is wrong for the Constructor&#x27;s // security check to work) java.security.AccessController.doPrivileged( new java.security.PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; c.setAccessible(true); return null; &#125; &#125;); cachedConstructor = c; &#125; catch (NoSuchMethodException e) &#123; throw (InstantiationException) new InstantiationException(getName()).initCause(e); &#125; &#125; Constructor&lt;T&gt; tmpConstructor = cachedConstructor; // Security check (same as in java.lang.reflect.Constructor) int modifiers = tmpConstructor.getModifiers(); if (!Reflection.quickCheckMemberAccess(this, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); if (newInstanceCallerCache != caller) &#123; Reflection.ensureMemberAccess(caller, this, null, modifiers); newInstanceCallerCache = caller; &#125; &#125; // Run constructor try &#123; // 调用无参构造器 return tmpConstructor.newInstance((Object[])null); &#125; catch (InvocationTargetException e) &#123; Unsafe.getUnsafe().throwException(e.getTargetException()); // Not reached return null; &#125;&#125; newInstance()主要做了三件事： 权限检测，如果不通过就抛出异常 查找无参构造，并将其缓存起来； 调用具体方法的无参构造方法，生成实例并返回 下面是获取构造器的过程： 12345678910111213private Constructor&lt;T&gt; getConstructor0(Class&lt;?&gt;[] parameterTypes, int which) throws NoSuchMethodException &#123; // 获取所有构造器 Constructor&lt;T&gt;[] constructors = privateGetDeclaredConstructors((which == Member.PUBLIC)); for (Constructor&lt;T&gt; constructor : constructors) &#123; if (arrayContentsEq(parameterTypes, constructor.getParameterTypes())) &#123; return getReflectionFactory().copyConstructor(constructor); &#125; &#125; throw new NoSuchMethodException(getName() + &quot;.&lt;init&gt;&quot; + argumentTypesToString(parameterTypes)); &#125; getConstructor()为获取匹配的构造器，分三步 先获取所有的constructors，然后通过进行参数比较 找到匹配后，通过ReflectionFactory copy一份constructor返回； 否则抛出NoSuchMethodException 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 获取当前类所有的构造方法，通过jvm或者缓存 // Returns an array of &quot;root&quot; constructors. These Constructor // objects must NOT be propagated to the outside world, but must // instead be copied via ReflectionFactory.copyConstructor. private Constructor&lt;T&gt;[] privateGetDeclaredConstructors(boolean publicOnly) &#123; checkInitted(); Constructor&lt;T&gt;[] res; // 调用 reflectionData(), 获取保存的信息，使用软引用保存，从而使内存不够可以回收 ReflectionData&lt;T&gt; rd = reflectionData(); if (rd != null) &#123; res = publicOnly ? rd.publicConstructors : rd.declaredConstructors; // 存在缓存，则直接返回 if (res != null) return res; &#125; // No cached value available; request value from VM if (isInterface()) &#123; @SuppressWarnings(&quot;unchecked&quot;) Constructor&lt;T&gt;[] temporaryRes = (Constructor&lt;T&gt;[]) new Constructor&lt;?&gt;[0]; res = temporaryRes; &#125; else &#123; // 使用native方法从jvm获取构造器 res = getDeclaredConstructors0(publicOnly); &#125; if (rd != null) &#123; // 最后，将从jvm中读取的内容，存入缓存 if (publicOnly) &#123; rd.publicConstructors = res; &#125; else &#123; rd.declaredConstructors = res; &#125; &#125; return res; &#125; // Lazily create and cache ReflectionData private ReflectionData&lt;T&gt; reflectionData() &#123; SoftReference&lt;ReflectionData&lt;T&gt;&gt; reflectionData = this.reflectionData; int classRedefinedCount = this.classRedefinedCount; ReflectionData&lt;T&gt; rd; if (useCaches &amp;&amp; reflectionData != null &amp;&amp; (rd = reflectionData.get()) != null &amp;&amp; rd.redefinedCount == classRedefinedCount) &#123; return rd; &#125; // else no SoftReference or cleared SoftReference or stale ReflectionData // -&gt; create and replace new instance return newReflectionData(reflectionData, classRedefinedCount); &#125; // 新创建缓存，保存反射信息 private ReflectionData&lt;T&gt; newReflectionData(SoftReference&lt;ReflectionData&lt;T&gt;&gt; oldReflectionData, int classRedefinedCount) &#123; if (!useCaches) return null; // 使用cas保证更新的线程安全性，所以反射是保证线程安全的 while (true) &#123; ReflectionData&lt;T&gt; rd = new ReflectionData&lt;&gt;(classRedefinedCount); // try to CAS it... if (Atomic.casReflectionData(this, oldReflectionData, new SoftReference&lt;&gt;(rd))) &#123; return rd; &#125; // 先使用CAS更新，如果更新成功，则立即返回，否则测查当前已被其他线程更新的情况，如果和自己想要更新的状态一致，则也算是成功了 oldReflectionData = this.reflectionData; classRedefinedCount = this.classRedefinedCount; if (oldReflectionData != null &amp;&amp; (rd = oldReflectionData.get()) != null &amp;&amp; rd.redefinedCount == classRedefinedCount) &#123; return rd; &#125; &#125; &#125; 如上，privateGetDeclaredConstructors(), 获取所有的构造器主要步骤： 先尝试从缓存中获取； 如果缓存没有，则从jvm中重新获取，并存入缓存，缓存使用软引用进行保存，保证内存可用； 另外，使用reflectionData()进行缓存保存；ReflectionData 的数据结构如下: 1234567891011121314151617181920// reflection data that might get invalidated when JVM TI RedefineClasses() is called private static class ReflectionData&lt;T&gt; &#123; volatile Field[] declaredFields; volatile Field[] publicFields; volatile Method[] declaredMethods; volatile Method[] publicMethods; volatile Constructor&lt;T&gt;[] declaredConstructors; volatile Constructor&lt;T&gt;[] publicConstructors; // Intermediate results for getFields and getMethods volatile Field[] declaredPublicFields; volatile Method[] declaredPublicMethods; volatile Class&lt;?&gt;[] interfaces; // Value of classRedefinedCount when we created this ReflectionData instance final int redefinedCount; ReflectionData(int redefinedCount) &#123; this.redefinedCount = redefinedCount; &#125; &#125; 其中，还有一个点，就是如何比较构造是否是要查找构造器，其实就是比较类型完成相等就完了，有一个不相等则返回false。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354private static boolean arrayContentsEq(Object[] a1, Object[] a2) &#123; if (a1 == null) &#123; return a2 == null || a2.length == 0; &#125; if (a2 == null) &#123; return a1.length == 0; &#125; if (a1.length != a2.length) &#123; return false; &#125; for (int i = 0; i &lt; a1.length; i++) &#123; if (a1[i] != a2[i]) &#123; return false; &#125; &#125; return true; &#125; // sun.reflect.ReflectionFactory /** Makes a copy of the passed constructor. The returned constructor is a &quot;child&quot; of the passed one; see the comments in Constructor.java for details. */ public &lt;T&gt; Constructor&lt;T&gt; copyConstructor(Constructor&lt;T&gt; arg) &#123; return langReflectAccess().copyConstructor(arg); &#125; // java.lang.reflect.Constructor, copy 其实就是新new一个 Constructor 出来 Constructor&lt;T&gt; copy() &#123; // This routine enables sharing of ConstructorAccessor objects // among Constructor objects which refer to the same underlying // method in the VM. (All of this contortion is only necessary // because of the &quot;accessibility&quot; bit in AccessibleObject, // which implicitly requires that new java.lang.reflect // objects be fabricated for each reflective call on Class // objects.) if (this.root != null) throw new IllegalArgumentException(&quot;Can not copy a non-root Constructor&quot;); Constructor&lt;T&gt; res = new Constructor&lt;&gt;(clazz, parameterTypes, exceptionTypes, modifiers, slot, signature, annotations, parameterAnnotations); // root 指向当前 constructor res.root = this; // Might as well eagerly propagate this if already present res.constructorAccessor = constructorAccessor; return res; &#125; 通过上面，获取到 Constructor 了。 接下来就只需调用其相应构造器的 newInstance()，即返回实例了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// return tmpConstructor.newInstance((Object[])null); // java.lang.reflect.Constructor @CallerSensitive public T newInstance(Object ... initargs) throws InstantiationException, IllegalAccessException, IllegalArgumentException, InvocationTargetException &#123; if (!override) &#123; if (!Reflection.quickCheckMemberAccess(clazz, modifiers)) &#123; Class&lt;?&gt; caller = Reflection.getCallerClass(); checkAccess(caller, clazz, null, modifiers); &#125; &#125; if ((clazz.getModifiers() &amp; Modifier.ENUM) != 0) throw new IllegalArgumentException(&quot;Cannot reflectively create enum objects&quot;); ConstructorAccessor ca = constructorAccessor; // read volatile if (ca == null) &#123; ca = acquireConstructorAccessor(); &#125; @SuppressWarnings(&quot;unchecked&quot;) T inst = (T) ca.newInstance(initargs); return inst; &#125; // sun.reflect.DelegatingConstructorAccessorImpl public Object newInstance(Object[] args) throws InstantiationException, IllegalArgumentException, InvocationTargetException &#123; return delegate.newInstance(args); &#125; // sun.reflect.NativeConstructorAccessorImpl public Object newInstance(Object[] args) throws InstantiationException, IllegalArgumentException, InvocationTargetException &#123; // We can&#x27;t inflate a constructor belonging to a vm-anonymous class // because that kind of class can&#x27;t be referred to by name, hence can&#x27;t // be found from the generated bytecode. if (++numInvocations &gt; ReflectionFactory.inflationThreshold() &amp;&amp; !ReflectUtil.isVMAnonymousClass(c.getDeclaringClass())) &#123; ConstructorAccessorImpl acc = (ConstructorAccessorImpl) new MethodAccessorGenerator(). generateConstructor(c.getDeclaringClass(), c.getParameterTypes(), c.getExceptionTypes(), c.getModifiers()); parent.setDelegate(acc); &#125; // 调用native方法，进行调用 constructor return newInstance0(c, args); &#125; 返回构造器的实例后，可以根据外部进行进行类型转换，从而使用接口或方法进行调用实例功能了。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"Java机制","slug":"Java机制","permalink":"https://xiaoyuge5201.github.io/tags/Java%E6%9C%BA%E5%88%B6/"}]},{"title":"Java SPI机制详解","slug":"spi","date":"2023-03-17T11:21:07.000Z","updated":"2023-03-17T11:21:07.000Z","comments":false,"path":"spi/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/spi/index.html","excerpt":"","text":"1. 什么是SPI机制 SPI（Service Provider Interface），是JDK内置的一种服务提供发现机制，可以用来启动框架扩展和和替换组建 比如java.sql.Driver接口，其他不同厂商够可以针对统一接口做出不同的实现，Mysql和PostgreSQL都有不同的实现提供给用户，而Java的SPI机制可以为某个接口寻找服务发现。 Java中SPI机制的主要是想是将装配的控制权移到程序之外，在模块化设计中这个机制尤其重要，其核心思想就是：解耦 SPI 整体机制图如下： 当服务的提供者提供一种接口的实现之后，需要在classpath下的META-INF/services/目录里创建和一个以服务接口命名的文件，这个文件里的内容就是这个接口的实现类。当前他的程序需要这个服务时，就可以通过查找这个jar包（一般都是以jar包做依赖）的 META-INF/servies中的配置文件，配置文件中有这个接口的具体实现类名，可以根据这个类名进行加载实例化，就可以使用该服务了，JDK查找服务的实现工具类时：java.util.ServiceLoader。 2. SPI机制的简单示例 先定一个内容搜索接口，搜索的实现可能时基于文件系统的搜索，也可能是基于数据库的搜索 定义接口 123public interface Search &#123; List&lt;String&gt; searchDoc(String keyword);&#125; 文件搜索实现 1234567public class FileSearch implements Search&#123; @Override public List&lt;String&gt; searchDoc(String keyword) &#123; System.out.println(&quot;文件搜索&quot;+keyword); return null; &#125;&#125; 数据库搜索实现 1234567public class DatabaseSearch implements Search&#123; @Override public List&lt;String&gt; searchDoc(String keyword) &#123; System.out.println(&quot;数据库搜索&quot;+keyword); return null; &#125;&#125; 配置META-INF/services 在resources下新建 META-INF/services目录，然后新建接口全限定名的文件com.ygb.Search，里面添加需要用到的实现类 12com.ygb.FileSearchcom.ygb.DatabaseSearch 测试 12345678910111213public class TestSPI &#123; public static void main(String[] args) &#123; ServiceLoader&lt;Search&gt; service = ServiceLoader.load(Search.class); Iterator&lt;Search&gt; iterator = service.iterator(); while (iterator.hasNext()) &#123; Search search = iterator.next(); search.searchDoc(&quot;hello world&quot;); &#125; &#125;&#125;//在services目录下配置了两个实现类，那么会输出：// 文件搜索hello world// 数据库搜索hello world 总结 ServiceLoader.load(Search.class)在加载某接口时，会去META-INF/services下查找接口的全限定名文件，再根据里面的内容加载相应的实现类。 这就是SPI思想，接口的实现由provider实现，provider只用在提交的jar包里面的META-INF/services下根据平台定义的接口新建文件，并添加相应的实现内容就可以。 3. SPI机制的应用 3.1. JDBC DriverManger JDBC4.0之前，我们开发连接数据库的时候，通常会用Class.forName(“com.mysql.jdbc.Driver”)这句先加载数据库驱动，然后在获取数据库连接等操作。 JDBC4.0之后，直接获取连接即可，不需要采用Class.forName这种方式。 3.1.1. JDBC接口定义 首先在Java中定义了接口java.sql.Driver，并没有具体的实现，具体的都是根据不同的厂商来提供的 3.1.2. Mysql实现 在mysql的jar包mysql-connector-java-6.0.6.jar中，可以找到META-INF/services目录，该目录下会有一个名字为java.sql.Driver的文件，文件内容是com.mysql.cj.jdbc.Driver，这里面的内容就是针对Java中定义的接口的实现。# 3.1.3. Postgresql实现 同样在postgresql的jar包postgresql-42.0.0.jar中，也可以找到同样的配置文件，文件内容是org.postgresql.Driver，这是postgresql对Java的java.sql.Driver的实现。 3.1.4. 使用方法 上面说了，现在使用SPI扩展来加载具体的驱动，我们在使用数据库连接代码时，不需要再使用Class.forName()来加载驱动来，而是使用以下嗲吗 12String url = &quot;jdbc:mysq://192.168.0.1:3306/db&quot;;Connection conn = DriverManger.getConnnection(url, username,password); 这里并没有涉及到SPI的实现，接着看下面的解析。 3.1.5. 源码实现 上面的代码没有来加载驱动的代码，怎么去确定使用那个数据库连接的驱动呢？这里就涉及到Java的SPI扩展机制来查找相关驱动的东西来， 关于驱动的查找其实都在DriverManager中，在DriverManager有一个静态代码块如下： 123456789/** * Load the initial JDBC drivers by checking the System property* jdbc.properties and then use the &#123;@code ServiceLoader&#125; mechanism* 通过检查System属性jdbc.properties和然后使用&#123;@code ServiceLoader&#125;机制来加载初始JDBC驱动程序 */static &#123; loadInitialDrivers(); println(&quot;JDBC DriverManager initialized&quot;);&#125; 可以看到是加载实例化驱动的，接着看loadInitialDrivers方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private static void loadInitialDrivers() &#123; String drivers; try &#123; drivers = AccessController.doPrivileged(new PrivilegedAction&lt;String&gt;() &#123; public String run() &#123; return System.getProperty(&quot;jdbc.drivers&quot;); &#125; &#125;); &#125; catch (Exception ex) &#123; drivers = null; &#125; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; //使用SPI的ServiceLoader来加载接口的实现 //这里没有去META-INF/services目录下查找配置文件，也没有加载具体实现类，做的事情就是封装了我们的接口类型和类加载器，并初始化了一个迭代器 ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); //获取迭代器 Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try&#123; //遍历所有的驱动实现 //搜索classpath下以及jar包中所有的META-INF/services目录下的java.sql.Driver文件，并找到文件中的实现类的名字，此时并没有实例化具体的实现类 while(driversIterator.hasNext()) &#123; //这里会根据驱动名字具体实例化各个实现类 driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125; return null; &#125; &#125;); println(&quot;DriverManager.initialize: jdbc.drivers = &quot; + drivers); if (drivers == null || drivers.equals(&quot;&quot;)) &#123; return; &#125; String[] driversList = drivers.split(&quot;:&quot;); println(&quot;number of Drivers:&quot; + driversList.length); for (String aDriver : driversList) &#123; try &#123; println(&quot;DriverManager.Initialize: loading &quot; + aDriver); Class.forName(aDriver, true, ClassLoader.getSystemClassLoader()); &#125; catch (Exception ex) &#123; println(&quot;DriverManager.Initialize: load failed: &quot; + ex); &#125; &#125; &#125; 上面的代码主要步骤时： 从系统变量中获取有关驱动的定义 使用SPI来获取驱动的实现 便利使用SPI获取到的具体实现，实例话各个实现类 根据第一步获取到的驱动列表来实例话具体实现类 3.2. Common-Logging common-logging(Jakarta Commons Logging,缩写JCL)是常用的日志库门面，可以看下它是怎么解耦的 首先，日志示例是通过LogFactory的getLog(String)方法创建的 12345//private Log log = LogFactory.getLog(TestSPI.class);public static Log getLog(Class clazz) throws LogConfigurationException &#123; return getFactory().getInstance(clazz);&#125; LogFactory是一个抽象类，它负责加载具体的日志实现，分析其Factory getFactory()方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186public static LogFactory getFactory() throws LogConfigurationException &#123; // Identify the class loader we will be using ClassLoader contextClassLoader = getContextClassLoaderInternal(); if (contextClassLoader == null) &#123; // This is an odd enough situation to report about. This // output will be a nuisance on JDK1.1, as the system // classloader is null in that environment. if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;Context classloader is null.&quot;); &#125; &#125; // Return any previously registered factory for this class loader LogFactory factory = getCachedFactory(contextClassLoader); if (factory != null) &#123; return factory; &#125; if (isDiagnosticsEnabled()) &#123; logDiagnostic( &quot;[LOOKUP] LogFactory implementation requested for the first time for context classloader &quot; + objectId(contextClassLoader)); logHierarchy(&quot;[LOOKUP] &quot;, contextClassLoader); &#125; // classpath根目录下寻找commons-logging.properties Properties props = getConfigurationFile(contextClassLoader, FACTORY_PROPERTIES); // Determine whether we will be using the thread context class loader to // load logging classes or not by checking the loaded properties file (if any). // classpath根目录下commons-logging.properties是否配置use_tccl ClassLoader baseClassLoader = contextClassLoader; if (props != null) &#123; String useTCCLStr = props.getProperty(TCCL_KEY); if (useTCCLStr != null) &#123; if (Boolean.valueOf(useTCCLStr).booleanValue() == false) &#123; baseClassLoader = thisClassLoader; &#125; &#125; &#125; // 这里真正开始决定使用哪个factory // 首先，尝试查找vm系统属性org.apache.commons.logging.LogFactory，其是否指定factory // Determine which concrete LogFactory subclass to use. // First, try a global system property if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] Looking for system property [&quot; + FACTORY_PROPERTY + &quot;] to define the LogFactory subclass to use...&quot;); &#125; try &#123; String factoryClass = getSystemProperty(FACTORY_PROPERTY, null); if (factoryClass != null) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] Creating an instance of LogFactory class &#x27;&quot; + factoryClass + &quot;&#x27; as specified by system property &quot; + FACTORY_PROPERTY); &#125; factory = newFactory(factoryClass, baseClassLoader, contextClassLoader); &#125; else &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] No system property [&quot; + FACTORY_PROPERTY + &quot;] defined.&quot;); &#125; &#125; &#125; catch (SecurityException e) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] A security exception occurred while trying to create an&quot; + &quot; instance of the custom factory class&quot; + &quot;: [&quot; + trim(e.getMessage()) + &quot;]. Trying alternative implementations...&quot;); &#125; // ignore &#125; catch (RuntimeException e) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] An exception occurred while trying to create an&quot; + &quot; instance of the custom factory class&quot; + &quot;: [&quot; + trim(e.getMessage()) + &quot;] as specified by a system property.&quot;); &#125; throw e; &#125; // 第二，尝试使用java spi服务发现机制，载META-INF/services下寻找org.apache.commons.logging.LogFactory实现 if (factory == null) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] Looking for a resource file of name [&quot; + SERVICE_ID + &quot;] to define the LogFactory subclass to use...&quot;); &#125; try &#123; // META-INF/services/org.apache.commons.logging.LogFactory, SERVICE_ID final InputStream is = getResourceAsStream(contextClassLoader, SERVICE_ID); if( is != null ) &#123; BufferedReader rd; try &#123; rd = new BufferedReader(new InputStreamReader(is, &quot;UTF-8&quot;)); &#125; catch (java.io.UnsupportedEncodingException e) &#123; rd = new BufferedReader(new InputStreamReader(is)); &#125; String factoryClassName = rd.readLine(); rd.close(); if (factoryClassName != null &amp;&amp; ! &quot;&quot;.equals(factoryClassName)) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] Creating an instance of LogFactory class &quot; + factoryClassName + &quot; as specified by file &#x27;&quot; + SERVICE_ID + &quot;&#x27; which was present in the path of the context classloader.&quot;); &#125; factory = newFactory(factoryClassName, baseClassLoader, contextClassLoader ); &#125; &#125; else &#123; // is == null if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] No resource file with name &#x27;&quot; + SERVICE_ID + &quot;&#x27; found.&quot;); &#125; &#125; &#125; catch (Exception ex) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic( &quot;[LOOKUP] A security exception occurred while trying to create an&quot; + &quot; instance of the custom factory class&quot; + &quot;: [&quot; + trim(ex.getMessage()) + &quot;]. Trying alternative implementations...&quot;); &#125; // ignore &#125; &#125; // Third try looking into the properties file read earlier (if found) // 第三，尝试从classpath根目录下的commons-logging.properties中查找org.apache.commons.logging.LogFactory属性指定的factory if (factory == null) &#123; if (props != null) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic( &quot;[LOOKUP] Looking in properties file for entry with key &#x27;&quot; + FACTORY_PROPERTY + &quot;&#x27; to define the LogFactory subclass to use...&quot;); &#125; String factoryClass = props.getProperty(FACTORY_PROPERTY); if (factoryClass != null) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic( &quot;[LOOKUP] Properties file specifies LogFactory subclass &#x27;&quot; + factoryClass + &quot;&#x27;&quot;); &#125; factory = newFactory(factoryClass, baseClassLoader, contextClassLoader); // TODO: think about whether we need to handle exceptions from newFactory &#125; else &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] Properties file has no entry specifying LogFactory subclass.&quot;); &#125; &#125; &#125; else &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic(&quot;[LOOKUP] No properties file available to determine&quot; + &quot; LogFactory subclass from..&quot;); &#125; &#125; &#125; // Fourth, try the fallback implementation class // 最后，使用后备factory实现，org.apache.commons.logging.impl.LogFactoryImpl if (factory == null) &#123; if (isDiagnosticsEnabled()) &#123; logDiagnostic( &quot;[LOOKUP] Loading the default LogFactory implementation &#x27;&quot; + FACTORY_DEFAULT + &quot;&#x27; via the same classloader that loaded this LogFactory&quot; + &quot; class (ie not looking in the context classloader).&quot;); &#125; factory = newFactory(FACTORY_DEFAULT, thisClassLoader, contextClassLoader); &#125; if (factory != null) &#123; /** * Always cache using context class loader. */ cacheFactory(contextClassLoader, factory); if (props != null) &#123; Enumeration names = props.propertyNames(); while (names.hasMoreElements()) &#123; String name = (String) names.nextElement(); String value = props.getProperty(name); factory.setAttribute(name, value); &#125; &#125; &#125; return factory; &#125; 可以看出，抽象类LogFactory加载具体实现的步骤如下： 从vm系统属性org.apache.commons.logging.LogFactory 使用SPI服务发现机制，发现org.apache.commons.logging.LogFactory的实现 查找classpath根目录commons-logging.properties的org.apache.commons.logging.LogFactory属性是否指定factory实现 使用默认factory实现，org.apache.commons.logging.impl.LogFactoryImpl LogFactory的getLog()方法返回类型是org.apache.commons.logging.Log接口，提供了从trace到fatal方法。可以确定，如果日志实现提供者只要实现该接口，并且使用继承自org.apache.commons.logging.LogFactory的子类创建Log，必然可以构建一个松耦合的日志系统。 3.3. Spring中的SPI机制 在springboot的自动装配过程中，最终会加载META-INF/spring.factories文件，而加载的过程是由SpringFactoriesLoader加载的。从CLASSPATH下的每个Jar包中搜寻所有META-INF/spring.factories配置文件，然后将解析properties文件，找到指定名称的配置后返回。需要注意的是，其实这里不仅仅是会去ClassPath路径下查找，会扫描所有路径下的Jar包，只不过这个文件只会在Classpath下的jar包中。 1234567891011121314151617181920public static final String FACTORIES_RESOURCE_LOCATION = &quot;META-INF/spring.factories&quot;;// spring.factories文件的格式为：key=value1,value2,value3// 从所有的jar包中找到META-INF/spring.factories文件// 然后从文件中解析出key=factoryClass类名称的所有value值public static List&lt;String&gt; loadFactoryNames(Class&lt;?&gt; factoryClass, ClassLoader classLoader) &#123; String factoryClassName = factoryClass.getName(); // 取得资源文件的URL Enumeration&lt;URL&gt; urls = (classLoader != null ? classLoader.getResources(FACTORIES_RESOURCE_LOCATION) : ClassLoader.getSystemResources(FACTORIES_RESOURCE_LOCATION)); List&lt;String&gt; result = new ArrayList&lt;String&gt;(); // 遍历所有的URL while (urls.hasMoreElements()) &#123; URL url = urls.nextElement(); // 根据资源文件URL解析properties文件，得到对应的一组@Configuration类 Properties properties = PropertiesLoaderUtils.loadProperties(new UrlResource(url)); String factoryClassNames = properties.getProperty(factoryClassName); // 组装数据，并返回 result.addAll(Arrays.asList(StringUtils.commaDelimitedListToStringArray(factoryClassNames))); &#125; return result;&#125; 4. SPI机制深入理解 4.1. SPI机制通常怎么使用 看完上面的几个例子解析，应该都知道大致的流程了： 组织或公司定义标准 就是定义接口，比如java.sql.Driver 具体厂商或框架开发者实现 在META-INF/services目录下定一个名字为接口全限定名的文件，文件内容是具体实现类的全限定名，比如com.cj.mysql.Driver 使用 引用jar来实现功能 12345678ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); //获取迭代器Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); //遍历while(driversIterator.hasNext()) &#123; driversIterator.next(); //可以做具体的业务逻辑&#125; 规范 总结以下jdk中SPI需要遵循的规范 4.2. SPI和API的区别 SPI： &quot;接口&quot;位于调用方所在的包中 概念上更依赖调用方 位于调用方所在的包中 实现位于独立的包中 常见的例子：插件模式的插件 API： &quot;接口&quot;位于实现方所在的包中 概念上更接近实现方 位于实现方所在的包中 实现和接口在一个包中 4.3. SPI机制实现原理 看下JDK中ServiceLoader&lt;S&gt;方法的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259//ServiceLoader实现了Iterable接口，可以遍历所有的服务实现者public final class ServiceLoader&lt;S&gt; implements Iterable&lt;S&gt;&#123; //查找配置文件的目录 private static final String PREFIX = &quot;META-INF/services/&quot;; //表示要被加载的服务的类或接口 private final Class&lt;S&gt; service; //这个ClassLoader用来定位，加载，实例化服务提供者 private final ClassLoader loader; // 访问控制上下文 private final AccessControlContext acc; // 缓存已经被实例化的服务提供者，按照实例化的顺序存储 private LinkedHashMap&lt;String,S&gt; providers = new LinkedHashMap&lt;&gt;(); // 迭代器 private LazyIterator lookupIterator; //重新加载，就相当于重新创建ServiceLoader了，用于新的服务提供者安装到正在运行的Java虚拟机中的情况。 public void reload() &#123; //清空缓存中所有已实例化的服务提供者 providers.clear(); //新建一个迭代器，该迭代器会从头查找和实例化服务提供者 lookupIterator = new LazyIterator(service, loader); &#125; //私有构造器 //使用指定的类加载器和服务创建服务加载器 //如果没有指定类加载器，使用系统类加载器，就是应用类加载器。 private ServiceLoader(Class&lt;S&gt; svc, ClassLoader cl) &#123; service = Objects.requireNonNull(svc, &quot;Service interface cannot be null&quot;); loader = (cl == null) ? ClassLoader.getSystemClassLoader() : cl; acc = (System.getSecurityManager() != null) ? AccessController.getContext() : null; reload(); &#125; //解析失败处理的方法 private static void fail(Class&lt;?&gt; service, String msg, Throwable cause) throws ServiceConfigurationError &#123; throw new ServiceConfigurationError(service.getName() + &quot;: &quot; + msg, cause); &#125; private static void fail(Class&lt;?&gt; service, String msg) throws ServiceConfigurationError &#123; throw new ServiceConfigurationError(service.getName() + &quot;: &quot; + msg); &#125; private static void fail(Class&lt;?&gt; service, URL u, int line, String msg) throws ServiceConfigurationError &#123; fail(service, u + &quot;:&quot; + line + &quot;: &quot; + msg); &#125; //解析服务提供者配置文件中的一行 //首先去掉注释校验，然后保存 //返回下一行行号 //重复的配置项和已经被实例化的配置项不会被保存 private int parseLine(Class&lt;?&gt; service, URL u, BufferedReader r, int lc, List&lt;String&gt; names) throws IOException, ServiceConfigurationError &#123; //读取一行 String ln = r.readLine(); if (ln == null) &#123; return -1; &#125; //#号代表注释行 int ci = ln.indexOf(&#x27;#&#x27;); if (ci &gt;= 0) ln = ln.substring(0, ci); ln = ln.trim(); int n = ln.length(); if (n != 0) &#123; if ((ln.indexOf(&#x27; &#x27;) &gt;= 0) || (ln.indexOf(&#x27;\\t&#x27;) &gt;= 0)) fail(service, u, lc, &quot;Illegal configuration-file syntax&quot;); int cp = ln.codePointAt(0); if (!Character.isJavaIdentifierStart(cp)) fail(service, u, lc, &quot;Illegal provider-class name: &quot; + ln); for (int i = Character.charCount(cp); i &lt; n; i += Character.charCount(cp)) &#123; cp = ln.codePointAt(i); if (!Character.isJavaIdentifierPart(cp) &amp;&amp; (cp != &#x27;.&#x27;)) fail(service, u, lc, &quot;Illegal provider-class name: &quot; + ln); &#125; if (!providers.containsKey(ln) &amp;&amp; !names.contains(ln)) names.add(ln); &#125; return lc + 1; &#125; //解析配置文件，解析指定的url配置文件 //使用parseLine方法进行解析，未被实例化的服务提供者会被保存到缓存中去 private Iterator&lt;String&gt; parse(Class&lt;?&gt; service, URL u) throws ServiceConfigurationError &#123; InputStream in = null; BufferedReader r = null; ArrayList&lt;String&gt; names = new ArrayList&lt;&gt;(); try &#123; in = u.openStream(); r = new BufferedReader(new InputStreamReader(in, &quot;utf-8&quot;)); int lc = 1; while ((lc = parseLine(service, u, r, lc, names)) &gt;= 0); &#125; return names.iterator(); &#125; //服务提供者查找的迭代器 private class LazyIterator implements Iterator&lt;S&gt; &#123; Class&lt;S&gt; service;//服务提供者接口 ClassLoader loader;//类加载器 Enumeration&lt;URL&gt; configs = null;//保存实现类的url Iterator&lt;String&gt; pending = null;//保存实现类的全名 String nextName = null;//迭代器中下一个实现类的全名 private LazyIterator(Class&lt;S&gt; service, ClassLoader loader) &#123; this.service = service; this.loader = loader; &#125; private boolean hasNextService() &#123; if (nextName != null) &#123; return true; &#125; if (configs == null) &#123; try &#123; String fullName = PREFIX + service.getName(); if (loader == null) configs = ClassLoader.getSystemResources(fullName); else configs = loader.getResources(fullName); &#125; &#125; while ((pending == null) || !pending.hasNext()) &#123; if (!configs.hasMoreElements()) &#123; return false; &#125; pending = parse(service, configs.nextElement()); &#125; nextName = pending.next(); return true; &#125; private S nextService() &#123; if (!hasNextService()) throw new NoSuchElementException(); String cn = nextName; nextName = null; Class&lt;?&gt; c = null; try &#123; c = Class.forName(cn, false, loader); &#125; if (!service.isAssignableFrom(c)) &#123; fail(service, &quot;Provider &quot; + cn + &quot; not a subtype&quot;); &#125; try &#123; S p = service.cast(c.newInstance()); providers.put(cn, p); return p; &#125; &#125; public boolean hasNext() &#123; if (acc == null) &#123; return hasNextService(); &#125; else &#123; PrivilegedAction&lt;Boolean&gt; action = new PrivilegedAction&lt;Boolean&gt;() &#123; public Boolean run() &#123; return hasNextService(); &#125; &#125;; return AccessController.doPrivileged(action, acc); &#125; &#125; public S next() &#123; if (acc == null) &#123; return nextService(); &#125; else &#123; PrivilegedAction&lt;S&gt; action = new PrivilegedAction&lt;S&gt;() &#123; public S run() &#123; return nextService(); &#125; &#125;; return AccessController.doPrivileged(action, acc); &#125; &#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125; //获取迭代器 //返回遍历服务提供者的迭代器 //以懒加载的方式加载可用的服务提供者 //懒加载的实现是：解析配置文件和实例化服务提供者的工作由迭代器本身完成 public Iterator&lt;S&gt; iterator() &#123; return new Iterator&lt;S&gt;() &#123; //按照实例化顺序返回已经缓存的服务提供者实例 Iterator&lt;Map.Entry&lt;String,S&gt;&gt; knownProviders = providers.entrySet().iterator(); public boolean hasNext() &#123; if (knownProviders.hasNext()) return true; return lookupIterator.hasNext(); &#125; public S next() &#123; if (knownProviders.hasNext()) return knownProviders.next().getValue(); return lookupIterator.next(); &#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; &#125;; &#125; //为指定的服务使用指定的类加载器来创建一个ServiceLoader public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service, ClassLoader loader) &#123; return new ServiceLoader&lt;&gt;(service, loader); &#125; //使用线程上下文的类加载器来创建ServiceLoader public static &lt;S&gt; ServiceLoader&lt;S&gt; load(Class&lt;S&gt; service) &#123; ClassLoader cl = Thread.currentThread().getContextClassLoader(); return ServiceLoader.load(service, cl); &#125; //使用扩展类加载器为指定的服务创建ServiceLoader //只能找到并加载已经安装到当前Java虚拟机中的服务提供者，应用程序类路径中的服务提供者将被忽略 public static &lt;S&gt; ServiceLoader&lt;S&gt; loadInstalled(Class&lt;S&gt; service) &#123; ClassLoader cl = ClassLoader.getSystemClassLoader(); ClassLoader prev = null; while (cl != null) &#123; prev = cl; cl = cl.getParent(); &#125; return ServiceLoader.load(service, prev); &#125; public String toString() &#123; return &quot;java.util.ServiceLoader[&quot; + service.getName() + &quot;]&quot;; &#125;&#125; 首先，ServiceLoader实现了Iterable接口，所以它有迭代器的属性，这里主要都是实现了迭代器的hasNext和next方法。这里主要都是调用的lookupIterator的相应hasNext和next方法，lookupIterator是懒加载迭代器。 其次，LazyIterator中的hasNext方法，静态变量PREFIX就是”META-INF/services/”目录，这也就是为什么需要在classpath下的META-INF/services/目录里创建一个以服务接口命名的文件。 最后，通过反射方法Class.forName()加载类对象，并用newInstance方法将类实例化，并把实例化后的类缓存到providers对象中，(LinkedHashMap&lt;String,S&gt;类型）然后返回实例对象 4.4. SPI缺陷 通过上面的解析，可以发现，我们使用SPI机制的缺陷： 不能按需加载，需要遍历所有的实现，并实例话，然后在循环中才能找到我们需要的实现，如果不像用某些实现类，或则某些类实例化很耗时，它也被载入实例化，造成浪费 获取某个实现类的方式不够灵活，只能通过Iterator形式获取，不能根据某个参数来获取对应的实现类 多个并发多线程使用ServiceLoader类的示例是不安全的 参考文章：Java常用机制 - SPI机制详解","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"Java机制","slug":"Java机制","permalink":"https://xiaoyuge5201.github.io/tags/Java%E6%9C%BA%E5%88%B6/"}]},{"title":"Redis持久化之AOF（八）","slug":"redis-special-aof","date":"2023-03-07T13:31:16.000Z","updated":"2023-03-07T13:31:16.000Z","comments":false,"path":"redis-special-aof/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-aof/index.html","excerpt":"","text":"1. 前言 Redis是先执行命令，把数据写入内存，然后才记录日志。日志里记录的是Redis收到的每一条命令，这些命令是以文本形式保存。 Redis是&quot;写后&quot;日志， 而大多数的数据库采用的是写前日志(WAL),例如Mysql，通过写前日志和两阶段提交，实现数据和逻辑的一致性。 AOF日志采用写后日志，即：先写内存，后写日志 为什么采用写后日志？ Redis要求高性能，采用后写日志有两方面的好处： 避免额外的检查开销：Redis向AOF里面记录日志的时候，并不会先去会这些命令进行语法检查，如果先记日志再执行命令，日志可能会记录错误的命令，在使用日志恢复数据的时候，就可能会报错。 不会阻塞当前的写操作 但是这种方式潜在的风险： 如果命令执行完成，写日志之前宕机了，会丢失数据 主线程写磁盘压力大，导致写磁盘满，阻塞后续操作 2. 如何实现AOF AOF 日志记录Redis的每个写命令，步骤分为：命令追加append、文件写入write和文件同步sync 命令追加：当AOF持久功能打开后，服务器在执行完一个命令之后，会以协议格式将被执行的写命令追加到服务器的aof_buf缓冲区 文件写入和同步：关于何时将aof_buf缓冲区的内容写入AOF文件中，redis提供了3中写回策略： 配置项 写回时机 优点 缺点 Always 同步写回 可靠性高，基本不会丢失 每个写命令都要落盘，性能影响大 Everysec 每秒写回 性能适中 宕机时丢失1秒内的数据 No 操作系统控制的写回 性能好 宕机时丢失的数据较多 Always: 同步写回：每个写命令执行完，立马同步地将日志写回磁盘 Everysec: 每秒写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘 No: 操作系统控制的写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 2.1 三种写回策略的优缺点 上面的三种写回策略体现了一个重要原则：trade-off取舍，在性能和可靠性之间做取舍 关于AOF的同步策略是涉及到操作系统的 write 函数和 fsync 函数的，在《Redis设计与实现》中是这样说明的： 为了提高文件写入效率，在现代操作系统中，当用户调用write函数，将一些数据写入文件时，操作系统通常会将数据暂存到一个内存缓冲区里，当缓冲区的空间被填满或超过了指定时限后，才真正将缓冲区的数据写入到磁盘里。 这样的操作虽然提高了效率，但也为数据写入带来了安全问题：如果计算机停机，内存缓冲区中的数据会丢失。为此，系统提供了fsync、fdatasync同步函数，可以强制操作系统立刻将缓冲区中的数据写入到硬盘里，从而确保写入数据的安全性。 3. redis.conf配置AOF 默认情况下，Redis是没有开启AOF的，可以通过配置redis.conf文件来开启AOF持久化，关于AOF的配置如下： 1234567891011121314151617181920212223242526# appendonly参数开启AOF持久化appendonly no# AOF持久化的文件名，默认是appendonly.aofappendfilename &quot;appendonly.aof&quot;# AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的dir ./# 同步策略# appendfsync alwaysappendfsync everysec# appendfsync no# aof重写期间是否同步no-appendfsync-on-rewrite no# 重写触发配置auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# 加载aof出错如何处理aof-load-truncated yes# 文件重写策略aof-rewrite-incremental-fsync yes 以下是Redis中关于AOF的主要配置信息： appendonly：默认情况下AOF功能是关闭的，将该选项改为yes以便打开Redis的AOF功能。 appendfilename：这个参数项很好理解了，就是AOF文件的名字。 appendfsync：这个参数项是AOF功能最重要的设置项之一，主要用于设置“真正执行”操作命令向AOF文件中同步的策略。什么叫“真正执行”呢？还记得Linux操作系统对磁盘设备的操作方式吗？ 为了保证操作系统中I/O队列的操作效率，应用程序提交的I/O操作请求一般是被放置在linux Page Cache中的，然后再由Linux操作系统中的策略自行决定正在写到磁盘上的时机。而Redis中有一个fsync()函数，可以将Page Cache中待写的数据真正写入到物理设备上，而缺点是频繁调用这个fsync()函数干预操作系统的既定策略，可能导致I/O卡顿的现象频繁 。与上节对应，appendfsync参数项可以设置三个值，分别是：always、everysec、no，默认的值为everysec。 no-appendfsync-on-rewrite：always和everysec的设置会使真正的I/O操作高频度的出现，甚至会出现长时间的卡顿情况，这个问题出现在操作系统层面上，所有靠工作在操作系统之上的Redis是没法解决的。为了尽量缓解这个情况，Redis提供了这个设置项，保证在完成fsync函数调用时，不会将这段时间内发生的命令操作放入操作系统的Page Cache（这段时间Redis还在接受客户端的各种写操作命令）。 auto-aof-rewrite-percentage：上文说到在生产环境下，技术人员不可能随时随地使用“BGREWRITEAOF”命令去重写AOF文件。所以更多时候我们需要依靠Redis中对AOF文件的自动重写策略。Redis中对触发自动重写AOF文件的操作提供了两个设置：auto-aof-rewrite-percentage表示如果当前AOF文件的大小超过了上次重写后AOF文件的百分之多少后，就再次开始重写AOF文件。例如该参数值的默认设置值为100，意思就是如果AOF文件的大小超过上次AOF文件重写后的1倍，就启动重写操作。 auto-aof-rewrite-min-size：参考auto-aof-rewrite-percentage选项的介绍，auto-aof-rewrite-min-size设置项表示启动AOF文件重写操作的AOF文件最小大小。如果AOF文件大小低于这个值，则不会触发重写操作。注意，auto-aof-rewrite-percentage和auto-aof-rewrite-min-size只是用来控制Redis中自动对AOF文件进行重写的情况，如果是技术人员手动调用“BGREWRITEAOF”命令，则不受这两个限制条件左右。# 4. 深入理解AOF重写 AOF会记录每个写命令到AOF文件，随着时间越长，AOF文件会越来越大，如果不加以控制，会对Redis服务器，甚至操作系统造成影响，而且AOF文件越大，数据恢复越慢， 为了解决AOF文件体积膨胀的问题，Redis提供AOF文件重写机制来对AOF文件进行&quot;瘦身&quot;。 4.1. 图例 Redis通过创建一个新的AOF文件来替换现有的AOF，新旧两个AOF文件保存的数据相同，但新的文件没有冗余的命令 4.2. AOF重写会阻塞吗？ AOF重写过程是由后台进程bgrewriteaof来完成的。 主线程fork出后台的bgrewriteaof子进程，fork会把主线程的内存拷贝一份给bgrewriteaof子进程， 这里面就包含来数据库的最新数据，然后，bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 所以aof在重写是，在fork进程时会阻塞主线程的。 4.3. AOF日志何时会重写 有两个配置项控制AOF重写的触发： auto-aof-rewrite-min-size: 表示运行AOF重写时文件的最小大小，默认是64MB auto-aof-rewrite-percentage: 这个值的计算方式是，当前aof文件大小和上一次重写aof文件的大小差值，再除以上一次重写后的aof文件大小，也就是当前aof文件比上一次重写aof文件的增量大小，和上一次重写后aof后文件大小的比值 1简单公式：(current - before) / before 4.4. 重写日志时，有新的数据写入 重写过程总结为：“一个拷贝，两次日志”。在fork出子进程时的拷贝，以及在重写时，如果有新的数据写入。主线程就会将命令记录到两个aof日志内存缓冲区中，如果AOF写回策略配置的时always，则直接将命令写回旧的日志文件，并且保存一份命令至AOF重写缓冲区，这些操作对新的日志文件时不存在影响的。 旧日志文件：主线程使用的日志文件 新日志文件：bgrewriteaof进程使用的日志文件 而在bgrewriteaof子进程完成日志重写操作后，会提示主线程，主线程会将AOF重写缓冲区中的命令追加到新的日志文件后面，这时在高并发的情况下，AOF重写缓冲区积累可能会很大，这样就会造成阻塞， Redis后来通过Linux管道技术让aof重写期间就能同时进行回放，这样aof重写结束后只需回放少量剩余的数据即可。 最后通过修改文件名的方式，保证文件切换的原子性。 在AOF重写日志期间发生宕机的话，因为日志文件还没有切换，所以恢复数据时，用的还是旧的日志文件。 总结操作： 主线程fork出子进程重写aof日志 子进程重写日志完成后，主线程追加aof日志缓冲 替换日志文件 温馨提示： 这里的进程和线程的概念有点混乱。因为后台的bgreweiteaof进程就只有一个线程在操作，而主线程是Redis的操作进程，也是单独一个线程。这里想表达的是Redis主进程在fork出一个后台进程之后，后台进程的操作和主进程是没有任何关联的，也不会阻塞主线程 4.5. 主线程fork出子进程的是如何复制内存数据的？ fork采用操作系统提供的写时复制（copy on write）机制，就是为了避免一次性拷贝大量内存数据给子进程造成阻塞。fork子进程时，子进程时会拷贝父进程的页表，即虚实映射关系（虚拟内存和物理内存的映射索引表），而不会拷贝物理内存。这个拷贝会消耗大量cpu资源，并且拷贝完成前会阻塞主线程，阻塞时间取决于内存中的数据量，数据量越大，则内存页表越大。拷贝完成后，父子进程使用相同的内存地址空间。 但主进程是可以有数据写入的，这时候就会拷贝物理内存中的数据。如下图（进程1看做是主进程，进程2看做是子进程）： 在主进程有数据写入时，而这个数据刚好在页c中，操作系统会创建这个页面的副本（页c的副本），即拷贝当前页的物理数据，将其映射到主进程中，而子进程还是使用原来的的页c。 4.6. 在重写日志整个过程时，主线程有哪些地方会被阻塞？ fork子进程时，需要拷贝虚拟页表，会对主线程阻塞。 主进程有bigkey写入时，操作系统会创建页面的副本，并拷贝原有的数据，会对主线程阻塞。 子进程重写日志完成后，主进程追加aof重写缓冲区时可能会对主线程阻塞。 4.7. 为什么AOF重写不复用原AOF日志？ 父子进程写同一个文件会产生竞争问题，影响父进程的性能。 如果AOF重写过程中失败了，相当于污染了原本的AOF文件，无法做恢复数据使用。 5. RDB和AOF混合方式（4.0版本） Redis4.0帮本提出了一个混合使用AOF日志和内存快照的方法，简单来说，内存快照以一定的频率执行，在两次快照期间，使用AOF日志记录这期间的所有命令操作。 这样依赖，快照不用很频繁的执行，这就避免了频繁fork对主线程的影响，而且AOF日志只用记录两次快照间的操作，不需要记录所有的操作，避免来文件过大的情况，也避免来重写开销。 如下图所示，T1和T2时刻的修改，用AOF日志记录，等到第二次做全量快照时，就可以清空AOF日志，因为此时的修改已经记录到快照中，恢复时就不再用日志来。 这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势, 实际环境中用的很多。 6. 从持久化中恢复数据 数据的备份、持久化做完来，我们如何从这些持久化文件中恢复数据呢？如果一台服务器上既有RDB文件，又有AOF文件，该加载谁呢？ 其实想要从这些文件中恢复数据，只需要重新启动Redis即可。通过以下图了解流程： redis重启时判断是否开启aof，如果开启了aof，那么就优先加载aof文件； 如果aof存在，那么就去加载aof文件，加载成功的话redis重启成功，如果aof文件加载失败，那么会打印日志表示启动失败，此时可以去修复aof文件后重新启动； 若aof文件不存在，那么redis就会转而去加载rdb文件，如果rdb文件不存在，redis直接启动成功； 如果rdb文件存在就会去加载rdb文件恢复数据，如加载失败则打印日志提示启动失败，如加载成功，那么redis重启成功，且使用rdb文件恢复数据； 那么为什么会优先加载AOF呢？因为AOF保存的数据更完整，通过上面的分析我们知道AOF基本上最多损失1s的数据。 7. 性能与实践 RDB的快照、AOF的重写都需要fork，这是一个重量级操作，会对Redis造成阻塞。因此为了不影响Redis主进程响应，我们需要尽可能降低阻塞。 降低fork的频率，比如可以手动来触发RDB生成快照、与AOF重写； 控制Redis最大使用内存，防止fork耗时过长； 使用更牛逼的硬件； 合理配置Linux的内存分配策略，避免因为物理内存不足导致fork失败。 在线上我们到底该怎么做？我提供一些自己的实践经验。 如果Redis中的数据并不是特别敏感或者可以通过其它方式重写生成数据，可以关闭持久化，如果丢失数据可以通过其它途径补回； 自己制定策略定期检查Redis的情况，然后可以手动触发备份、重写数据； 单机如果部署多个实例，要防止多个机器同时运行持久化、重写操作，防止出现内存、CPU、IO资源竞争，让持久化变为串行； 可以加入主从机器，利用一台从机器进行备份处理，其它机器正常响应客户端的命令； RDB持久化与AOF持久化可以同时存在，配合使用 感谢原博文Redis进阶 !!!","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"缓存穿透、击穿、雪崩、分布式锁","slug":"redis-special-application-problems","date":"2023-02-26T12:20:07.000Z","updated":"2023-02-26T12:20:07.000Z","comments":false,"path":"redis-special-application-problems/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-application-problems/index.html","excerpt":"","text":"1. 缓存穿透 缓存穿透是访问一个不存在的key，缓存不起作用，请求会穿透到DB，流量大时DB会挂掉。缓存击穿是访问一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。 比如用一个不存在的用户ID获取用户信息，不论数据库还是缓存都没有，如果大量的攻击会导致数据库崩溃 1.1. 解决方法 对空值缓存 如果查询返回的数据为空（不管数据库是否存在），仍然把这个结果（null）进行缓存，给其设置一个很短的过期时间（30秒） 设置可访问的名单（白名单） 使用redis中的bitmaps类型定一个可访问的名单，名单id作为偏移量，每次访问和bitmaps里面进行比较，如果访问的ID不存在，不允许访问 采用布隆过滤器 布隆过滤器：实际上是一个很长的二进制向量（位图）和一系列随机映射函数（哈希函数） 布隆过滤器可以用于检测一个元素是否在一个集合中，它的优点是空间效率和查询的时间都远超过一般的算法，缺点是有一定的误判和删除困难。将所有可能存在的数据哈希到一个足够到的bitmaps中，一个一定不存在的数据会被这个bitmaps拦截掉，从而避免了对底层存储系统的查询压力。 进行实时监控 当发现redis的命中率降低，需要排查访问对象和访问的数据，和运维人员配合可以设置黑名单限制对其提供服务（IP黑名单等） 2. 缓存击穿 redis中某个热点key过期，此时大量的请求同时过来，发现没有命中缓存，请求都打到了db上，导致db压力瞬间大增，可能会造成数据库崩溃 缓存击穿出现的现象： 数据库访问压力瞬间增大 redis里面没有出现大量的key过期 redis正常运行 2.1. 解决方法 预先设置热门数据，适时调整过期时间 在redis高峰之前，把一些热门数据提前存入到redis里面，对缓存中的热门数据进行监控，实时调整过期时间 使用锁 缓存中拿不到数据时，此时不是立即取db中查询，而是去获取分布式锁(如redis中的setnx)，拿到了锁再去db获取数据，没有拿到锁的线程休眠一段时间再重试获取数据的方法 3. 缓存雪崩 key对应的数据存在，但是极短的时间内有大量的key集中过期，此时若有大量的并发请求过来，发现缓存没有数据，大量的请求会落到db上去加载数据，导致数据库服务器崩溃 缓存雪崩和缓存击穿的区别在于：前者时大量的key集中过期，后者时某个热点key过期 3.1. 解决方案 构建多级缓存 nginx缓存 + redis 缓存 + 其他缓存(ehcache等) 使用锁或者队列 加锁或者队列的方式来保证不及有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上，不适用高并发的情况 监控缓存过期，提前更新 监控缓存，发现缓存快过期来，提前对缓存进行更新 缓存失效时间分散 在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样缓存的过期时间重复率就会降低，减少集体失效的事件 4. 分布式锁 随着业务发展的需要，原单体单机部署的系统被演化为分布式集群系统后，由于分布式系统多线程、多进程且分布在不同的机器上，这使原单机部署情况下的并发控制锁策略失效 单纯的Java API并不能提供分布式锁的能力，为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题 4.1. 分布式锁主流的实现方案 基于数据库实现分布式锁 基于缓存（redis等） 基于zookeeper 每一种分布式锁解决方案各有优点 性能：redis最高 可靠性：zookeeper最高 4.2. 使用redis实现分布式锁 需要使用下面这个命令来实现分布式锁： 1234set key value NX PX &lt;times&gt;# time：有效期（毫秒）# 当key不存在的时候，设置其值为value，并且同时设置有效期 NX：当数据库中key不存在时，可以将key-value添加到数据库 XX：当数据库中key存在时，可以将key-value添加数据库，与NX参数互斥 EX：key的超时秒数 PX：key的超时毫秒数，与EX互斥 示例： 1set user:1:info &quot;ok&quot; NX PX 10000 1. 上锁的过程 执行set key value NX PX 有效期(毫秒) 命令，返回OK表示执行成功，获取锁成功，多个客户端并发执行此命令时，redis可以保证只有一个执行成功 2. 为什么要设置过期时间? 客户端获取锁后，由于系统问题，如果宕机来，会导致锁无法释放（死锁），其他客户端就无法获取锁，所以要指定一个过期时间 3. 设置有效期时间控制 比如有效期设置10秒，但是业务处理时间大于10s，导致还没有处理完，锁就释放了，其他客户端就会进来，这种情况需要引入看门狗机制来解决这个问题 4. 解决锁误删的问题 锁存在误删的情况：所谓误删就是自己把别人持有的锁给删掉了。 比如线程A获取锁的时候，设置的有效期是10秒，但是执行业务的时候，A程序突然卡主了超过了10秒，此时这个锁就可能被其他线程拿到，比如被线程B拿到了，然后A从卡顿中恢复了，继续执行业务，业务执行完毕之后，去执行了释放锁的操作，此时A会执行del命令，此时就出现了锁的误删，导致的结果就是把B持有的锁给释放了，然后其他线程又会获取这个锁，挺严重的。 解决方法：获取锁的之前，生成一个全局唯一id，将这个id也丢到key对应的value中，释放锁之前，从redis中将这个id拿出来和本地的比较一下，看看是不是自己的id，如果是的再执行del释放锁的操作 5. 还是存在误删的可能（原子操作问题） 刚才上面说了，del之前，会先从redis中读取id，然后和本地id对比一下，如果一致，则执行删除，伪代码如下 12step1:判断 redis.get(&quot;key&quot;).id==本地id 是否相当,如果是则执行step2step2:del key; 此时如果执行step2的时候系统卡主了，比如卡主了10秒，然后redis才收到，这个期间锁可能又被其他线程获取了，此时又发生了误删的操作。 这个问题的根本原因是：判断和删除这2个步骤对redis来说不是原子操作导致的，这个时候就需要使用Lua脚本来解决。 6. Lua脚本来释放锁 将复杂的或者多步的redis操作，写为一个脚本，一次提交给redis执行，减少反复连接redis的次数，提升性能。Lua脚本类似于redis事务，有一定的原子性，不会被其他命令插队，可以完成一些redis事务的操作。 但是注意redis的LUA脚本功能，只能在redis2.6以上版本才能使用。 1234567891011121314151617181920212223242526272829303132333435import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.core.script.DefaultRedisScript;import org.springframework.http.MediaType;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;import java.util.Arrays;import java.util.UUID;import java.util.concurrent.TimeUnit;@RestControllerpublic class LockTest &#123; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @RequestMapping(value = &quot;/lock&quot;, produces = MediaType.TEXT_PLAIN_VALUE) public String lock() &#123; String lockKey = &quot;k1&quot;; String uuid = UUID.randomUUID().toString(); //1.获取锁,有效期10秒 if (this.redisTemplate.opsForValue().setIfAbsent(lockKey, uuid, 10, TimeUnit.SECONDS)) &#123; //2.执行业务 // todo 业务 //3.使用Lua脚本释放锁(可防止误删) String script = &quot;if redis.call(&#x27;get&#x27;,KEYS[1])==ARGV[1] then return redis.call(&#x27;del&#x27;,KEYS[1]) else return 0 end&quot;; DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptText(script); redisScript.setResultType(Long.class); Long result = redisTemplate.execute(redisScript, Arrays.asList(lockKey), uuid); System.out.println(result); return &quot;获取锁成功!&quot;; &#125; else &#123; return &quot;加锁失败!&quot;; &#125; &#125;&#125; 7. 分布式锁总结 为了确保分布式锁可用，至少需要确保分布式锁的实现同时满足以下4个条件： 互斥性：在任意时刻只能有一个客户端持有锁 不会死锁：即有一个客户端在持有锁期间崩溃而没有释放锁，也能够保证后续其他客户端能够获取锁 加锁和解锁必须时同一个客户端，客户端不能把别人的锁解除了 加锁和解锁必须有原子性 5. 分布式锁实现Demo 引入pom 123456 &lt;!-- Redis客户端 start --&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; Controller层 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 车主端 确认支付 * * @param requestDto * @param rpcPacket * @return */ @UrlMapping(url = &quot;carOwner/confirmPayment&quot;) public RpcPacket changeConfirmPaymentForCarOwner(RpcCarOwnerConfirmPaymentReqDto requestDto, RpcPacket rpcPacket) &#123; logger.info(&quot;车主端 确认支付,requestDto &#123;&#125;,rpcPacket &#123;&#125;&quot;, JsonUtil.toJson(requestDto), JsonUtil.toJson(rpcPacket)); RpcPacket packet = new RpcPacket(); String lockKey = OrderServiceModulConstant.LOCK_CAROWNER_COMFIRM_PAY_LOKENAME + requestDto.getOrderId(); String lockName = &quot;&quot;; try &#123; //先分布锁一下 订单 lockName = redisCache.getLockLua(lockKey, OrderServiceModulConstant.LOCK_CAROWNER_COMFIRM_PAY_EXPIRETIME, OrderServiceModulConstant.LOCK_CAROWNER_COMFIRM_PAY_TIMEOUT); if (!StringUtil.isEmpty(lockName)) &#123; Long oldCouponId = orderFeeService.updateBindCounpByOrderId(requestDto);// if (oldCouponId!=null) &#123;// rpcServie.updateUnBindCouponByCouponId(oldCouponId);// &#125; RpcCarOwnComfirmPayRetDto res = orderFeeService.changeConfirmPaymentForCarOwner(requestDto, rpcPacket); packet.setData(res); if (res != null) &#123; switch (res.getStatus()) &#123; case 1: packet.setAnwserCode(new AnwserCode(1, &quot;车主端 支付成功&quot;)); break; case 2: packet.setAnwserCode(new AnwserCode(1, &quot;车主端 请调起三方支付&quot;)); break; case 3: packet.setAnwserCode(new AnwserCode(-2, &quot;车主端 支付失败&quot;)); break; &#125; &#125; &#125; else &#123; throw new ArgsException(&quot;支付处理中，请勿重复提交&quot;); &#125; &#125; catch (ArgsException e) &#123; logger.error(&quot;车主端 确认支付异常 &#123;&#125;&quot;, e.toString()); packet.setAnwserCode(e.getAnwserCode()); &#125; catch (Exception e) &#123; logger.error(&quot;车主端 确认支付异常 &#123;&#125;&quot;, e.toString()); packet.setAnwserCode(OrderServiceAnwserCode.BUSS_ERROR_CALCULPAYMENT_CAROWNER); &#125; finally &#123; redisCache.releaseLock(lockKey, lockName); &#125; logger.info(&quot;车主端 确认支付,packet &#123;&#125;&quot;, JsonUtil.toJson(packet)); return packet; &#125; 3 . 编写工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754755756757758759760761762763764765766767768769770771772773774775776777778779780781782783784785786787788789790791792793794795796797798799800801802803804805806807808809810811812813814815816817818819820821822823824825826827828829830831832833834835836837838839840841842843844845846847848849850851852853854855856857858859860861862863864865866867868869870871872873874875876877878879880881882883884885886887888889890891892893894895896897898899900901902903904905906907908909910911912913914915916917918919920921922923924925926927928929930931932933934935936937938939940941942943944945946947948949950951952953954955956957958959960961962963964965966967968969970971972973974975976977978979980981982983984985986987988989import org.apache.commons.lang3.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import redis.clients.jedis.*;import redis.clients.jedis.exceptions.JedisException;import redis.clients.jedis.params.geo.GeoRadiusParam;import java.io.IOException;import java.util.*;public class RedisCache &#123; private final static Logger logger = LoggerFactory.getLogger(RedisCache.class); private JedisCluster jedisCluster; private String prefixKey; @Autowired private RedisConfig redisConfig; public RedisCache(String prefixKey, RedisConfig redisConfig) &#123; this.prefixKey = prefixKey + &quot;_&quot;; this.redisConfig = redisConfig; this.init(); &#125; public void init() &#123; if (null == jedisCluster) &#123; if (StringUtils.isEmpty(prefixKey) || prefixKey.length() &lt;= 1) &#123; logger.error(&quot;初始化redisCache失败，该模块的redis的key为空&quot;); System.exit(0); &#125; //加锁 synchronized (RedisCache.class) &#123; if (null == jedisCluster) &#123; JedisPoolConfig poolConfig = new JedisPoolConfig(); Set&lt;HostAndPort&gt; shardInfoSet = new HashSet&lt;&gt;(16); try &#123; //配置 poolConfig.setMaxIdle(redisConfig.getMaxIdle()); poolConfig.setMinIdle(redisConfig.getMinIdle()); poolConfig.setTestOnReturn(redisConfig.getTestOnReturn()); poolConfig.setTestOnBorrow(redisConfig.getTestOnBorrow()); String[] shardList = redisConfig.getShared().split(&quot;;&quot;); for (String server : shardList) &#123; String[] values = server.split(&quot;:&quot;); HostAndPort node = new HostAndPort(values[0], Integer.parseInt(values[1])); shardInfoSet.add(node); &#125; int timeout = redisConfig.getTimeout() == null ? 2000 : redisConfig.getTimeout(); String password = redisConfig.getPassword(); Integer maxActive = redisConfig.getMaxActive(); //是否有密码 if (StringUtils.isNotBlank(password)) &#123; jedisCluster = new JedisCluster(shardInfoSet, timeout, timeout, maxActive, poolConfig); &#125; else &#123; jedisCluster = new JedisCluster(shardInfoSet, timeout, timeout, maxActive, password, poolConfig); &#125; &#125; catch (Exception e) &#123; logger.error(&quot;getSharedJedisPool&quot;, e); jedisCluster = null; throw e; &#125; &#125; &#125; &#125; &#125; /** * 以新换旧，设置新值同时返回旧值 * * @param key key * @param value 新值 * @return 旧值 */ public String getSet(String key, String value) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.getSet(realKey, value); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 判断key是否存在 * * @param key key * @return 存在与否 */ public Boolean exists(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.exists(realKey); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 删除key * * @param key key * @return 结果 */ public Boolean deleteKey(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.del(realKey) &gt; 0; &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 删除不含前缀的key * * @param key key */ public Boolean deleteKeyNoPrefixKey(String key) &#123; try &#123; return jedisCluster.del(key) &gt; 0; &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 设置带有过期时间的key-value * * @param key key * @param value 值 * @param seconds 有效时间 * @return 结果 */ public boolean set(String key, String value, Integer seconds) &#123; String realKey = this.prefixKey + key; try &#123; jedisCluster.setex(realKey, seconds, value); return true; &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 设置key-value * * @param key key * @param value 值 */ public boolean setNoExpire(String key, String value) &#123; String prefixKey = this.prefixKey + key; try &#123; jedisCluster.set(prefixKey, value); return true; &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 根据key获取value * * @param key key * @return 结果 */ public String get(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.get(realKey); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 查询key的过期时间还剩多少秒 * * @param key key * @return 生育过期时间 */ public Long ttl(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.ttl(realKey); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 设置过期时间 * * @param key key * @param seconds 过期时间 * @return 结果 */ public boolean cacheExpire(String key, Integer seconds) &#123; String realKey = this.prefixKey + key; try &#123; jedisCluster.expire(realKey, seconds); return true; &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 获取序列化对象 * * @param key key * @return 结果 */ public byte[] get(byte[] key) &#123; String realKey = this.prefixKey + new String(key); try &#123; return jedisCluster.get(realKey).getBytes(); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 自增1， value必须是数值型，否则报错 * * @param key key * @return 自增结果 */ public Long incr(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.incr(realKey); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 增加值， value必须是数值型，否则报错 * * @param key key * @param integer 增加的值 * @return 自增结果 */ public Long incrBy(String key, long integer) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.incrBy(realKey, integer); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 自减1， value必须是数值型，否则报错 * * @param key key * @return 自增结果 */ public Long decr(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.decr(realKey); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 减少值， value必须是数值型，否则报错 * * @param key key * @param integer 减少的值 * @return 减少后的结果 */ public Long decrBy(String key, long integer) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.decrBy(realKey, integer); &#125; catch (Exception ex) &#123; throw ex; &#125; &#125; /** * 新建LIST * * @param key * @param index * @param value * @return */ public boolean setList(String key, Long index, String value) &#123; String realKey = this.prefixKey + key; if (value == null) &#123; return false; &#125; try &#123; jedisCluster.lset(realKey, index, value); return true; &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 设置hash 字段-值 * * @param key 键 * @param field 字段 * @param value 值 * @return */ public boolean setHash(String key, String field, String value) &#123; String realKey = this.prefixKey + key; if (value == null) &#123; return false; &#125; try &#123; jedisCluster.hset(realKey, field, value); return true; &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 获得HashSet对象 * * @param key 键 * @param field 字段 * @return Json String or String value */ public String getHash(String key, String field) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hget(realKey, field); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 存储多个值-- 存储 map * * @param key key * @param map value集合 * @return */ public boolean hmset(String key, Map&lt;String, String&gt; map) &#123; String realKey = this.prefixKey + key; try &#123; jedisCluster.hmset(realKey, map); return true; &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 获取多个key对应的值 * * @param key * @param fields * @return */ public List&lt;String&gt; hmget(String key, String... fields) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hmget(realKey, fields); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 删除Hash 字段对象 * * @param key 键 * @param field 字段 * @return 删除的记录数 */ public long delHash(String key, String field) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hdel(realKey, field); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 删除Hash 多个字段对象 * * @param key 键值 * @param field 字段 * @return 删除的记录数 */ public long delHash(String key, String... field) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hdel(realKey, field); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 判断key下的field是否存在 * * @param key 键 * @param field 字段 * @return */ public boolean existsHash(String key, String field) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hexists(realKey, field); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 返回 key 指定的哈希集中所有字段的value值 * * @param key * @return */ public List&lt;String&gt; hvals(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hvals(realKey); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 列出所有的filed * hkeys key * * @param key key * @return */ public Set&lt;String&gt; hkeys(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hkeys(realKey); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 返回field的数量 * * @param key * @return */ public long lenHash(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.hlen(realKey); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * list 添加 * * @param key * @param strings */ public Long lpush(String key, String... strings) &#123; String realKey = this.prefixKey + key; return jedisCluster.lpush(realKey, strings); &#125; /** * list 获取长度 */ public long llen(String key) &#123; String realKey = this.prefixKey + key; return jedisCluster.llen(realKey); &#125; //redis 监听消息通道=========================== /** * 推入消息到redis消息通道 * * @param String channel * @param String message */ public Long publish(String channel, String message) &#123; return jedisCluster.lpush(channel, message); &#125; /** * 推入消息到redis消息通道 * * @param byte[] channel * @param byte[] message */ public Long publish(byte[] channel, byte[] message) &#123; return jedisCluster.lpush(channel, message); &#125; /** * 获取队列数据 * * @param byte[] key 键名 * @return */ public byte[] rpop(byte[] key) &#123; String realKey = new String(key); realKey = this.prefixKey + key; byte[] bytes = null; try &#123; bytes = jedisCluster.rpop(realKey.getBytes()); &#125; catch (Exception e) &#123; //释放redis对象 e.printStackTrace(); &#125; return bytes; &#125; /** * 获取队列数据 * * @param byte[] key 键名 * @return */ public String rpop(String key) &#123; String realKey = this.prefixKey + key; String bytes = null; try &#123; bytes = jedisCluster.rpop(realKey); &#125; catch (Exception e) &#123; //释放redis对象 e.printStackTrace(); &#125; return bytes; &#125; /** * 监听消息通道 * * @param jedisPubSub - 监听任务 * @param channels - 要监听的消息通道 * @throws IOException */ public void subscribe(BinaryJedisPubSub jedisPubSub, byte[]... channels) throws IOException &#123; try &#123; jedisCluster.subscribe(jedisPubSub, channels); &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; jedisCluster.close(); &#125; &#125; /** * 监听消息通道 * * @param jedisPubSub - 监听任务 * @param channels - 要监听的消息通道 * @throws IOException */ public void subscribe(JedisPubSub jedisPubSub, String... channels) throws IOException &#123; try &#123; jedisCluster.subscribe(jedisPubSub, channels); &#125; catch (Exception e) &#123; throw e; &#125; finally &#123; jedisCluster.close(); &#125; &#125; //redis 监听消息通道=========================== /** * 删除指定元素 * * @param key * @param count * @param value */ public boolean lrem(String key, int count, String value) &#123; String realKey = this.prefixKey + key; try &#123; jedisCluster.lrem(realKey, count, value); return true; &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * list 指定元素 0，-1 所有 * * @param key * @param start * @param end * @return */ public List&lt;String&gt; lrange(String key, int start, int end) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.lrange(realKey, start, end); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * add string to set * * @param key * @param members */ public boolean sadd(String key, String... members) &#123; String realKey = this.prefixKey + key; try &#123; jedisCluster.sadd(realKey, members); return true; &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * 判断集合中是否存在某个值 * * @param key * @param member * @return */ public boolean sismember(String key, String member) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.sismember(realKey, member); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * select all * * @param key * @return */ public Set&lt;String&gt; smembers(String key) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.smembers(realKey); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * delete value in set * * @param key * @param value */ public boolean srem(String key, String value) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.srem(realKey, value) &gt; 0; &#125; catch (Exception e) &#123; throw e; &#125; &#125; public String ltrim(String key, int start, int end) &#123; try &#123; return jedisCluster.ltrim(this.prefixKey + key, start, end); &#125; catch (Exception e) &#123; throw e; &#125; &#125; /** * score降序，获取指定索引范围的元素 * @param key * @param start * @param end * @return */ public Set&lt;String&gt; zrevrange(String key, long start, long end) &#123; String realKey = this.prefixKey + key; try &#123; return jedisCluster.zrevrange(realKey, start, end); &#125; catch (Exception e) &#123; throw e; &#125; &#125; public boolean setNx(String key, String obj, int seconds) &#123; Long result; try &#123; result = jedisCluster.setnx(key, obj); if (result == 1) &#123; jedisCluster.expire(key, seconds); return true; &#125; else &#123; return false; &#125; &#125; catch (Exception e) &#123; &#125; return false; &#125; /** * 添加元素---set * @param key * @param score * @param value * @return */ public Long zadd(String key, Long score, String value) &#123; String realKey = this.prefixKey + key; Long result = 0L; try &#123; result = jedisCluster.zadd(realKey, score.doubleValue(), value); &#125; catch (Exception e) &#123; &#125; return result; &#125; /** * 分页获取ZSET数据 * * @param key * @param start * @param end * @return */ public Set&lt;Tuple&gt; zrevrangeWithScores(String key, long start, long end) &#123; logger.info(&quot;RedisCache zrange key=&#123;&#125;,start=&#123;&#125;,end=&#123;&#125;&quot;, key, start, end); String realKey = this.prefixKey + key; try &#123; return jedisCluster.zrevrangeWithScores(realKey, start, end); &#125; catch (Exception e) &#123; logger.error(&quot;RedisCache zrange key=&#123;&#125;,start=&#123;&#125;,end=&#123;&#125;,e=&#123;&#125;&quot;, realKey, start, end, e); &#125; return Collections.emptySet(); &#125; /** * 获取对应元素的排名 * * @param key * @param member * @return */ public Long zrevrank(String key, String member) &#123; logger.info(&quot;RedisCache zrevrank key=&#123;&#125;,member=&#123;&#125;&quot;, key, member); String realKey = this.prefixKey + key; try &#123; return jedisCluster.zrevrank(realKey, member); &#125; catch (Exception e) &#123; logger.error(&quot;RedisCache zrevrank key=&#123;&#125;,member=&#123;&#125;,e =&#123;&#125;&quot;, key, member, e); &#125; return -1L; &#125; /** * 获取对应元素的分数 * * @param key * @param member * @return */ public Double zscore(String key, String member) &#123; logger.info(&quot;RedisCache zscore key=&#123;&#125;,member=&#123;&#125;&quot;, key, member); String realKey = this.prefixKey + key; try &#123; return jedisCluster.zscore(realKey, member); &#125; catch (Exception e) &#123; logger.error(&quot;RedisCache zscore key=&#123;&#125;,member=&#123;&#125;,e =&#123;&#125;&quot;, key, member, e); &#125; return 0.00; &#125; public Long zrem(String key, String value) &#123; String realKey = this.prefixKey + key; Long result = 0L; try &#123; result = jedisCluster.zrem(realKey, value); &#125; catch (Exception e) &#123; &#125; return result; &#125; /** * 加锁 * * @param key 锁的key * @param acquireTimeout 获取超时时间 * @param timeout 锁的超时时间 * @return 锁标识 */ public String lockWithTimeout(String key, long acquireTimeout, long timeout) &#123; String retIdentifier = null; try &#123; // 随机生成一个value String identifier = UUID.randomUUID().toString(); // 超时时间，上锁后超过此时间则自动释放锁 int lockExpire = (int) (timeout / 1000); // 获取锁的超时时间，超过这个时间则放弃获取锁 long end = System.currentTimeMillis() + acquireTimeout; while (System.currentTimeMillis() &lt; end) &#123; if (jedisCluster.setnx(key, identifier) == 1) &#123; jedisCluster.expire(key, lockExpire); // 返回value值，用于释放锁时间确认 retIdentifier = identifier; return retIdentifier; &#125; // 返回-1代表key没有设置超时时间，为key设置一个超时时间 if (jedisCluster.ttl(key) == -1) &#123; jedisCluster.expire(key, lockExpire); &#125; try &#123; Thread.sleep(10); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; catch (JedisException e) &#123; throw e; &#125; return retIdentifier; &#125; /** * 加锁 * 运用lua脚本 * * @param key 锁的key * @param acquireTimeout 获取超时时间 * @param timeout 锁的超时时间 * @return 锁标识 */ public String getLockLua(String key, long acquireTimeout, long timeout) &#123; return getLockLua(key, timeout); &#125; public String getLockLua(String key, long timeout) &#123; List&lt;String&gt; keys = new ArrayList&lt;&gt;(); int lockExpire = (int) (timeout / 1000); keys.add(key); List&lt;String&gt; args = new ArrayList&lt;&gt;(); args.add(lockExpire + &quot;&quot;); args.add(UUID.randomUUID().toString()); return &quot;OK&quot;.equals(jedisCluster.eval(&quot;return redis.call(&#x27;set&#x27;, KEYS[1],ARGV[2],&#x27;nx&#x27;, &#x27;ex&#x27;, ARGV[1]) &quot;, keys, args)) ? args.get(1) : &quot;&quot;; &#125; public String getLock(String key, long timeout) &#123; String retIdentifier = null; try &#123; // 随机生成一个value String identifier = UUID.randomUUID().toString(); // 超时时间，上锁后超过此时间则自动释放锁 int lockExpire = (int) (timeout / 1000); if (jedisCluster.setnx(key, identifier) == 1) &#123; jedisCluster.expire(key, lockExpire); // 返回value值，用于释放锁时间确认 retIdentifier = identifier; return retIdentifier; &#125; // 返回-1代表key没有设置超时时间，为key设置一个超时时间 if (jedisCluster.ttl(key) == -1) &#123; jedisCluster.expire(key, lockExpire); &#125; &#125; catch (JedisException e) &#123; throw e; &#125; return retIdentifier; &#125; /** * 释放锁 * * @param key 锁的key * @param identifier 释放锁的标识 * @return */ public boolean releaseLock(String key, String identifier) &#123; boolean retFlag = false; try &#123; // 通过前面返回的value值判断是不是该锁，若是该锁，则删除，释放锁 if (identifier.equals(jedisCluster.get(key))) &#123; jedisCluster.del(key); retFlag = true; &#125; &#125; catch (JedisException e) &#123; throw e; &#125; return retFlag; &#125; public Set&lt;String&gt; keys(String pattern) &#123; HashSet&lt;String&gt; keys = new HashSet&lt;String&gt;(); Map&lt;String, JedisPool&gt; clusterNodes = jedisCluster.getClusterNodes(); for (String node : clusterNodes.keySet()) &#123; JedisPool jp = clusterNodes.get(node); Jedis connection = jp.getResource(); try &#123; keys.addAll(connection.keys(pattern)); &#125; catch (Exception e) &#123; &#125; finally &#123; connection.close(); &#125; &#125; return keys; &#125; //订单开始坐标存入redis public Long addReo(double lon, double lat, String orderId) &#123; try &#123; return jedisCluster.geoadd(&quot;orderStation&quot;, lon, lat, orderId); &#125; catch (Exception e) &#123; logger.error(&quot;reids 缓存坐标异常：&quot;, e); &#125; return null; &#125; /** * 查询坐标系附近的订单 * * @param lon 经度 * @param lat 纬度 * @param radius 半径 * @return 结果 */ public List&lt;GeoRadiusResponse&gt; queryReo(Double lon, Double lat, double radius) &#123; try &#123; return jedisCluster.georadius(&quot;orderStation&quot;, lon, lat, radius, GeoUnit.KM, GeoRadiusParam.geoRadiusParam().withDist()); &#125; catch (Exception e) &#123; logger.error(&quot;reids 查询坐标系附近的订单异常：&quot;, e); &#125; return null; &#125; /** * redis删除订单坐标 * * @param orderId 订单号 * @return 结果 */ public Long delReo(String orderId) &#123; try &#123; return jedisCluster.zrem(&quot;orderStation&quot;, orderId); &#125; catch (Exception e) &#123; logger.error(&quot;reids 删除订单坐标异常：&quot;, e); &#125; return null; &#125; public JedisCluster getJedisCluster() &#123; return jedisCluster; &#125; &#125;","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Redis持久化之RDB（七）","slug":"redis-special-rdb","date":"2023-02-26T08:20:06.000Z","updated":"2023-02-26T08:20:06.000Z","comments":false,"path":"redis-special-rdb/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-rdb/index.html","excerpt":"","text":"1. 总体介绍 Redis是一个基于内存的数据库，它的数据时存放在内存中，内存有个问题就是关闭服务或者断电会丢失。 Redis的数据也支持写到硬盘中，这个过程就要持久化。Redis提供了4种不同的持久化方式： RDB（Redis Database） AOF（Append Of File） 虚拟内存（VM） --reids2.4已经弃用 DISKSTORE 2. RDB（Redis Database） 2.1. RDB介绍 在指定的时间间隔内将内存中的数据集快照（snapshot）写入磁盘，它恢复时是将快照文件直接读到内存里 2.2. 备份是如何执行的 Redis会单独创建(fork)一个子进程进行持久化，会将数据写入到一个临时文件中，待持久化过程都结束后， 再用这个临时文件替换上次持久化好的文件中，整个过程中主进程不进行任何IO操作，这就是确保了极高的性能。如果需要进行大规模的恢复，且对数据恢复的完整性不是非常敏感，那RDB方式要比AOF更加高效， RDB的缺点是最后一次持久化后的数据可能丢失。 2.3. Fork Fork的作用是复制一个与当前进程一样的进程，新进程的所有数据（变量、环境变量、程序计数器等）数值和原进程一致，它是一个全新的进程，并作为原进程的子进程 在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后会多次exec系统调用，出于效率考虑，Linux中引入了&quot;写时复制技术&quot;&quot; 一般情况弗进程和子进程会共用一段物理内存，只有进程空间的各段的内容要发生变化是，才会将父进程的内容复制一份给子进程。 2.4. RDB持久化流程 2.5. 指定备份文件的名称 在redis.conf中，可以修改rdb备份文件的名称(dbfilename)以及保存路径(dir)，默认为dump.rdb，如下 12345678910111213141516# sanitize-dump-payload no# The filename where to dump the DBdbfilename dump.rdb# The working directory.## The DB will be written inside this directory, with the filename specified# above using the &#x27;dbfilename&#x27; configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.# ./表示执行redis-server命令启动redis时所在的目录dir ./ 2.6. 触发RDB备份 方式1：自动备份 以下4中情况会自动触发： redis.conf中配置save m n，即在m秒内有n次修改时，自动触发bgsave生成rdb文件； save用来配置备份规则， 格式为：save 秒 写操作次数 ，默认是1分钟内修改了1万次，或5分钟内需修改了10次，或30分钟内修改了1次。 示例：设置20秒内有最少3次key发生备份，则进行备份 1save 20 3 主从复制时，从节点要从主节点进行全量复制时也会触发bgsave操作，生成当时的快照发送到从节点； 执行debug reload命令重新加载redis时也会触发bgsave操作； 默认情况下执行shutdown命令时，如果没有开启aof持久化，那么也会触发bgsave操作； 方式2：手动执行命令备份（save | bgsave） 触发备份有两个命令： save：阻塞当前Redis服务器，直到RDB过程完成为止，手动保存，不建议使用 bgsave：redis会在后台一步进行快照操作，快照同时还可以响应客户端情况，可以通过lastsave命令获取最后一次生成快照的时间 具体流程如下： redis客户端执行bgsave命令或者自动触发bgsave命令； 主进程判断当前是否已经存在正在执行的子进程，如果存在，那么主进程直接返回； 如果不存在正在执行的子进程，那么就fork一个新的子进程进行持久化数据，fork过程是阻塞的，fork操作完成后主进程即可执行其他操作； 子进程先将数据写入到临时的rdb文件中，待快照数据写入完成后再原子替换旧的rdb文件； 同时发送信号给主进程，通知主进程rdb持久化完成，主进程更新相关的统计信息（info Persitence下的rdb_*相关选项）。# 方式3： flushall命令 执行flushall命令，也会产生dump.rdb文件，但是里面是空的，无意义。 redis.conf其他的一些配置 stop-writes-on-bgsave-error：当磁盘满时，是否关闭redis的写操作,默认yes。 rdbcompression：rdb备份是否开启压缩 对于存储到磁盘中的rdb快照文件，可以设置是否进行压缩，如果是的话，redis采用LZF算法进行压缩，默认yes rdbchecksum：是否检查rdb备份文件的完整性 存储快照后，还可以让redis使用CRC64算法来进行数据校验，但是这样会加大性能消耗，如果希望获取最大的性能，可以关闭，默认 yes 2.7. rdb的备份和恢复 获取rdb文件的目录 123127.0.0.1:6379&gt; config get dir1) &quot;dir&quot;2) &quot;/usr/local/redis/src&quot; 将rdb的备份文件*.rdb文件拷贝到其他地方 12cd /usr/local/redis/srccp dump.rdb dump2.rdb rdb恢复 关闭redis 把备份的文件拷贝到工作目录/usr/local/redis/src，比如cp dump2.rdb dump.rdb 启动redis，备份数据直接加载，数据恢复 2.8. 优缺点 优势： 适合大规模数据恢复 对数据完整性和一致性要求不高更适合使用 节省磁盘空间 恢复速度块 劣势： Fork的时候，内存中的数据会被克隆一份，大致2倍的膨胀，需要考虑 虽然Redis在fork的时候使用了写时拷贝技术，但是如果数据庞大时还是比较消耗性能 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down的话，就会丢失最后一次快照后所有修改 2.9. 停止RDB 动态停止RDB：redis-cli config set save &quot;&quot; #save后给空值，表示禁用保存策略。 3. RDB深入理解 RDB中的核心思路是Copy-on-Write，来保证快照操作的时间，需要压缩写入磁盘上的数据在内存中不会发生便哈，在正常的快照操作中，一方面Redis主进程会fork一个新的快照进程专门处理这个事情，这样保证了Redis服务不会停止对 客户端包括写请求在哪的任何响应。另一方面这段时间发生的数据变化会以副本的方式存放在另一个新的内存区域，等到快照操作结束后才会同步到原来的内存区域。 举个例子：如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。 3.1. 快照间隔时间 对于快照来说，所谓“连拍”就是指连续地做快照。这样一来，快照的间隔时间变得很短，即使某一时刻发生宕机了，因为上一时刻快照刚执行，丢失的数据也不会太多。但是，这其中的快照间隔时间就很关键了 如果频繁地执行全量快照，也会带来两方面的开销： 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 4. RDB优缺点 优点： RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景； Redis加载RDB文件恢复数据要远远快于AOF方式； 缺点： RDB方式实时性不够，无法做到秒级的持久化； 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高； RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全； 版本兼容RDB文件问题；针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决#","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Redis事务操作（六）","slug":"redis-special-transaction","date":"2023-02-25T12:58:29.000Z","updated":"2023-02-25T12:58:29.000Z","comments":false,"path":"redis-special-transaction/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-transaction/index.html","excerpt":"","text":"1. Redis事务定义 Redis事务是一个单独的隔离操作，事务中的所有命令都会序列化、按顺序的执行，事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis事务的主要作用就是串联多个命令防止别的命令插队。 2. Multi、Exec、discard 从输入Multi命令开始，输入的命令都会依次进入命令队列中，但不会立即执行，直到输入Exec后，redis会将之前的命令依次执行。 组队的过程中可以通过discard放弃组队。 Redis事务分2个阶段： 组队阶段：只是将所有的命令加入队列 执行阶段：依次执行队列中的命令，在执行这些命令的过程中，不会被其他客户端发送的请求命令插队或者打断。 2.1 常用命令 multi： 标记一个事务块的开始 标记一个事务的开始，事务块内的多条命令会按照先后顺序被放进一个队列中，最后由exec命令原子性(atomic)的执行。 示例： 123456789101112131415127.0.0.1:6379&gt; multi #标记事务开始OK127.0.0.1:6379(TX)&gt; incr userId # 多条命令按顺序入队，返回值为QUEUED，表示这个命令加入队列了，还没有被执行。QUEUED127.0.0.1:6379(TX)&gt; incr userIdQUEUED127.0.0.1:6379(TX)&gt; incr userIdQUEUED127.0.0.1:6379(TX)&gt; pingQUEUED127.0.0.1:6379(TX)&gt; exec #执行1) (integer) 12) (integer) 23) (integer) 34) PONG #如果ping通了，返回pong exec：执行所有事务块内的命令 执行事务块内的所有命令 加入某个（某些）key正处于watch命令的监视之下，且事务块有和这个（这些）key相关的命令，那么exec命令只在这个(这些)key没有被其他命令所改动的情况下执行并生效，否则该事务被打断abort 当操作被打断是，返回空值nil 示例1： 1234567891011127.0.0.1:6379&gt; watch lock lock_times #watch监听多个key OK 127.0.0.1:6379&gt; multi #开启事务OK127.0.0.1:6379(TX)&gt; set lock &quot;hangz&quot; #设置值QUEUED127.0.0.1:6379(TX)&gt; incr lock_timesQUEUED127.0.0.1:6379(TX)&gt; exec #执行1) OK2) (integer) 1 示例2： 12345678910127.0.0.1:6379&gt; watch k1 k2OK127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 v1 #另外一个客户端执行了set k1 aaa，修改了k1的值QUEUED127.0.0.1:6379(TX)&gt; incr k2QUEUED127.0.0.1:6379(TX)&gt; exec # 因为k1被修改，事务执行失败(nil) discard：取消事务 取消事务，放弃执行事务块内的所有命令。 总是返回 OK 示例： 12345678127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; pingQUEUED127.0.0.1:6379(TX)&gt; set name xiaoyugeQUEUED127.0.0.1:6379(TX)&gt; discardOK 3. 事务的错误处理 情况一：组队中命令有误，导致所有命令取消执行 示例代码：事务中执行了3个set命令，而第3个set命令有问题，加入队列失败，最后在执行exec时，所有命令被取消执行 12345678910127.0.0.1:6379&gt; multi #开启一个事务块OK127.0.0.1:6379(TX)&gt; set name xiaoyuge QUEUED127.0.0.1:6379(TX)&gt; set age 18QUEUED127.0.0.1:6379(TX)&gt; set address #命令有问题，导致加入队列失败(error) ERR wrong number of arguments for &#x27;set&#x27; command127.0.0.1:6379(TX)&gt; exec #执行exec的时候，事务中所有命令都被取消(error) EXECABORT Transaction discarded because of previous errors. 情况二：组队没有问题，执行中部分成功部分失败 示例代码如下，事务中有3个命令，3个命令都入队列成功了，执行exec命令的时候，1和3命令成功了，第2个失败了 123456789101112131415127.0.0.1:6379&gt; multiOK127.0.0.1:6379(TX)&gt; set k1 v1QUEUED127.0.0.1:6379(TX)&gt; incr k1 #命令2：k1的值递增1，由于k1的值不是数字，执行的时候会失败的QUEUED127.0.0.1:6379(TX)&gt; set k2 v2QUEUED127.0.0.1:6379(TX)&gt; exec #执行命令，1和3命令成功，第2个失败了1) OK2) (error) ERR value is not an integer or out of range3) OK127.0.0.1:6379&gt; mget k1 k2 #查看k1和k2的值1) &quot;v1&quot;2) &quot;v2&quot; 4. 事务冲突的问题 例子： 1234你的账户中只有10000，有多个人使用你的账户，同时去参加双十一抢购一个请求想给金额减8000一个请求想给金额减5000一个请求想给金额减1000 3个请求同时看到的余额都是10000，大于操作金额，都去执行修改余额的操作，最后导致金额变成了-4000，这显然是有问题的。 4.1 悲观锁 悲观锁（Pessimistic Lock）：顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人拿到这个数据就会block直到它拿到锁。传统的关系型数据库里面就用到了很多这种锁机制，比如行锁、表锁、读锁、写锁等，都是在做操作之前先上锁。 4.2 乐观锁 乐观锁（Optimistic Lock）：顾名思义，就是很乐观，每次去那数据的时候都认为别人不会修改，所以不会上锁，但是在修改的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。redis就是使用这种check-and-set机制实现事务的。 4.3 watch监视 在执行multi之前，先执行watch key1 [key2 …]，可以监视一个或者多个key，若在事务的exec命令之前这些key对应的值被其他命令所改动了，那么事务中所有命令都将被打断，即事务所有操作将被取消执行。 窗口1示例： 123456789101112131415161718127.0.0.1:6379&gt; flushdbOK127.0.0.1:6379&gt; set balance 1000OK127.0.0.1:6379&gt; watch balance #监视balance，若balance在事务阶段被其他命令修改，事务执行将被取消OK127.0.0.1:6379&gt; multi #开启事务OK127.0.0.1:6379(TX)&gt; set name xiaoyuge #设置name的值为xiaoyugeQUEUED127.0.0.1:6379(TX)&gt; incrby balance 10 #将balance的值+10， 然后新开一个窗口执行窗口2的命令QUEUED127.0.0.1:6379(TX)&gt; exec #当窗口2的命令执行完了之后，在执行窗口1的事务(nil)127.0.0.1:6379&gt; get balance #窗口1的事务执行失败，因为窗口2修改了值，结果为1050&quot;1050&quot;127.0.0.1:6379&gt; get name #事务执行失败，name为空(nil) 窗口2示例： 1234127.0.0.1:6379&gt; incrby balance 50 #balance原子+50(integer) 1050127.0.0.1:6379&gt; get balance&quot;1050&quot; 窗口1中，对balance进行了监视，也就是说在执行watch balance命令之后，在exec命令之前，如果有其他请求对balance进行了修改，那么窗口1事务中所有的命令都会将会被取消执行。 窗口1watch balance后，由于此时窗口2对balance进行了修改，导致窗口1中事务所有命令被取消执行。 4.4 unwatch：取消监视 取消watch命令对所有的key的监视，如果在执行watch命令之后，exec命令或discard命令先被执行的话，那么就不需要再执行unwatch了。 因为EXEC命令会执行事务，因此 WATCH 命令的效果已经产生了；而 DISCARD 命令在取消事务的同时也会取消所有对 key 的监视，因此这两个命令执行之后，就没有必要执行 UNWATCH 了。 1234127.0.0.1:6379&gt; watch k1 k2OK127.0.0.1:6379&gt; unwatchOK 5. Redis事务的3大特性 单独的隔离操作 事务中的所有命令都会序列化、按顺序地执行，事务在执行过程中，不会被其他客户端发送来的命令请求所打断。 没有隔离级别的概念 队列中的命令没有提交（exec）之前，都不会实际被执行，因为事务提交前任何指令都不会被实际执行。 不能保证原子性 事务中如果有一条命令执行失败，后续的命令仍然会被执行，没有回滚。 如果在组队阶段，有1个失败了，后面都不会成功；如果在组队阶段成功了，在执行阶段有那个命令失败就这条失败，其他的命令则正常执行，不保证都成功或都失败。","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"SpringBoot整合Redis（五）","slug":"redis-special-springboot","date":"2023-02-25T11:39:59.000Z","updated":"2023-02-25T11:39:59.000Z","comments":false,"path":"redis-special-springboot/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-springboot/index.html","excerpt":"","text":"1. 引入Redis依赖 1234567891011121314151617181920212223&lt;parent&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;version&gt;2.3.5.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;!-- Redis依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 测试包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2. 配置Redis连接信息 在application.yaml配置文件中配置： 1234567spring: redis: host: localhost port: 6379 password: password timeout: 6000 #超时时间 database: 0 #第一个数据库，0-15 3. 配置启动类 123456@SpringBootApplicationpublic class App &#123; public static void main(String[] args) &#123; SpringApplication.run(App.class, args); &#125;&#125; 4. 使用RedisTemplate工具类操作Redis springboot中使用RedisTemplate来操作redis，需要在我们的bean中注入这个对象，代码如下： 123456789@Autowiredprivate RedisTemplate&lt;String, String&gt; redisTemplate;// 用下面5个对象来操作对应的类型this.redisTemplate.opsForValue(); //提供了操作string类型的所有方法this.redisTemplate.opsForList(); // 提供了操作list类型的所有方法this.redisTemplate.opsForSet(); //提供了操作set的所有方法this.redisTemplate.opsForHash(); //提供了操作hash表的所有方法this.redisTemplate.opsForZSet(); //提供了操作zset的所有方法 5.RedisTemplate操作Demo 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.test.context.junit4.SpringRunner;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Set;/** * SpringBoot集成Redis，测试常用操作命令 * @author xiaoyuge */@SpringBootTest(classes = App.class)@RunWith(SpringRunner.class)public class RedisTemplateTest &#123; @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @Test public void stringTest() &#123; //删除key this.redisTemplate.delete(&quot;name&quot;); //设置值 this.redisTemplate.opsForValue().set(&quot;name&quot;, &quot;xiaoyuge&quot;); //获取值 String name = this.redisTemplate.opsForValue().get(&quot;name&quot;); System.out.println(name); &#125; @Test public void listTest() &#123; this.redisTemplate.delete(&quot;names&quot;); this.redisTemplate.opsForList().rightPushAll(&quot;names&quot;, &quot;tom&quot;, &quot;chelly&quot;, &quot;xiaoyuge&quot;, &quot;fan&quot;); List&lt;String&gt; list = this.redisTemplate.opsForList().range(&quot;names&quot;, 0, -1); list.forEach(System.out::println); &#125; @Test public void setTest() &#123; this.redisTemplate.opsForSet().add(&quot;courses&quot;, &quot;java&quot;, &quot;php&quot;, &quot;node&quot;, &quot;js&quot;); //获取set集合中的值 Set&lt;String&gt; courseSet = this.redisTemplate.opsForSet().members(&quot;courses&quot;); courseSet.forEach(System.out::println); &#125; @Test public void hashTest() &#123; Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;() &#123;&#123; put(&quot;name&quot;, &quot;xiaoyuge&quot;); put(&quot;age&quot;, &quot;18&quot;); &#125;&#125;; this.redisTemplate.opsForHash().putAll(&quot;userMap&quot;, map); //获取值 Map&lt;Object, Object&gt; resultMap = this.redisTemplate.opsForHash().entries(&quot;userMap&quot;); System.out.println(resultMap); &#125; @Test public void zsetTest() &#123; this.redisTemplate.delete(&quot;mv&quot;); this.redisTemplate.opsForZSet().add(&quot;mv&quot;, &quot;赵丽颖&quot;, 960d); this.redisTemplate.opsForZSet().add(&quot;mv&quot;, &quot;袁冰妍&quot;, 98d); this.redisTemplate.opsForZSet().add(&quot;mv&quot;, &quot;刘亦菲&quot;, 100d); this.redisTemplate.opsForZSet().add(&quot;mv&quot;, &quot;杨超越&quot;, 80d); //获取全部的属性值 Set&lt;String&gt; mvs = this.redisTemplate.opsForZSet().range(&quot;mv&quot;, 0, -1); mvs.forEach(System.out::println); &#125;&#125;","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Jedis操作Redis6（四）","slug":"redis-special-jedis","date":"2023-02-25T09:07:27.000Z","updated":"2023-02-25T09:07:27.000Z","comments":false,"path":"redis-special-jedis/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-jedis/index.html","excerpt":"","text":"1. Jedis的用法 引入maven依赖 123456&lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt;&lt;/dependency&gt; 使用Redis的API操作redis Jedis工具类，这个类中包含了操作redis的所有方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182/** * 使用Jedis测试常用的Redis API * @author xiaoyuge */public class AppTest &#123; Jedis jedis; @Before public void before() &#123; this.jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); jedis.auth(&quot;password&quot;); //设置连接密码，如果没有省略 &#125; @After public void after() &#123; this.jedis.close(); &#125; /** * 测试Redis是否能连上 */ @Test public void ping() &#123; System.out.println(jedis.ping()); &#125; @Test public void listTest() &#123; jedis.rpush(&quot;courses&quot;, &quot;java&quot;, &quot;php&quot;, &quot;node&quot;, &quot;js&quot;, &quot;C++&quot;); List&lt;String&gt; courses = jedis.lrange(&quot;courses&quot;, 0, -1); for (String course : courses) &#123; System.out.print(course + &quot;,&quot;); &#125; //输出：java,php,node,js,C++ &#125; @Test public void setTest() &#123; jedis.sadd(&quot;users&quot;, &quot;tom&quot;, &quot;jack&quot;, &quot;lilei&quot;, &quot;lilei&quot;); Set&lt;String&gt; sets = jedis.smembers(&quot;users&quot;); for (String set : sets) &#123; System.out.print(set + &quot;,&quot;); &#125; //输出：lilei,tom,jack &#125; @Test public void hashTest() &#123; jedis.hset(&quot;user:1001&quot;, &quot;id&quot;, &quot;1001&quot;); jedis.hset(&quot;user:1001&quot;, &quot;name&quot;, &quot;xiaoyuge&quot;); jedis.hset(&quot;user:1001&quot;, &quot;age&quot;, &quot;18&quot;); Map&lt;String, String&gt; map = jedis.hgetAll(&quot;user:1001&quot;); System.out.println(map); //&#123;name=xiaoyuge, age=18, id=1001&#125; &#125; @Test public void zsetTest() &#123; jedis.zadd(&quot;score&quot;, 100d, &quot;maths&quot;); jedis.zadd(&quot;score&quot;, 86d, &quot;chinese&quot;); jedis.zadd(&quot;score&quot;, 96d, &quot;English&quot;); List&lt;String&gt; list = jedis.zrange(&quot;score&quot;, 0, -1); for (String s : list) &#123; System.out.print(s+&quot;,&quot;); &#125; //输出：chinese,English,maths &#125; @Test public void subscribeTest() throws InterruptedException &#123; //subscribe 消息监听 jedis.subscribe(new JedisPubSub() &#123; @Override public void onMessage(String channel, String message) &#123; System.out.print(channel+&quot;: &quot;+message); &#125; &#125;, &quot;channelName&quot;); TimeUnit.SECONDS.sleep(1); &#125; @Test public void publishTest()&#123; jedis.publish(&quot;channelName&quot;,&quot;hello redis&quot;); &#125;&#125;","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Redis新的3种数据类型（三）","slug":"redis-special-newDataType","date":"2023-02-23T13:49:20.000Z","updated":"2023-02-23T13:49:20.000Z","comments":false,"path":"redis-special-newDataType/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-newDataType/index.html","excerpt":"","text":"1. Bitmaps 位操作字符串 现代计算机使用二进制(位)作为信息的基本单位，1个字节=8位，例如&quot;abc&quot;字符串有3个字节组成，计算机存储是使用其二进制。 &quot;abc&quot;分别对应ASCII码：97，98，99，对应的二进制分别是：01100001、01100010、01100011，如下图 合理的使用位操作能够有效地提高内存使用率和开发效率。 Redis提供了Bitmaps这个&quot;数据类型&quot;可以实现对位的操作： Bitmaps本身不是一种数据类型，实际上就是字符串（key-value),但它可以对字符串进行操作，字符串中每一个字符对应1个字节，也就是8位，一个字符可以存储8bit位信息 Bitmaps单独提供了一套命令， 所以在Redis中使用Bitmaps和使用字符串的方法不太相同。 可以把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在Bitmaps中叫做偏移量。 1.1 常用命令 setbit：设置某个偏移量的值(0或1) 1setbit key offset value 设置offset偏移位的值为value，offset的值是从0开始的，n代表第n+1个bit位置的。 offset 参数必须大于或等于 0 ，小于 2^32 (bit 映射被限制在 512 MB 之内)。 value的值只能为0或1 返回值：指定偏移量原来储存的位。 示例： 123456127.0.0.1:6379&gt; setbit bitkey 100 1(integer) 0127.0.0.1:6379&gt; getbit bitkey 100(integer) 1127.0.0.1:6379&gt; getbit bitkey 101 #bit默认初始化为 0 (integer) 0 每个独立用户是否访问国网站存放在bitmaps中，将访问的用户记录记做1，没有反问的记做0，用户ID作为offset。假设现在有20个用户，userId=1,6,11，15,19的用户访问了网站，那么当前的bitmaps初始化结果如图： users:20230225这个bitmaps中表示2023-02-25这天独立访问的用户，如下： 12345678910127.0.0.1:6379&gt; setbit users:20230225 1 1(integer) 0127.0.0.1:6379&gt; setbit users:20230225 6 1(integer) 0127.0.0.1:6379&gt; setbit users:20230225 11 1(integer) 0127.0.0.1:6379&gt; setbit users:20230225 15 1(integer) 0127.0.0.1:6379&gt; setbit users:20230225 19 1(integer) 0 getbit：获取某个偏移位的值 1getbit key offset 获取key锁对应的bitmaps中offset偏移为的值，返回0或者1 bitcount：统计bit位都为1的数量 1bitcount key [start] [end] 统计bit被设置为1的数量，一般情况下，给定的整个字符串都会被进行统计，通过指定额外的start或者end参数，可以让计数只在特定位上进行 start和end都可以使用负数，比如 -1表示最后一个，-2表示倒数第二个，以此类推 注意：start、end是指bit数组的字节下标，一个字节对应8个bit,所以[a,b]对应的offset范围是[8a, 8b+7] 示例： 12345# offset值为：1,6,11，15,19127.0.0.1:6379&gt; bitcount users:20230225 # 获取user这个bitmaps中1的数量(integer) 5127.0.0.1:6379&gt; bitcount users:20230225 0 1 # 获取[0,1]这个字节内bit位上1的数量，也就是offset是[0,15]的位置上1的数量，所以是4个(integer) 4 bitop：对一个或者多个bitmaps执行位操作 1bitop &lt;operation&gt; destkey key [key ....] 对一个或多个保存二进制位的字符串key进行位元操作，并将结果保存到destkey上。 operation可以是 AND 、OR、NOT、XOR中的一种： BITOP AND destkey key [key …] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOP OR destkey key [key …] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOP XOR destkey key [key …] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOP NOT destkey key ，对给定 key 求逻辑非，并将结果保存到 destkey 。 除了 NOT 操作之外，其他操作都可以接受一个或多个 key 作为输入。 返回值：保存到 destkey 的字符串的长度，和输入 key 中最长的字符串长度相等。 示例： 12345678910127.0.0.1:6379&gt; setbit bits-1 0 1(integer) 0127.0.0.1:6379&gt; setbit bits-1 3 1(integer) 0127.0.0.1:6379&gt; setbit bits-2 0 1(integer) 0127.0.0.1:6379&gt; setbit bits-2 3 1(integer) 0127.0.0.1:6379&gt; bitop and result-and bits-1 bits-2(integer) 1 1.2 bitmaps与set比较 假设网站有1亿的用户，每天独立访问的用户有5千万，如果每天用集合类型和bitmaps分别存储活跃用户可以得到表： 数据类型 每个用户ID占用空间 需要存储的用户量 全部存储两 Set集合 64位 5千万 64位 * 50000000 = 400MB Bitmaps 1位 1亿 1位 * 100000000 = 12.5MB 很明显， 这种情况下使用Bitmaps能节省很多的内存空间， 尤其是随着时间推移节省的内存还是非常可观的。 但是如果该网站每天独立访问用户很少，那么这两者对比起来，bitmaps就不太合适了，因为大部份位都是0； 2. HyperLoglog 在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站 PV（PageView 页面访问量），可以使用 Redis 的 incr、incrby 轻松实现。但像 UV（UniqueVisitor 独立访客）、独立 IP 数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。 解决基数问题有很多种方案： 数据存储在 MySQL 表中，使用 distinct count 计算不重复个数。 使用 Redis 提供的 hash、set、bitmaps 等数据结构来处理。 以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。能否能够降低一定的精度来平衡存储空间？Redis 推出了 HyperLogLog。 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是：在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog不能像集合那样，返回输入的各个元素。 基数： 比如数据集 {1, 3, 5, 7, 5, 7, 8}，那么这个数据集的基数集为 {1, 3, 5 ,7, 8}，基数 (不重复元素) 为 5。 基数估计就是在误差可接受的范围内，快速计算基数。 2.1 常用命令 pfadd：添加多个元素 1pfadd key element [element ....] 向HyperLoglog类型key添加一个或者多个元素,1添加成功， 0添加失败 示例： 123456127.0.0.1:6379&gt; pfadd program java php js node # program中添加4个元素[java,php,js,node]，添加成功，返回1(integer) 1127.0.0.1:6379&gt; pfadd program java #再次添加java，由于已经存在，所以添加失败，返回0(integer) 0127.0.0.1:6379&gt; pfadd program java c++ # 再次添加2个元素，java已经存在了，但是c++不存在，添加成功，返回1(integer) 1 pfcount：获取多个HLL合并后元素的个数 1pfcount key1 key2 统计一个或者多个key去重后元素的数量 示例： 123456127.0.0.1:6379&gt; pfadd k1 a b c d java #k1中5个元素：[a,b,c,d,java],其中Java在program中存在(integer) 1127.0.0.1:6379&gt; pfcount k1(integer) 5127.0.0.1:6379&gt; pfcount k1 program 获取k1和program去重之后数量合集：[a,b,c,d,java,php,js,node,c++]，数量为9(integer) 9 pfmerge：将多个HLL合并后元素放入另一个HLL 1pfmerge &lt;destkey&gt; &lt;sourcekey&gt; [sourcekey ....] 将过个sourcekey合并放到destkey中 示例： 1234127.0.0.1:6379&gt; pfmerge mergekey k1 program #将k1和program合并后放入mergekeyOK127.0.0.1:6379&gt; pfcount mergekey #mergekey中元素个数为9(integer) 9 3. Geographic Reids3.2 中增加了对GEO类型的支持，GEO（Geographic），地理信息的缩写。 该类型，就是元素的2维坐标，在地图上就是经纬度，redis基于该类型，提供了经纬度设置、查询、范围查询、距离查询，经纬度Hash等常见操作。 3.1 常用命令 geoadd：添加多个位置的经纬度 1geoadd key longitude latitude member [longitude latitude member...] longitude latitude member：经度 纬度 名称 geo实际上使用的是zset类型存储的 示例： 1234567891011121314127.0.0.1:6379&gt; geoadd china:city 121.47 31.23 shanghai #添加上海的经纬度(integer) 1127.0.0.1:6379&gt; geoadd china:city 106.50 29.53 chongqing #添加重庆经纬度(integer) 1127.0.0.1:6379&gt; type china:city #查看类型，发现geo实际上使用zset类型存储的zset127.0.0.1:6379&gt; zrange china:city 0 -1 #查询key的全部元素1) &quot;chongqing&quot;2) &quot;shanghai&quot;127.0.0.1:6379&gt; zrange china:city 0 -1 withscores # 查询key的全部元素包含score1) &quot;chongqing&quot;2) &quot;4026042091628984&quot;3) &quot;shanghai&quot;4) &quot;4054803462927619&quot; 两级无法直接添加，一般会下载城市数据，直接通过java程序一次性导入。 有效的经纬度从-180度到180度，有效的维度从-85.05112878度到85.05112878度。 当坐标位置超出指定范围时，该命令将会返回一个错误。 已经添加的数据，是无法再次往里面添加的。 geopos：获取多个位置的坐标值 1geopos key member [member.....] 示例： 123456127.0.0.1:6379&gt; geopos china:city shanghai chongqing wuhan #获取上海、重庆、武汉3个城市的坐标,由于没有添加武汉的数据，所以没有获取到，其他2个获取到了1) 1) &quot;121.47000163793563843&quot; 2) &quot;31.22999903975783553&quot;2) 1) &quot;106.49999767541885376&quot; 2) &quot;29.52999957900659211&quot;3) (nil) geodist：获取两个位置的直线距离 123geodist key member1 member2 [m | km | ft | mi]#单位：[m|km|ft|mi] -&gt; [米|千米|英里|英尺]，默认为米 示例： 123456127.0.0.1:6379&gt; zrange china:city 0 -11) &quot;chongqing&quot;2) &quot;shanghai&quot;127.0.0.1:6379&gt; geodist china:city shanghai chongqing #获取上海到重庆的直线距离&quot;1447673.6920&quot;127.0.0.1:6379&gt; georadius：以给定的经纬度为中心，找出某一半径内的元素(附近的人) 123georadius key longitude latitude radius m |km |ft|mi #单位：[m|km|ft|mi] -&gt; [米|千米|英里|英尺]，默认为米 示例： 12345678910127.0.0.1:6379&gt; geoadd china:city 114.05 22.52 shenzhen 116.38 39.90 beijing #添加深圳、北京2个城市的经纬度(integer) 2127.0.0.1:6379&gt; zrange china:city 0 -1 #输出key中的元素，里面包含了重庆，深圳，上海，北京4个地方的经纬度1) &quot;chongqing&quot;2) &quot;shenzhen&quot;3) &quot;shanghai&quot;4) &quot;beijing&quot;127.0.0.1:6379&gt; georadius china:city 110 30 1000 km #在china:city中检索：以经纬度(110,30)为中心，半径为1000km内的位置列表1) &quot;chongqing&quot;2) &quot;shenzhen&quot;","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Redis的发布和订阅（二）","slug":"redis-special-pub","date":"2023-02-23T13:12:50.000Z","updated":"2023-02-23T13:12:50.000Z","comments":false,"path":"redis-special-pub/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-pub/index.html","excerpt":"","text":"1. 发布和订阅介绍 Redis发布和订阅（pub/sub）是一种消息通信模式：发布者（pub）发布消息，订阅者（sub）接收消息。Redis客户端可以订阅任意数量的频道 2. Redis的发布和订阅 客户段可以订阅频道 当发布者给频道发布消息后，消息就会发送给订阅的客户端 3. 发布和订阅的命令行实现 打开客户端订阅channel1 订阅命令: subscribe channel1 channel2 ...，可以订阅多个 12345127.0.0.1:6379&gt; subscribe channel1Reading messages... (press Ctrl-C to quit)1) &quot;subscribe&quot;2) &quot;channel1&quot;3) (integer) 1 打开另一个客户端，给channel1发布消息helloworld 发布消息命令：publish channel 消息，channel表示发布的频道，返回值表示有几个订阅者 12127.0.0.1:6379&gt; publish channel1 helloworld(integer) 1 切换到订阅者窗口，可以看到收到的消息 4. 发布和订阅常用命令 subscribe：订阅一个或者多个频道 1subscribe channel [channel ...] publish：发布消息到指定的频道 1publish channel message 将消息message发送到频道channel，返回的是订阅者的数量 psubscribe：订阅一个或多个符合给定模式的频道 1psubscribe pattern [pattern...] 订阅一个或多个符合给定模式的频道，每个模式以 * 作为匹配符，比如：it*匹配it开头的频道(it.news, it.blog…) 示例： 123456789127.0.0.1:6379&gt; psubscribe news.* blog.*Reading messages... (press Ctrl-C to quit)1) &quot;psubscribe&quot; # 返回值的类型：显示订阅成功2) &quot;news.*&quot; # 订阅的模式3) (integer) 1 # 目前已订阅的模式的数量1) &quot;psubscribe&quot;2) &quot;blog.*&quot;3) (integer) 2","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"Redis 5大数据类型（一）","slug":"redis-special-1","date":"2023-02-19T08:57:18.000Z","updated":"2023-02-19T08:57:18.000Z","comments":false,"path":"redis-special-1/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-special-1/index.html","excerpt":"","text":"前言 Reids安装请看：Linux环境下安装Redis 这里说的数据类型是value的数据类型，key的类型都是字符串。 5种数据类型： 字符串String 列表List 集合Set 哈希表 Hash 有序集合Zset Redis 常用数据类型操作命令： Redis命令中心 Redis键（key)的相关命令： keys * : 查看当前库所有的key exists key : 判断某个key是否存在 type key ： 查看key的类型 del key： 删除指定的key数据 unlink key : 根据value删除非阻塞删除，仅仅将keys从keyspace元数据中删除，真正的删除会在后续异步中操作 expire key 10: 设置key的有效时间为10秒 ttl key：查看指定的key还有多少秒过期，-1：表示永不过期，-2：表示已过期 select dbindex：切换数据库【0-15】，默认为0 dbsize：查看当前数据库key的数量 flushdb：清空当前库 flushall：清空全部库 1. 字符串String String是Redis最基本的类型，可以理解为和Memcached一样的类型，一个key 对应一个value。 String类型是二进制，意味着可以包含任何数据，比如jpg图片或者序列化对象。一个Redis字符串value最多可以是512M 1.1 常用命令 set： 添加键值对 1127.0.0.1:6379&gt; set key value [EX seconds|PX milliseconds|EXAT timestamp|PXAT milliseconds-timestamp|KEEPTTL] [NX|XX] [GET] NX：当数据库中key不存在时，可以将key-value添加到数据库 XX：当数据库中key存在时，可以将key-value添加数据库，与NX参数互斥 EX：key的超时秒数 PX：key的超时毫秒数，与EX互斥 value中若包含空格、特殊字符，需用双引号包裹 12127.0.0.1:6379&gt; set xiaoyuge 180 NX OK get： 获取值 1get &lt;key&gt; 示例： 12127.0.0.1:6379&gt; get xiaoyuge&quot;180&quot; append： 追加值 1append &lt;key&gt; &lt;value&gt; 将给定的value追加到原值的末尾。 示例： 12345678127.0.0.1:6379&gt; set xiaoyuge 180OK127.0.0.1:6379&gt; get xiaoyuge&quot;180&quot;127.0.0.1:6379&gt; append xiaoyuge cm(integer) 5127.0.0.1:6379&gt; get xiaoyuge&quot;180cm&quot; strlen： 获取值的长度 1strlen &lt;key&gt; 示例： 12127.0.0.1:6379&gt; strlen xiaoyuge(integer) 8 setnx： key不存在时，设置key的值 1setnx &lt;key&gt; &lt;value&gt; 示例： 12345127.0.0.1:6379&gt; setnx ygb cool ## ygb不存在，返回1，表示设置成功(integer) 1127.0.0.1:6379&gt; setnx ygb cool ## 再次通过setnx设置ygb，由于已经存在了，所以设置失败，返回0(integer) 0127.0.0.1:6379&gt; incr： 原子递增1 1incr &lt;key&gt; 将key中存储的值增加1， 只能对数值操作，如果key不存在，则会新建一个，值为1 示例： 1234567891011127.0.0.1:6379&gt; flushdb #清空db，方便测试OK127.0.0.1:6379&gt; set age 18 #age值为18OK127.0.0.1:6379&gt; incr age #age增加1，返回19(integer) 19127.0.0.1:6379&gt; get age #获取age的值&quot;19&quot;127.0.0.1:6379&gt; incr salary #salary不存在，自动创建一个，值为1(integer) 1127.0.0.1:6379&gt; get salary #获取salary的值 decr： 原子递减1 1decr &lt;key&gt; 将key中存储的值减1，只能对数值操作，如果为空，新增值为-1 示例： 12345678910127.0.0.1:6379&gt; set age 18OK127.0.0.1:6379&gt; get age&quot;18&quot;127.0.0.1:6379&gt; decr age(integer) 17127.0.0.1:6379&gt; keys age1(empty array)127.0.0.1:6379&gt; decr age1(integer) -1 incrby/decrby： 递增或递减指定的数字 12incrby &lt;key&gt; &lt;步长&gt;decrby &lt;key&gt; &lt;步长&gt; 将key中存储的数字值递增指定的步长，若key不存在，则相当于在原值为0的值上递增指定的步长。 示例： 123456127.0.0.1:6379&gt; incrby age 18(integer) 18127.0.0.1:6379&gt; decrby age 2(integer) 16127.0.0.1:6379&gt; get age&quot;16&quot; mset： 同时设置多个key-value 1234mset &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; ...# 设置name = xiaoyuge, age = 18# 127.0.0.1:6379&gt; mset name xiaoyuge age 18 mget： 获取多个key对应的值 123456mget &lt;key1&gt; &lt;key2&gt; ....#示例127.0.0.1:6379&gt; mget name age#1) &quot;xiaoyuge&quot;#2) &quot;18&quot; msetnx： 当多个key都不存在时，则设置成功 1msetnx &lt;key1&gt; &lt;value1&gt; &lt;key2&gt; &lt;value2&gt; ... 原子性，要么都成功，或者都失败 示例： 1234567891011121314127.0.0.1:6379&gt; flushdb #清空OK127.0.0.1:6379&gt; set k1 v1 #先设置k1OK127.0.0.1:6379&gt; msetnx k1 v1 k2 v2 #当k1和k2都不存在，同时设置k1,k2,由于k1已经存在，所以操作失败(integer) 0127.0.0.1:6379&gt; mget k1 k2 # 获取k1、k2， k2不存在1) &quot;v1&quot;2) (nil)127.0.0.1:6379&gt; msetnx k2 v2 k3 v3 #当k2和k3都不存在，同时设置k2,k3，设置成功(integer) 1127.0.0.1:6379&gt; mget k2 k3 # 获取k2、k31) &quot;v2&quot;2) &quot;v3&quot; getrange： 获取值的范围，类似于Java的substring 1getrange key &lt;start&gt; &lt;end&gt; 获取[start， end]之间的字符，返回为字符串 示例： 1234127.0.0.1:6379&gt; set k1 xiaoyugeOK127.0.0.1:6379&gt; getrange k1 0 4&quot;xiaoy&quot; setrange： 覆盖自定位置的值 1setrange &lt;key&gt; &lt;start&gt; &lt;value&gt; 从 start 位置开始覆盖，覆盖的长度为value的长度， 总长度不变 示例： 123456789101112127.0.0.1:6379&gt; get k1&quot;xiaoyuge&quot;127.0.0.1:6379&gt; setrange k1 0 nb(integer) 8127.0.0.1:6379&gt; get k1&quot;nbaoyuge&quot;127.0.0.1:6379&gt; get k1&quot;nbaoyuge&quot;127.0.0.1:6379&gt; setrange k1 0 xiao(integer) 8127.0.0.1:6379&gt; get k1&quot;xiaoyuge&quot; setex： 设置键值&amp;过期时间(秒) 1setex &lt;key&gt; &lt;过期时间s&gt; &lt;value&gt; 示例： 123456127.0.0.1:6379&gt; setex k1 120 v1 #设置k1的值为v1，有效期120秒OK127.0.0.1:6379&gt; get k1 #获取k1的值&quot;v1&quot;127.0.0.1:6379&gt; ttl k1 #获取k1还有多少秒失效(integer) 113 getset： 以新换旧，设置新值同时返回旧值 1getset &lt;key&gt; &lt;value&gt; 示例： 12345678910127.0.0.1:6379&gt; set name xiaoyuge #设置name为xiaoyugeOK127.0.0.1:6379&gt; getset name xiaoyuge666 #设置name为xiaoyuge666，返回name的旧值&quot;xiaoyuge&quot;127.0.0.1:6379&gt; getset age 18 #设置age为18，age未设置过，返回age的旧值为null(nil)127.0.0.1:6379&gt; get name #获取现在的name，返回xiaoyuge666&quot;xiaoyuge666&quot;127.0.0.1:6379&gt; get age #获取age&quot;18&quot; 1.2 数据结构 String的数据结构为简单动态字符串(Simple Dynamic String,简写SDS)。是可以修改的字符串，内部结构类似于Java中的ArrayList，采用分配冗余空间的方式来减少内存的频繁分配。 如图所示，内部为当前字符串实际分配的空间capacity，一般要高于实际字符串长度len。当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次会多扩容1M的空间 要注意的是字符串最大长度为512M。 2. 列表List 列表List是简单的字符串列表，按照插入顺序排序，添加元素时可以插入列表的头部或者尾部。 它的底层实现实际上使用双向链表实现的，对两端的操作性能很高，通过索引下标操作中间节点性能会较差。 2.1 常用命令 lupsh/rpush： 从左边或者右边插入一个或多个值 12lpush &lt;key1&gt; &lt;value1&gt; &lt;value2&gt; &lt;value3&gt; ...rpush &lt;key1&gt; &lt;value1&gt; &lt;value2&gt; &lt;value3&gt; ... 示例： 1234567891011127.0.0.1:6379&gt; rpush k v1 v2 v3 #列表k的右边插入3个元素(integer) 3127.0.0.1:6379&gt; lpush k v4 v5 v6 #列表k的左边插入3个元素(integer) 6127.0.0.1:6379&gt; lrange k 0 6 #输出[0,6]范围内的元素1) &quot;v6&quot;2) &quot;v5&quot;3) &quot;v4&quot;4) &quot;v1&quot;5) &quot;v2&quot;6) &quot;v3&quot; lrange： 从列表左边获取指定范围内的值 1lrange &lt;key&gt; &lt;start&gt; &lt;end&gt; 返回列表key中指定的区间的元素，区间偏移量start ,end指定。 下标index参数start和end都从0开始，也可以使用负数下标，以 -1表示列表最后一个元素，-2表示倒数第二个元素… 示例： 1234567891011121314127.0.0.1:6379&gt; lpush n v1 v2 v3 v4 v5 v6(integer) 6127.0.0.1:6379&gt; lrange n 0 -1 #取出n集合中所有元素 start 0表示从第一个开始, end -1表示最后一个1) &quot;v6&quot;2) &quot;v5&quot;3) &quot;v4&quot;4) &quot;v3&quot;5) &quot;v2&quot;6) &quot;v1&quot;127.0.0.1:6379&gt; lrange n -3 -1 #获取倒数第三至倒数一个元素 start 一定要小于end1) &quot;v3&quot;2) &quot;v2&quot;3) &quot;v1&quot;127.0.0.1:6379&gt; lpop/rpop： 从左边或者右边弹出多个元素 1lpop/rpop &lt;key&gt; &lt;count&gt; count: 可以省略，默认为1 lpop/rpop操作之后，弹出的值会从列表中删除，当所有的值都删除后，键就删除 示例： 123456789127.0.0.1:6379&gt; rpush k v1 v2 v3 #集合k 右边添加3个元素(integer) 3127.0.0.1:6379&gt; lpop k #左边弹出1个元素&quot;v1&quot;127.0.0.1:6379&gt; rpop k 2 # 右边弹出2个元素 &quot;v3&quot;&quot;v2&quot;127.0.0.1:6379&gt; exists k # 查询key是否存在(integer) 0 rpoplpush： 从一个列表右边弹出一个元素放到另外一个列表中 1rpoplpush &lt;source&gt; &lt;destination&gt; 从source的右边弹出一个元素放到destination列表的左边 示例 12345678910111213141516171819202122127.0.0.1:6379&gt; rpush k 1 2 3 #列表k的右边添加3个元素[1,2,3](integer) 3127.0.0.1:6379&gt; lrange k 0 -1 #从左到右输出k列表中的元素1) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;127.0.0.1:6379&gt; rpush k1 4 5 6 #列表k1的右边添加3个元素[4,5,6](integer) 3127.0.0.1:6379&gt; lrange k1 0 -1 #从左到右输出k1列表中的元素1) &quot;4&quot;2) &quot;5&quot;3) &quot;6&quot;127.0.0.1:6379&gt; rpoplpush k k1 #从k的右边弹出一个元素放到k1的左边&quot;3&quot;127.0.0.1:6379&gt; lrange k 0 -1 #k中剩下2个元素了1) &quot;1&quot;2) &quot;2&quot;127.0.0.1:6379&gt; lrange k1 0 -1 #k1变成来4个元素1) &quot;3&quot;2) &quot;4&quot;3) &quot;5&quot;4) &quot;6&quot; lindex： 获取指定索引位置的元素（从左到右） 1lindex &lt;key&gt; &lt;index&gt; 返回列表key中下标为index的元素。 下标从0开始，也可以是负数，-1 表示列表最后一个元素，-2表示列表倒数第二个元素… 如果key不是列表类型，返回一个错误 如果index超出了列表的长度范围，返回 nil 示例： 1234567891011127.0.0.1:6379&gt; lrange k1 0 -11) &quot;3&quot;2) &quot;4&quot;3) &quot;5&quot;4) &quot;6&quot;127.0.0.1:6379&gt; lindex k1 2 #返回索引位置2的元素&quot;5&quot;127.0.0.1:6379&gt; lindex k1 6 #返回索引位置6的元素，超出了列表长度(nil)127.0.0.1:6379&gt; lindex k1 -1 #返回最后一个元素&quot;6&quot; llen： 获取列表长度 1llen &lt;key&gt; 返回列表长度，如果key不存在，则被解释为一个空列表，返回0； 如果key不是列表类型，返回一个错误 示例： 123456789101112131415127.0.0.1:6379&gt; lrange k1 0 -11) &quot;3&quot;2) &quot;4&quot;3) &quot;5&quot;4) &quot;6&quot;127.0.0.1:6379&gt; llen k1 #返回k1集合的长度(integer) 4127.0.0.1:6379&gt; set name xiaoyuge #设置字符串OK127.0.0.1:6379&gt; type name #查看name的类型string127.0.0.1:6379&gt; llen name #获取name的长度，因为name为String类型，所以报错(error) WRONGTYPE Operation against a key holding the wrong kind of value127.0.0.1:6379&gt; strlen name #字符串的长度使用strlen(integer) 8 linsert： 在某个值的前或者后插入一个值 1linsert &lt;key&gt; before|after &lt;value&gt; &lt;newvalue&gt; 将值newvalue插入到列表key中，位于value值之前或者之后 当value不存在列表key中，不执行任何操作，返回 -1 当key不存在时，key被视为空列表，不执行任何操作， 返回 0 如果命令执行成功，返回插入操作完成之后，列表的长度 如果key不是列表类型，返回一个错误 示例： 1234567891011121314151617127.0.0.1:6379&gt; rpush k 1 2 3 #列表k中添加3个元素(integer) 3127.0.0.1:6379&gt; lrange k 0 -1 #输出k全部元素1) &quot;1&quot;2) &quot;2&quot;3) &quot;3&quot;127.0.0.1:6379&gt; linsert k before 1 0 #在1前面添加0，添加成功，返回列表长度(integer) 4127.0.0.1:6379&gt; lrange k 0 -1 #输出k全部元素1) &quot;0&quot;2) &quot;1&quot;3) &quot;2&quot;4) &quot;3&quot;127.0.0.1:6379&gt; linsert k before 4 5 #在4前面添加5，由于元素4不存在，插入失败返回-1(integer) -1127.0.0.1:6379&gt; linsert k1 before 4 5 #在列表k1中元素4前面插入5，由于列表k1不存在，返回0(integer) 0 lrem： 删除指定数量的某个相同的元素 1lrem &lt;key&gt; &lt;count&gt; &lt;value&gt; 根据count的值，移除列表中与参数value相等的count个元素 count的值可以是以下几种 count &gt; 0: 从表头开始向表尾搜索，移除与 value 相等的元素，数量为 count count &lt; 0: 从表尾开始向表头搜索，移除与 value 相等的元素，数量为 count 的绝对值。 count = 0: 移除列表总所有与value相等的值 因为不存在的 key 被视作空表(empty list)，所以当 key 不存在时，总是返回 0 。 示例： 123456789101112131415161718127.0.0.1:6379&gt; flushdb #清空db，方便测试OK127.0.0.1:6379&gt; rpush k1 v1 v2 v3 v2 v2 v1 #k1列表中插入6个元素(integer) 6127.0.0.1:6379&gt; lrange k1 0 -1 #输出k1集合中所有元素1) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;4) &quot;v2&quot;5) &quot;v2&quot;6) &quot;v1&quot;127.0.0.1:6379&gt; lrem k1 2 v2 #k1集合中从左边删除2个v2(integer) 2127.0.0.1:6379&gt; lrange k1 0 -1 #输出列表，列表中还有1个v2，前面2个v2干掉了1) &quot;v1&quot;2) &quot;v3&quot;3) &quot;v2&quot;4) &quot;v1&quot; lset： 替换指定位置的值 1lset &lt;key&gt; &lt;index&gt; &lt;value&gt; 将列表key下标为index的元素替换为value, 当index参数超出范围，或者对一个空列表进行操作时，返回一个错误 示例： 123456789101112131415161718127.0.0.1:6379&gt; flushdb #清空db，方便测试OK127.0.0.1:6379&gt; rpush k v1 v2 v3 #k集合中放入3个元素(integer) 3127.0.0.1:6379&gt; lrange k 0 -1 #输出k集合元素1) &quot;v1&quot;2) &quot;v2&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; lset k 1 vv #将k集合中第2个元素替换为vvOK127.0.0.1:6379&gt; lrange k 0 -11) &quot;v1&quot;2) &quot;vv&quot;3) &quot;v3&quot;127.0.0.1:6379&gt; lset k 10 vv #将k集合中第11个元素替换为vv，由于集合长度小于10，报错(error) ERR index out of range127.0.0.1:6379&gt; lset k1 1 vv #k1不存在，报错(error) ERR no such key 2.2 数据结构 List的数据结构为快速链表quickList 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也就是压缩列表。 它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当就比较多的时候才会改成quickList。 因为普通的链表需要的附加指针空间太大，会比较浪费空间，比如这个列表里存储的只是int类型的书，结构上还需要2个额外的指针prev和next。 redis将链表和ziplist结合起来组成了quicklist。也就是将多个ziplist使用双向指针串起来使用，这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 3. 集合Set Redis set对外提供的功能与list类似，是一个列表的功能，特殊之处在于set可以自动去重 set时String类型的无序集合，它的底层实际上时一个value为null的hash表，添加、删除、查找复杂度都是O(1) 一个算法，如果时间复杂度是O(1)，那么随着数据的增加，查找数据的时间不变，也就是不管数据多少，查找时间都是一样的。 3.1 常用命令 sadd： 添加一个或者多个元素 123456sadd &lt;key&gt; &lt;value1&gt; &lt;value2&gt; ...#示例：127.0.0.1:6379&gt; sadd s1 v1 v2 v3(integer) 3127.0.0.1:6379&gt; smembers取出所有的元素 1234567smembers &lt;key&gt;#示例：127.0.0.1:6379&gt; smembers s1 #输出结果为无序1) &quot;v3&quot;2) &quot;v1&quot;3) &quot;v2&quot; sismember： 判断集合中是否存在某个值 1234sismember &lt;key&gt; &lt;value&gt;#判断集合key是否包含元素value， 1：有， 0：没有#sismember: set is member（是否是set中的成员） scard： 返回集合中元素的个数 1scard &lt;key&gt; 返回集合key中元素个数，当key不存在返回0 srem： 删除多个元素 1srem &lt;key&gt; &lt;member&gt; [member ...] 移除集合key中的一个或多个member元素，不存在的元素会被忽略，当key不是集合类型，返回一个错误 返回被成功移除的元素数量，不包括被忽略的元素 spop： 随机弹出多个值 1spop &lt;key&gt; &lt;count&gt; 随机从key集合中弹出count个元素，count默认为1，返回被移除的元素 当count大于元素集合个数的时候，弹出全部 当key不存在或者空集时，返回nil srandmember： 随机获取多个元素，不会从集合中删除 1srandmember &lt;key&gt; &lt;count&gt; 从key指定的集合中随机返回count个元素，count可以不指定，默认值是1。 srandmember 和 spop的区别： 都可以随机获取多个元素，srandmember 不会删除元素，而spop会删除元素。 返回值: 1. 只提供 key 参数时，返回一个元素；如果集合为空，返回 nil 。 2. 如果提供了count参数，那么返回一个数组；如果集合为空，返回空数组。 smove： 将某个元素从一个集合移到另一个集合 1smove &lt;source&gt; &lt;destination&gt; &lt;member&gt; 将member元素从source集合移动到destination集合。 smove是原子性操作,如果source集合不存在或者不保护指定的member元素，则smove命令不执行任何操作，仅返回0。 否则，member元素从source集合中移除，并添加到destination集合中去(destination集合不存在的话，会自动添加一个) 当destination已经包含member元素，smove只是删除source中的member元素 如果member被成功删除，返回1；如果member不是source集合成员，并且没有任何对destination的操作，那么返回0 *sinter 取多个集合的交集 12345678910111213141516sinter key [key ...]#示例127.0.0.1:6379&gt; smembers s #查询s集合的所有元素1) &quot;v3&quot;2) &quot;v4&quot;3) &quot;v2&quot;127.0.0.1:6379&gt; sadd s2 v1 v2 v3 #集合s2添加三个元素(integer) 3127.0.0.1:6379&gt; smembers s2 #查询s2集合的所有元素1) &quot;v3&quot;2) &quot;v1&quot;3) &quot;v2&quot;127.0.0.1:6379&gt; sinter s s2 #获取集合s s2的交集1) &quot;v3&quot;2) &quot;v2&quot; sinterstore： 将多个集合的交集放到一个新的集合中 1sinterstore destination key [key ...] 这个命令类似于sinter命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 返回结果集中的成员数量。 sunion： 取多个集合的并集，自动去重 1234567sunion key [key ...]#示例：127.0.0.1:6379&gt; sunion s s21) &quot;v1&quot;2) &quot;v3&quot;3) &quot;v2&quot;4) &quot;v4&quot; sunionstore： 将多个集合的并集放到一个新的集合中 1sinterstore destination key [key ...] 这个命令类似于 sunion 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 返回值:结果集中的成员数量。 sdiff： 取多个集合的差集 1sdiff key [key ...] 返回一个集合的全部成员，该集合是所有给定集合之间的差集。 不存在的 key 被视为空集。 sdiffstore： 将多个集合的差集放到一个新的集合中 1sdiffstore destination key [key ...] 这个命令类似于 sdiff 命令，但它将结果保存到 destination 集合，而不是简单地返回结果集。 3.2 数据结构 set数据机构是字典，字典是用hash表实现的。 Java中的hashSet的内部实现使用HashMap,只不过所有的value都指向同一个对象。 Redis的set结构也是一样的，它的内部也使用hash结构，所有的value都指向同一个内部值。 4. 哈希表 Hash Redis hash是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似于java里面的Map&lt;String,Object&gt; 4.1 常用命令 hset： 设置多个field值 1234 hset key field value [field value ...] 127.0.0.1:6379&gt; hset set1 name xiaoyuge age 18 (integer) 2 将哈希表 key 中的域 field 的值设为 value 。 如果 key 不存在，一个新的哈希表被创建并进行 hset 操作。 如果域 field 已经存在于哈希表中，旧值将被覆盖。 返回值： 如果 field 是哈希表中的一个新建域，并且值设置成功，返回 1 。 如果哈希表中域 field 已经存在且旧值已被新值覆盖，返回 0 。 hget： 获取指定field的值 123456hget &lt;key&gt; &lt;field&gt;127.0.0.1:6379&gt; hget set1 name&quot;xiaoyuge&quot;127.0.0.1:6379&gt; hget set1 age&quot;18&quot; hgetall： 返回hash表所有的fileld和value 1234567hgetall &lt;key&gt;127.0.0.1:6379&gt; hgetall set11) &quot;name&quot;2) &quot;xiaoyuge&quot;3) &quot;age&quot;4) &quot;18&quot; hexists： 判断给定的field是否存在，1：存在，0：不存在 1hexists key field 查看哈希表 key 中，给定域 field 是否存在。 返回值： 如果哈希表含有给定域，返回 1 。 如果哈希表不含有给定域，或 key 不存在，返回 0 。 hkeys： 列出所有的filed 1hkeys key hvals： 列出所有的value 1hvals key hlen： 返回field的数量 1hlen key hincrby： filed的值加上指定的增量 1hincrby key field increment 为哈希表 key 中的域 field 的值加上增量 increment 。 增量也可以为负数，相当于对给定域进行减法操作。 如果 key 不存在，一个新的哈希表被创建并执行 HINCRBY 命令。 如果域 field 不存在，那么在执行命令前，域的值被初始化为 0 。 对一个储存字符串值的域 field 执行 HINCRBY 命令将造成一个错误。 返回值： 执行 hincrby 命令之后，哈希表 key 中域 field 的值。 示例： 12345127.0.0.1:6379&gt; hincrby set1 age 3(integer) 21127.0.0.1:6379&gt; hvals set11) &quot;xiaoyuge&quot;2) &quot;21&quot; hsetnx： 当filed不存在的时候，设置filed的值 1hsetnx key field value 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在。 若域 field 已经存在，该操作无效。 如果 key 不存在，一个新哈希表被创建并执行 hsetnx 命令。 返回值： 设置成功，返回 1 。 如果给定域已经存在且没有操作被执行，返回 0 4.2 数据结构 Hash类型对应的数据结构是2种：ziplist（压缩列表），hashtable（哈希表）。 当field-value长度较短个数较少时，使用ziplist，否则使用hashtable。 5. 有序集合Zset 有序集合zset和普通集合set非常相似，是一个没有重复元素的字符串集合，不同之处是有序集合的每一个成员都关联一个评分（score）， 这个评分被用来按照从最低分到最高分的方式排序集合中的成员。 **成员是唯一的，但是评分可以重复。**因为元素是有序的，所以可以很快的根据评分score或者次序position来获取一个范围的元素，访问有序集合中的中间元素也是非常快的。 5.1 常用命令 zadd： 添加元素 1zadd &lt;key&gt; &lt;score1&gt; &lt;member1&gt; &lt;score2&gt; &lt;member2&gt; ... 将一个或多个member元素及其score值加入到有序集 key 当中。 如果某个 member 已经是有序集的成员，那么更新这个 member 的 score 值，并通过重新插入这个 member 元素，来保证该 member 在正确的位置上。 score 值可以是整数值或双精度浮点数。 如果 key 不存在，则创建一个空的有序集并执行 zadd 操作。 当 key 存在但不是有序集类型时，返回一个错误。 被成功添加的新成员的数量，不包括那些被更新的、已经存在的成员。 zrange： score生序，获取指定索引范围的元素 1zrange key start top [withscores] 返回存储在有序集合key中的指定范围的元素。 返回的元素可以认为是按score从最低到最高排列，如果得分相同，将按字典排序。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 zrange key 0 -1：可以获取所有元素 withscores：让成员和它的 score 值一并返回，返回列表以 value1,score1, …, valueN,scoreN 的格式表示 时间复杂度:O(log(N)+M)， N 为有序集的基数，而 M 为结果集的基数 zrevrange： score降序，获取指定索引范围的元素 1zrevrange key start stop [WITHSCORES] 返回存储在有序集合key中的指定范围的元素。 返回的元素可以认为是按score最高到最低排列， 如果得分相同，将按字典排序。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 withscores：让成员和它的 score 值一并返回，返回列表以 value1,score1, …, valueN,scoreN 的格式表示 zrangebyscore：按照score升序，返回指定score范围内的数据 1zrangebyscore key min max [WITHSCORES] [LIMIT offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 具有相同 score 值的成员按字典序来排列(该属性是有序集提供的，不需要额外的计算)。 可选的 LIMIT 参数指定返回结果的数量及区间(就像SQL中的 SELECT LIMIT offset, count )，注意当 offset 很大时，定位 offset 的操作可能需要遍历整个有序集，此过程最坏复杂度为 O(N) 时间。 zrevrangebyscore：按照score降序，返回指定score范围内的数据 1zrevrangebyscore key max min [WITHSCORES] [LIMIT offset count] 返回有序集 key 中， score 值介于 max 和 min 之间(默认包括等于 max 或 min )的所有的成员。有序集成员按 score 值递减(从大到小)的次序排列。 具有相同 score 值的成员按字典序的逆序排列。 除了成员按 score 值递减的次序排列这一点外， zrevrangebyscore 命令的其他方面和 zrangebyscore 命令一样。 zincrby：为指定元素的score加上指定的增量 1zincrby key increment member 可以通过传递一个负数值 increment ，让 score 减去相应的值，比如 ZINCRBY key -5 member ，就是让 member 的 score 值减去 5 。 当 key 不存在，或 member 不是 key 的成员时， ZINCRBY key increment member 等同于 ZADD key increment member zrem：删除集合中多个元素 1zrem key member [member ...] 移除有序集 key 中的一个或多个成员，不存在的成员将被忽略。 当 key 存在但不是有序集类型时，返回一个错误。 zremrangebyrank 根据索引范围删除元素 1zremrangebyrank key start stop 移除有序集 key 中，指定排名(rank)区间内的所有成员。 区间分别以下标参数 start 和 stop 指出，包含 start 和 stop 在内。 下标参数 start 和 stop 都以 0 为底，也就是说，以 0 表示有序集第一个成员，以 1 表示有序集第二个成员，以此类推。 你也可以使用负数下标，以 -1 表示最后一个成员， -2 表示倒数第二个成员，以此类推。 zremrangebyscore：根据score的范围删除元素 1zremrangebyscore key min max 移除有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员 zcount：统计指定score范围内的元素个数 1zcount key min max 返回有序集 key 中， score 值在 min 和 max 之间(默认包括 score 值等于 min 或 max )的成员的数量 zrank：按照score生序，返回某个元素在集合中的排名 1zrank key member 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递增(从小到大)顺序排列。 排名以 0 为底，也就是说， score 值最小的成员排名为 0 。 zrevrank：按照score将许，返回某个元素在集合中的排名 1zrevrank key member 返回有序集 key 中成员 member 的排名。其中有序集成员按 score 值递减(从大到小)排序。排名以 0 为底，也就是说， score 值最大的成员排名为 0 zscore：返回集合中指定元素的score 1zscore key member 返回有序集 key 中，成员 member 的 score 值。 如果 member 元素不是有序集 key 的成员，或 key 不存在，返回 nil 5.2 数据结构 SortedSet（zset）是redis提供的一个非常特别的数据结构，内部使用到了2种数据结构。 1. hash表 类似于java中的Map&lt;String,score&gt;，key为集合中的元素，value为元素对应的score，可以用来快速定位元素定义的score，时间复杂度为O(1) 2. 跳表 跳表（skiplist）是一个非常有限的数据结构，实现简单，插入、删除、查找的复杂度均为O(logN) 类似于Java中的ConcurrentSkipListSet，根据score的值排序后生成一个跳表，可以快速按照位置的顺序或者score的顺序查找元素。 来看一下跳表的原理： 首先从考虑一个有序列表开始： 从该有序表中搜索元素 &lt; 23, 43, 59 &gt; ，需要比较的次数分别为 &lt; 2, 4, 6 &gt;，总共比较的次数为 2 + 4 + 6 = 12 次。有没有优化的算法吗? 链表是有序的，但不能使用二分查找。类似二叉搜索树，我们把一些节点提取出来，作为索引。得到如下结构： 这里我们把 &lt; 14, 34, 50, 72 &gt; 提取出来作为一级索引，这样搜索的时候就可以减少比较次数了。我们还可以再从一级索引提取一些元素出来，作为二级索引，变成如下结构： 如果元素足够多，这种索引结构就能体现出优势来了。","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"负载均衡Nginx Session 一致性","slug":"nginx-session-consistency","date":"2023-02-19T06:40:23.000Z","updated":"2023-02-19T06:40:23.000Z","comments":false,"path":"nginx-session-consistency/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-session-consistency/index.html","excerpt":"","text":"HTTPS请求跳转 如下面的配置： 123location /test &#123; proxy_pass http://www.baidu.com/;&#125; 在访问URI为/test时会跳转到百度，但是此时浏览器的URL也发生了变化，这是因为向http://www.baidu.com/发送请求后，Nginx返回的是一个跳转的响应。 此后，客户端会重新向http://www.baidu.com/发送请求（此过程不再经过nginx），所以浏览器发生了跳转，而非 Nginx的负载均衡。 解决方式： 将配置中的http改成https即可 Session一致性 在使用nginx做反向代理时，如果后端服务器时Tomcat等动态服务器，可能会出现Session一致性的问题。即无法确保同一个Session一定对应同一个Server 配置IP地址 Server IP Nginx 172.20.1.10 Server1 172.20.1.101 Server2 172.20.1.102 并且在Server1 和 Server2上启动 Tomcat 配置Server 在Server段创建jsp页面： Server1 123456[root@c5477d71795c ROOT]# pwd/var/lib/tomcat/webapps/ROOT[root@c5477d71795c ROOT]# cat index.jsp from 172.20.1.101&lt;br/&gt;session=&lt;%=session.getId()%&gt; Server2 123456[root@c5477d71795c ROOT]# pwd/var/lib/tomcat/webapps/ROOT[root@c5477d71795c ROOT]# cat index.jsp from 172.20.1.102&lt;br/&gt;session=&lt;%=session.getId()%&gt; 然后访问 http://172.20.1.101:8080/ 和 http://172.20.1.102:8080/。 可分别显示来自哪个Server和对应的 SessionId，并且刷新页面时 SessionId 不会变化（即使是使用 Ctrl+F5 刷新） 配置Nginx 修改Nginx配置文件，加入upstream配置和server配置 1234567891011upstream tomcat &#123; server 172.20.1.101:8080; server 172.20.1.102:8080;&#125;server &#123; ...... location /cat &#123; proxy_pass http://tomcat/; &#125;&#125; 重启Nginx 1[root@ce12b3b4ce00 sbin]# ./nginx -s reload 访问 http://172.20.1.10/cat 并刷新，发现 from 172.20.1.10x 一直在变化，并且session=xxx 也变化。 说明：此时 Nginx 的配置无法保证 Session 一致性! 解决方案 在 Tomcat 后面部署 Redis，MemCached 等内存数据库来保存 Session 相关信息。 本例中在 Nginx 服务器上安装 memcached 来解决 Session 一致性问题。 启动memcached 12345# 1. 安装memcachedyum install -y memcached# 2. 启动memcachedmemcached -d -m 128m -p 11211 -l 172.20.1.10 -u root -P /tmp/ 参数说明 -d: 后台启动 -m: 缓存大小 -p: 端口 -l: IP地址 -P: 服务启动后系统进程 ID 存储文件的目录 -u: 服务器以哪个用户作为管理用户 修改tomcat配置 在两台Server中修改tomcat配置 1[root@3a53f7504511 ROOT]# vi /etc/tomcat/context.xml 12345678910&lt;!-- context标签中加入下面的内容 --&gt; &lt;Manager className=&quot;de.javakaffee.web.msm.MemcachedBackupSessionManager&quot; memcachedNodes=&quot;n1:172.20.1.10:11211&quot; sticky=&quot;false&quot; sessionBackupAsync=&quot;false&quot; lockingMode=&quot;auto&quot; requestUriIgnorePattern=&quot;.*\\.(ico|png|gif|jpg|css|js)$&quot; sessionBackupTimeout=&quot;1000&quot; transcoderFactoryClass=&quot;de.javakaffee.web.msm.serializer.kryo.KryoTranscoderFactory&quot; /&gt; 导入 jar 包使用 yum 安装的 Tomcat 可将 jar 包放在 /usr/share/java/tomcat/ 目录下。 需要以下jar包： 添加Maven依赖(Maven项目配置，测试可不加) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;dependency&gt; &lt;groupId&gt;asm&lt;/groupId&gt; &lt;artifactId&gt;asm&lt;/artifactId&gt; &lt;version&gt;3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.couchbase.client&lt;/groupId&gt; &lt;artifactId&gt;couchbase-client&lt;/artifactId&gt; &lt;version&gt;1.4.11&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.googlecode&lt;/groupId&gt; &lt;artifactId&gt;kryo&lt;/artifactId&gt; &lt;version&gt;1.04&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.javakaffee&lt;/groupId&gt; &lt;artifactId&gt;kryo-serializers&lt;/artifactId&gt; &lt;version&gt;0.11&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.javakaffee.msm&lt;/groupId&gt; &lt;artifactId&gt;memcached-session-manager&lt;/artifactId&gt; &lt;version&gt;1.8.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.javakaffee.msm&lt;/groupId&gt; &lt;artifactId&gt;memcached-session-manager-tc7&lt;/artifactId&gt; &lt;version&gt;1.8.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.googlecode&lt;/groupId&gt; &lt;artifactId&gt;minlog&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.javakaffee.msm&lt;/groupId&gt; &lt;artifactId&gt;msm-kryo-serializer&lt;/artifactId&gt; &lt;version&gt;1.8.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.esotericsoftware&lt;/groupId&gt; &lt;artifactId&gt;reflectasm&lt;/artifactId&gt; &lt;version&gt;1.01&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.spy&lt;/groupId&gt; &lt;artifactId&gt;spymemcached&lt;/artifactId&gt; &lt;version&gt;2.11.4&lt;/version&gt;&lt;/dependency&gt; 注: 如果依赖和 Tomcat 版本不对应可能会什么也不显示，此时响应码为 500。 验证 上述步骤都正确配置之后, 再次访问 http://172.20.1.10/cat 并刷新会发现 SessionId 不再变化。","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"Spring中9种经典设计模式","slug":"spring-design-pattern","date":"2023-02-12T13:51:22.000Z","updated":"2023-02-12T13:51:22.000Z","comments":false,"path":"spring-design-pattern/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/spring-design-pattern/index.html","excerpt":"","text":"目录 1. 简单工厂(非23种设计模式一种) 2. 工厂方法 3. 单例模式 4. 适配器模式 5. 装饰器模式 6. 代理模式 7. 观察者模式 8. 策略模式 9. 模板方法模式 1. 简单工厂(非23种设计模式一种) 实现方式 Spring中的BeanFactory就是简单工厂模式的体现，根据传入一个唯一的表示来获取Bean对象，但是否在传入参数后创建还是传入参数前创建这个要根据具体情况来定。 实质 由一个工厂类根据传入的参数，动态决定应该创建哪一个类 实现原理 bean容器的启动阶段 读取bean的xml配置文件，将bean元素分别转化为一个BeanDefinition对象 然后通过BeanDefinitionRegistry将这些bean注册到beanFactory中，保存在它的一个ConcurrentHashMap中 将BeanDefinition注册到了BeanFactory之后，在这里Spring为我们提供了一个扩展的切口，允许我们通过实现接口BeanFactoryPostProcessor 在此处来插入我们定义的代码 典型的例子就是：PropertyPlaceholderConfigurer，我们一般在配置数据库的dataSource时使用到的占位符的值，就是它注入进去的。 容器中bean的实例化阶段 主要是通过反射或者CGLIB对Bean进行实例化，在这个阶段Spring又给我们暴露了很多的扩展点 各种Aware接口：比如BeanFactoryAware，对于实现了这些Aware接口的bean，在实例话bean时Spring会帮我们注入对应的BeanFactory实例 BeanPostProcessor接口：实现了BeanPostProcessor接口的bean，在实例化bean时Spring会帮我们调用接口中的方法 InitializingBean接口：实现了InitializingBean接口的bean，在实例化bean时Spring会帮我们调用接口中的方法 DisposableBean接口：实现了BeanPostProcessor接口的bean，在该bean死亡时Spring会帮我们调用接口中的方法。 设计意义 松耦合：可以将原来硬编码的依赖，通过Spring这个beanFactory这个工厂来注入依赖，也就是说原来只有依赖方和被依赖方，现在我们引入了第三方——spring这个beanFactory，由它来解决bean之间的依赖问题，达到了松耦合的效果 Bean的额外处理：通过Spring接口的暴露，在实例化bean的阶段我们可以进行一些额外的处理，这些额外的处理只需要让bean实现对应的接口即可，那么spring就会在bean的生命周期调用我们实现的接口来处理该bean(重要) 2. 工厂方法 实现方式 FactoryBean接口 实现原理 实现了FactoryBean接口的bean是一类叫做factory的bean，其特点是：spring会在使用getBean()调用获得该bean时，会自动调用该bean的getObject()方法，所以返回的不是factory这个bean, 而是这个bean.getObject()方法的返回值。 例子 典型的例子有spring和shiro的结合 可以看到上面的ShiroFilterFactoryBean因为实现了FactoryBean接口，所以返回的不是 ShiroFilterFactoryBean 的实例，而是它的 ShiroFilterFactoryBean.getObject() 的返回值。 123456 public AbstractShiroFilter getObject() throws Exception &#123; if (this.instance == null) &#123; this.instance = this.createInstance(); &#125; return this.instance;&#125; 3. 单例模式 Spring依赖注入Bean实例默认是单例的 Spring的依赖注入(包括lazy-init方式)都是发生在AbstractBeanFactory的getBean里，getBean的doGetBean方法调用getSingleton进行bean 的创建 分析getSingleton()方法 123456789101112131415161718192021222324252627public Object getSingleton(String beanName)&#123; //参数true设置标识允许早期依赖 return getSingleton(beanName,true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; //检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; //如果为空，则锁定全局变量并进行处理。 synchronized (this.singletonObjects) &#123; //如果此bean正在加载，则不处理 singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; //当某些方法需要提前初始化的时候则会调用addSingleFactory 方法将对应的ObjectFactory初始化策略存储在singletonFactories ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; //调用预先设定的getObject方法 singletonObject = singletonFactory.getObject(); //记录在缓存中，earlysingletonObjects和singletonFactories互斥 this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125; getSingleton()过程图 Spring依赖注入时，使用了双重判断加锁的单例模式 总结： 单例模式定义：保证一个类仅有一个实例，并且提供一个访问它的全局访问点 Spring对单例的实现：spring总的单例模式完成了后半句化，即提供了全局的访问点BeanFactory。但没有从构造器级别去控制单例，这是因为spring管理的是任意Java对象。 4. 适配器模式 实现方式 SpringMVC中的适配器HandlerAdatper 实现原理 HandlerAdatper根据Handler规则执行不同的Handler 实现过程 DispatcherServlet根据HandlerMapping返回的handler，向HandlerAdatper发起请求，处理Handler。 HandlerAdapter根据规则找到对应的Handler并让其执行，执行完毕后Handler会向HandlerAdapter返回一个ModelAndView，最后由HandlerAdapter向DispatchServelet返回一个ModelAndView。 实现意义 HandlerAdatper使得Handler的扩展变得容易，只需要增加一个新的Handler和一个对应的HandlerAdapter即可。 因此Spring定义了一个适配接口，使得每一种Controller有一种对应的适配器实现类，让适配器代替controller执行相应的方法。这样在扩展Controller时，只需要增加一个适配器类就完成了SpringMVC的扩展了。 5. 装饰器模式 实现方式 Spring中用到的包装器模式在类名上有两种表现：一种是类名中含有Wrapper，另一种是类名中含有Decorator 实质 动态地给一个对象添加一些额外的职责。 就增加功能来说，Decorator模式相比生成子类更为灵活 6. 代理模式 实现方式 AOP底层，就是动态代理模式的实现 动态代理：在内存中构建的，不需要手动编写代理类 静态代理：需要手工编写代理类，代理类引用被代理对象 实现原理 切面在应用运行的时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象创建动态的创建一个代理对象。SpringAOP就是以这种方式织入切面的。 织入：把切面应用到目标对象并创建新的代理对象的过程 7. 观察者模式 实现方式： spring的事件驱动模型采用的是观察者模式，Spring中 ObServer模式常用的地方就是listener的实现 具体实现 事件机制的实现需要三个部分：事件源、事件、事件监听器 ApplicationContext接口 ApplicationContext是spring中的全局容器，翻译过来是：应用上下文 实现了ApplicationEventPublisher接口 职责：负责读取bean的配置文档，管理bean的加载，维护bean之间的依赖关系，可以说是负责bean的整个生命周期(IOC容器) 代码： 1234567891011121314public interface ApplicationEventPublisher &#123; void publishEvent(ApplicationEvent event);&#125;public void publishEvent(ApplicationEvent event) &#123; Assert.notNull(event, &quot;Event must not be null&quot;); if (logger.isTraceEnabled()) &#123; logger.trace(&quot;Publishing event in &quot; + getDisplayName() + &quot;: &quot; + event); &#125; getApplicationEventMulticaster().multicastEvent(event); if (this.parent != null) &#123; this.parent.publishEvent(event); &#125;&#125; ApplicationEvent抽象类【事件】 继承自jdk的EventObject，所有的事件都需要继承ApplicationEvent，并且通过构造参数source得到事件源 该类的实现类ApplicationContextEvent表示ApplicationContext的容器事件 代码： 1234567891011public abstract class ApplicationEvent extends EventObject &#123; private static final long serialVersionUID = 7099057708183571937L; private final long timestamp; public ApplicationEvent(Object source) &#123; super(source); this.timestamp = System.currentTimeMillis(); &#125; public final long getTimestamp() &#123; return this.timestamp; &#125;&#125; ApplicationListener接口【事件监听器】 继承自jdk的EventListener，所有的监听器都要实现这个接口 这个接口只有一个onApplicationEvent()方法，该方法接收一个ApplicationEvent或其子类对象作为参数，在方法体中，可以通过不同对Event类的判断来进行相应的处理 当事件触发时所有的监听器都会收到消息 代码： 123public interface ApplicationListener&lt;E extends ApplicationEvent&gt; extends EventListener &#123; void onApplicationEvent(E event);&#125; ApplicationEventMulticaster抽象类【事件源中publishEvent方法需要调用其getApplicationEventMulticaster方法】 属于事件广播器，它的作用是把ApplicationContext发布的Event广播给所有的监听器 代码： 12345678910111213141516public abstract class AbstractApplicationContext extends DefaultResourceLoader implements ConfigurableApplicationContext, DisposableBean &#123; private ApplicationEventMulticaster applicationEventMulticaster; protected void registerListeners() &#123; // Register statically specified listeners first. for (ApplicationListener&lt;?&gt; listener : getApplicationListeners()) &#123; getApplicationEventMulticaster().addApplicationListener(listener); &#125; // Do not initialize FactoryBeans here: We need to leave all regular beans // uninitialized to let post-processors apply to them! String[] listenerBeanNames = getBeanNamesForType(ApplicationListener.class, true, false); for (String lisName : listenerBeanNames) &#123; getApplicationEventMulticaster().addApplicationListenerBean(lisName); &#125; &#125;&#125; 8. 策略模式 实现方式: Spring框架的资源访问Resource接口，该接口提供了更强的资源访问能力，Spring框架本省大量使用了Resource接口来访问底层资源 Resource接口 source接口是具体资源访问策略的抽象，也是所有资源访问类所实现的接口 Resource接口主要提供了以下几个方法： getInputStream(): 定位并打开资源，返回资源对应的输入流，每次调用都返回新的输入流，调用则必须负责关闭 exists(): 返回Resource所只想的资源是否存在 isOpen(): 返回资源文件是否打开，如果资源文件不能多次读取，每次读取结束应该显式关闭，以防止资源泄漏 getDescription()： 返回资源的描述信息，通常用于资源处理出错时输出该信息，通常是全限定文件名或实际 URL。 getFile： 返回资源对应的 File 对象。 getURL： 返回资源对应的 URL 对象。 Resource 接口本身没有提供访问任何底层资源的实现逻辑，针对不同的底层资源，Spring 将会提供不同的 Resource 实现类，不同的实现类负责不同的资源访问逻辑。 Spring 为 Resource 接口提供了如下实现类： UrlResource： 访问网络资源的实现类。 ClassPathResource： 访问类加载路径里资源的实现类。 FileSystemResource： 访问文件系统里资源的实现类。 ServletContextResource： 访问相对于ServletContext路径里的资源的实现类. InputStreamResource： 访问输入流资源的实现类。 ByteArrayResource： 访问字节数组资源的实现类。 这些 Resource 实现类，针对不同的的底层资源，提供了相应的资源访问逻辑，并提供便捷的包装，以利于客户端程序的资源访问。 9. 模板方法模式 经典模板方法定义： 父类定义了骨架(调用哪些方法及顺序)，某些特定方法由子类实现。 最大的好处：代码复用，减少重复代码。除了子类要实现的特定方法，其他方法及方法调用顺序都在父类中预先写好了。 所以父类模板方法中有两类方法： 共同的方法： 所有子类都会用到的代码 不同的方法： 子类要覆盖的方法，分为两种： 抽象方法：父类中的是抽象方法，子类必须覆盖 钩子方法：父类中是一个空方法，子类继承了默认也是空的 注：为什么叫钩子，子类可以通过这个钩子(方法)，控制父类，因为这个钩子实际是父类的方法(空方法)！ Spring模板方法模式实质： 是模板方法模式和回调模式的接口，是Template Method不需要继承的另一种实现方式，Spring几乎所有的外接扩展都采用这种模式 具体实现： JDBC的抽象和对Hibernate的集成，都采用了一种理念或者处理方式，那就是模板方法模式与相应的Callback接口相结合。 采用模板方法模式是为了以一种统一而集中的方式来处理资源的获取和释放 以JdbcTemplate为例: 123456789101112131415161718public abstract class JdbcTemplate &#123; public final Object execute(String sql)&#123; Connection con=null; Statement stmt=null; try&#123; con=getConnection(); stmt=con.createStatement(); Object retValue=executeWithStatement(stmt,sql); return retValue; &#125;catch(SQLException e)&#123; //... &#125;finally&#123; closeStatement(stmt); releaseConnection(con); &#125; &#125; protected abstract Object executeWithStatement(Statement stmt, String sql);&#125; 引入回调的原因： JdbcTemplate是抽象类，不能够独立使用，我们每次进行数据访问的时候都要给出一个相应子类实现，这样肯定不方便，所以引入来回调 回调代码： 123public interface StatementCallback&#123; Object doWithStatement(Statement stmt);&#125; 利用回调方法重写JdbcTemplate方法 123456789101112131415161718public class JdbcTemplate &#123; public final Object execute(StatementCallback callback)&#123; Connection con=null; Statement stmt=null; try&#123; con=getConnection(); stmt=con.createStatement(); Object retValue=callback.doWithStatement(stmt); return retValue; &#125;catch(SQLException e)&#123; //... &#125;finally&#123; closeStatement(stmt); releaseConnection(con); &#125; &#125; //... 其它方法定义&#125; Jdbc使用方法如下： 12345678JdbcTemplate jdbcTemplate=...; final String sql=...; StatementCallback callback=new StatementCallback()&#123; public Object=doWithStatement(Statement stmt)&#123; return ...; &#125;&#125;jdbcTemplate.execute(callback); 为什么JdbcTemplate没有使用继承？ 因为这个类的方法太多，但是我们还是想用到JdbcTemplate已有的稳定的、公用的数据库连接，那么我们怎么办呢？ 我们可以把变化的东西抽出来作为一个参数传入JdbcTemplate的方法中。但是变化的东西是一段代码，而且这段代码会用到JdbcTemplate中的变量。怎么办？ 那我们就用回调对象吧。在这个回调对象中定义一个操纵JdbcTemplate中变量的方法，我们去实现这个方法，就把变化的东西集中到这里了。然后我们再传入这个回调对象到JdbcTemplate，从而完成了调用。 最后感谢博主！原文地址：Spring 中经典的 9 种设计模式，打死也要记住啊！","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"spring","slug":"spring","permalink":"https://xiaoyuge5201.github.io/tags/spring/"}]},{"title":"如何停止一个正在运行的线程","slug":"thread-stop","date":"2023-02-07T09:14:29.000Z","updated":"2023-02-07T09:14:29.000Z","comments":false,"path":"thread-stop/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/thread-stop/index.html","excerpt":"","text":"1. 前言 停止一个线程更意味着在任务处理完成任务之前停掉正在做的操作，也就是放弃当前的操作。停止一个线程可以用Thread.stop()方法，但最好不要用它。虽然它确实可以停止一个正在运行的线程，但是这个方法是不安全的，而且已被废弃。 在Java中有以下3种方法可以终止正在运行的线程： 使用退出标志，使线程正常退出，也就是当run方法完成后线程终止 使用stop方法强行终止，但是不推荐；因为stop和suspend以及resume一样都是过期作废的方法 使用interrupt方法中断线程 2. 停止不了的线程 interrupt()方法的使用效果并不像for+break语句那样，马上就停止循环，调用interrupt()方法是在当前线程中打一个停止标志，并不是真的停止线程。 1234567891011121314151617181920212223public class ThreadDemo extends Thread &#123; @Override public void run() &#123; super.run(); for (int i = 0; i &lt; 100000; i++) &#123; System.out.println(&quot;i=&quot;+(i+1)); &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try&#123; //这里休眠2秒，让线程执行一段时间，2s后中断线程 Thread.sleep(2000); thread.interrupt(); &#125;catch (Exception ex)&#123; ex.printStackTrace(); &#125; &#125;&#125; 输出结果： 12345678...i=99994i=99995i=99996i=99997i=99998i=99999i=100000 3. 判断线程是否停止状态 Thread.java类提供了两种方法： this.interrupted()：测试当前线程是否已经中断，当前线程是指运行this.interrput()方法的线程 12345678910111213141516171819202122232425public class ThreadDemo extends Thread &#123; @Override public void run() &#123; super.run(); for (int i = 0; i &lt; 1000000; i++) &#123; i++; &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try &#123; Thread.sleep(2000); thread.interrupt(); System.out.println(&quot;stop 1??&quot; + Thread.interrupted()); System.out.println(&quot;stop 2??&quot; + Thread.interrupted()); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 运行结果： 12stop 1??falsestop 2??false 从控制台输出信息可以看出：线程并未停止，这也证明来interrupt()方法的解释，测试当前线程是否已经中断，这个当前线程是main,它未被中断，所以打印的两个结果都是false； 如何使main线程产生中断效果呢？ 123456789public class TestThreadDemo2 &#123; public static void main(String[] args) &#123; Thread.currentThread().interrupt(); System.out.println(&quot;stop 1??&quot; + Thread.interrupted()); System.out.println(&quot;stop 2??&quot; + Thread.interrupted()); System.out.println(&quot;end&quot;); &#125;&#125; 输出： 123stop 1??truestop 2??falseend 方法interrupt()的确判断出当前线程是否停止状态，但是为什么第2个为false呢？ 官方帮助文档对interrupt()方法的解释是：**测试当前线程是否已经中断。线程的中断状态有该方法清除。**换句话说：如果连续两次调用该方法，则第二次返回false; this.isInterrupted()：测试线程是否已经中断 123456789public class IsInterruptDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); thread.interrupt(); System.out.println(&quot;stop 1??&quot; + thread.isInterrupted()); System.out.println(&quot;stop 2??&quot; + thread.isInterrupted()); &#125;&#125; 运行结果: 12stop 1??truestop 2??true isInterrupted()并为清除状态，所以打印来两个true 4. 停止线程-异常法 有了前面的知识，就可以在线程中用for来判断线程是否是停止状态，如果是停止状态，则后面的代卖不再运行 123456789101112131415161718192021222324252627public class ThreadDemo extends Thread &#123; @Override public void run() &#123; super.run(); for (int i = 0; i &lt; 500000; i++) &#123; if (Thread.interrupted()) &#123; System.out.println(&quot;线程已经终止，for循环不再执行&quot;); break; &#125; System.out.println(&quot;i = &quot; + (i + 1)); &#125; System.out.println(&quot;这是for循环外的语句，也会被执行&quot;); &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try &#123; Thread.sleep(1000); thread.interrupt(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 运行结果： 12345678......i = 285135i = 285136i = 285137i = 285138i = 285139线程已经终止，for循环不再执行这是for循环外的语句，也会被执行 虽然停止了线程，但是如果for语句下面还有语句，还是会继续执行的。如果不需要继续执行 只需要在上面break的地方改成throw new InterruptedException(); 即可，这样就会抛出异常。 5. 停止线程-sleep法 如果线程在sleep状态下停止线程，会是什么效果？ 1234567891011121314151617181920212223242526public class ThreadDemo extends Thread &#123; @Override public void run() &#123; super.run(); try &#123; System.out.println(&quot;线程开始....&quot;); Thread.sleep(200000); System.out.println(&quot;线程结束。&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;在沉睡中被停止, 进入catch， 调用isInterrupted()方法的结果是：&quot;+this.isInterrupted()); e.printStackTrace(); &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try &#123; thread.interrupt(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 运行结果： 12345线程开始....在沉睡中被停止, 进入catch， 调用isInterrupted()方法的结果是：falsejava.lang.InterruptedException: sleep interruptedtrueat java.lang.Thread.sleep(Native Method)trueat org.example.ThreadDemo.run(ThreadDemo.java:13) 从打印的结果来看，如果在sleep状态下停止线程，会进入catch语句，并且清除停止状态值，使之变为false。 前一个实验是先sleep然后再用interrupt()挺尸，与之相反的操作如下： 123456789.......i=9997i=9998i=9999i=10000先停止，再遇到sleep 进入catchjava.lang.InterruptedException: sleep interruptedtrueat java.lang.Thread.sleep(Native Method)trueat org.example.ThreadDemo.run(ThreadDemo.java:16) 6. 停止线程-暴力停止 使用stop()方法停止线程是非常暴力的，且方法已经被废弃了。 1234567891011121314151617181920212223242526272829public class ThreadDemo extends Thread &#123; private int i = 0; @Override public void run() &#123; super.run(); try &#123; //模拟线程一直在运行 while (true) &#123; System.out.println(&quot;i = &quot; + i); i++; Thread.sleep(200); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try&#123; Thread.sleep(2000); thread.stop(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 运行结果: 123456789101112i = 0i = 1i = 2i = 3i = 4i = 5i = 6i = 7i = 8i = 9Process finished with exit code 0 7. 方法stop()与java.lang.ThreadDeath异常 调用stop()方法时会抛出java.lang.ThreadDeath异常，但是通常情况下，此异常不需要显示地捕捉。 123456789101112131415161718public class ThreadDemo extends Thread &#123; @Override public void run() &#123; super.run(); try &#123; this.stop(); &#125; catch (ThreadDeath e) &#123; System.out.println(&quot;进入catch&quot;); e.printStackTrace(); &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); &#125;&#125; 运行结果： 1234进入catchjava.lang.ThreadDeathtrueat java.lang.Thread.stop(Thread.java:853)trueat org.example.ThreadDemo.run(ThreadDemo.java:11) stop()方法已经作废，因为如果强制让线程停止有可能是一些清理性的工作得不到完整，另一种情况就是对锁定的对象进行了解锁，导致数据得不到同步的处理，出现数据不一致的情况。 释放锁的不良后果 使用stop()释放锁将会给数据造成不一致性的结果。如果出现这样的情况，程序处理的数据就有可能遭到破坏，最终导致程序执行的流程错误，一定要特别注意： 12345678910111213141516171819202122232425262728293031323334353637public class SynchronizedObject &#123; private String name = &quot;a&quot;; private String password = &quot;aa&quot;; public synchronized void printString(String name, String password)&#123; try &#123; this.name = name; Thread.sleep(100000); this.password = password; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //省略getter/setter方法&#125;public class MyThread extends Thread &#123; private SynchronizedObject synchronizedObject; public MyThread(SynchronizedObject synchronizedObject)&#123; this.synchronizedObject = synchronizedObject; &#125; public void run()&#123; synchronizedObject.printString(&quot;b&quot;, &quot;bb&quot;); &#125;&#125;public class TestThreadDemo &#123; public static void main(String args[]) throws InterruptedException &#123; SynchronizedObject synchronizedObject = new SynchronizedObject(); Thread thread = new MyThread(synchronizedObject); thread.start(); Thread.sleep(500); thread.stop(); System.out.println(synchronizedObject.getName() + &quot; &quot; + synchronizedObject.getPassword()); &#125;&#125; 输出结果： 1b aa 由于stop()方法以及在JDK中被标明为“过期/作废”的方法，显然它在功能上具有缺陷，所以不建议在程序张使用stop()方法。 8. 使用return停止线程 将方法interrupt()与return结合使用也能实现停止线程的效果： 123456789101112131415161718192021222324public class ThreadDemo extends Thread &#123; @Override public void run() &#123; while (true) &#123; if (this.isInterrupted()) &#123; System.out.println(&quot;线程被停止了&quot;); return; &#125; System.out.println(&quot;time:&quot; + System.currentTimeMillis()); &#125; &#125;&#125;public class TestThreadDemo &#123; public static void main(String[] args) &#123; Thread thread = new ThreadDemo(); thread.start(); try &#123; Thread.sleep(2000); thread.interrupt(); &#125; catch (Exception ex) &#123; ex.printStackTrace(); &#125; &#125;&#125; 输出结果： 1234567.......time:1676209667113time:1676209667113time:1676209667113time:1676209667113time:1676209667113线程被停止了 总结：建议使用“抛异常”的方法来实现线程的停止，因为在catch块中还可以将异常向上抛，使线程停止事件得以传播。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"}]},{"title":"HashMap的底层原理","slug":"hashmap","date":"2023-02-05T13:13:51.000Z","updated":"2023-02-05T13:13:51.000Z","comments":false,"path":"hashmap/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/hashmap/index.html","excerpt":"","text":"1. 简介 HashMap是java后端面试必问内容，其中一个高频问题就是：HashMap的底层原理是怎样的？ 本文介绍HashMap的原理，包括数据结构、存储机制、hashCode方法。 2. HashMap原理总结 HashMap其实就是一个大的数组，将key的hashCode作为数组的下标，将value作为数组的值，如果key的hashCode重复（即：数组下标重复） ，则将新的key和旧的key放到链表中。 如果链表长度大于等于8（且数组大小大于等于64），则将链表改为红黑树（提高查询的效率） 如果红黑树节点个数小于6，则将红黑树转化为链表。 3. 数据结构 数组和链表 数据结构中有数组和链表来实现对数据的存储，但这两者个有利弊。 哈希表 哈希表：综合数组和链表的特性，查找（寻址）容易，插入删除容易，占空间中等的数据结构。 哈希表有多种不同的实现方法，HashMap则使用的是拉链法，也叫链地址法。 例如： 哈希表的数组初始化长度为16，每个元素存储的是一个链表的头结点，这些元素按照hash(key)% len获得，也就是元素key的哈希值对数组长度区模得到； 12 % 16=12；28 % 16=12；108 % 16=12；140 % 16=12 所以： 12， 28， 108， 140都存储在数组下标为12的位置 红黑树 4. 存储机制 键值对的数据 每个键值对都是一个Node&lt;K,V&gt;对象，它实现了Map.Entry&lt;K,V&gt;接口。Node&lt;K,V&gt;有四个属性：hash、key、value、next(下一个结点) 12345678910public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; &#125; // 其他代码&#125; 既然是线性数组，为什么能随机存取？这里HashMap用来一个小算法，大致实现如下： 123456789// 存储时:int hash = key.hashCode();int index = hash % Entry[].length;Entry[index] = value; // 取值时:int hash = key.hashCode();int index = hash % Entry[].length;return Entry[index]; 5. put方法 哈希冲突：若两个key通过hash(key) % Entry[].length得到的index相同，怎么处理？HashMap用的是链表，Entry类里面有一个next属性，指向下一个Entry. 比如： 第一个键值对A进来，通过计算key的hash得到的index=0， 记做：Entry[0] = A 又进来一个键值对B，通过计算key的hash得到的index=0， HashMap会这样做：B.next = A, Entry[0] = B 又进来C，index也等于0，那么C.next = B,Entry[0] = C； 这样我们发现index = 0的地方其实利用单链表的头插法存储了A，B，C三个键值对，他们通过next属性连接在一起，所以会发生覆盖的情况，数组中存储的总是最后插入的元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//部分代码transient Node&lt;K,V&gt;[] table;transient int size;static final int TREEIFY_THRESHOLD = 8;int threshold; public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 若链表的节点数大于等于8，则转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // 如果key在链表中已存在，则替换为新value if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null;&#125; 6. get方法 1234567891011121314public V get(Object key) &#123; if (key == null) return getForNullKey(); int hash = hash(key.hashCode()); //先定位到数组元素，再遍历该元素处的链表 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) return e.value; &#125; return null;&#125; 7. null key的存取 null key总是存放在Entry[]数组的第一个元素 123456789101112131415161718192021private V putForNullKey(V value) &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(0, null, value, 0); return null;&#125; private V getForNullKey() &#123; for (Entry&lt;K,V&gt; e = table[0]; e != null; e = e.next) &#123; if (e.key == null) return e.value; &#125; return null; &#125; 确定数组 index: hashcode % table.length 取模 HashMap存取时，都需要计算当前key对应Entry[]数组哪个元素，即计算数组下标，算法如下： 1234// Returns index for hash code h.static int indexFor(int h, int length) &#123; return h &amp; (length-1);&#125; 换位取并，作用上相当于取模mod获取取余%;这意味着数组下标相同，并不表示hashcode相同。 table初始大小 1234567891011public HashMap(int initialCapacity, float loadFactor) &#123; ..... // Find a power of 2 &gt;= initialCapacity int capacity = 1; while (capacity &lt; initialCapacity) capacity &lt;&lt;= 1; this.loadFactor = loadFactor; threshold = (int)(capacity * loadFactor); table = new Entry[capacity]; init();&#125; 注意：table初始大小并不是构造函数中的initialCapacity, 而是 &gt;= initialCapacity的2的n次幂 8. hashCode方法 String#hashCode 123456789101112public int hashCode() &#123; int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) &#123; char val[] = value; for (int i = 0; i &lt; value.length; i++) &#123; h = 31 * h + val[i]; &#125; hash = h; &#125; return h;&#125; 为什么乘 31 呢？ 选31是因为它是一个奇素（质）数，这里有两层意思：奇数 &amp;&amp; 素数 什么是奇数，偶数不行？ 因为如果乘子是偶数，并且当乘法溢出的时候(数太大，int装不下)，相当于在做位移运算，有信息就损失了 比如说只给2bit空间，二进制的10，乘以2相当于左移1位，10(bin) &lt;&lt; 1 = 00,1就损失了。 为什么是31 h * 31 == (h &lt;&lt; 5) - h;现代虚拟机会自动做这样的优化，计算快。 再反观其他数： h * 7 == (h &lt;&lt; 3) - h; //太小，容易hash冲突 h * 15 == (h &lt;&lt; 4) - h; //15不是素数 h*31 == (h&lt;&lt;5)-h; // 31既是素数又不大不小刚刚好 h*63 == (h&lt;&lt;6)-h; // 63不是素数 h*127 == (h&lt;&lt;7)-h; // 太大了，乘不到几下就溢出了 实例追踪： 123456789&quot;abc&quot;.hashCode()hash为：0value为：[a, b, c]第一次循环：h = 0 + 97 = 97第二次循环：h = 31 * 97 + 98 = 3105第三次循环：h = 3105 * 31 + 99 = 9635","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Plumelog一个简单易用的java分布式日志组件","slug":"plumelog","date":"2023-02-05T08:54:32.000Z","updated":"2023-02-05T08:54:32.000Z","comments":false,"path":"plumelog/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/plumelog/index.html","excerpt":"","text":"1. 系统介绍 无代码侵入的分布式日志系统，基于log4j、log4j2、logback搜集日志，设置链路ID，方便查询管关联日志 基于elasticsearch作为查询引擎 高吞吐，查询效率高 全程不占应用程序本地磁盘空间，免维护；对于项目透明，不影响项目本身运行 无需修改老项目，引入直接使用，支持dubbo，支持springCloud 2. 架构 plumelog-core 核心组件包含日志搜集端，负责搜集日志并推送到kafka，redis等队列 plumelog-server 负责把队列中的日志日志异步写入到elasticsearch plumelog-demo 基于springboot的使用案例 plumelog-lite plumelog的嵌入式集成版本，免部署 3. 使用方法 plumelog分三种启动模式，分别为redis,kafka,lite，外加嵌入式版本plumelog-lite lite模式，不依赖任何外部中间件直接启动使用，但是性能有限，一天10个G以内可以应付，还必须是SSD硬盘，适合管理系统类的小玩家 redis,kafka模式可以集群分布式部署，适合大型玩家，互联网公司 plumelog-lite plumelog的嵌入式集成版本，直接pom引用，嵌入在项目中，自带查询界面，适合单个独立小项目使用，外包软件的最佳伴侣 使用方法访问源码wikiPlumelog使用方法","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"screw 数据库表结构文档生成工具","slug":"screw","date":"2023-02-02T12:55:11.000Z","updated":"2023-02-02T12:55:11.000Z","comments":false,"path":"screw/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/screw/index.html","excerpt":"","text":"1. 简介 在企业级开发中、我们经常会有编写数据库表结构文档的时间付出，每次需要手动进行维护到文档中，很是繁琐、如果忘记一次维护、就会给以后工作造成很多困扰、无形中制造了很多坑留给自己和后人，随着项目的需求频繁设计和更改数据库、使用此插件、节省了很多时间，解决了很多问题 。 虽然是细小的螺丝钉，是个细微的小齿轮，然而如果缺了它，那整个的机器就无法运转了，慢说是缺了它，即使是一枚小螺丝钉没拧紧，一个小齿轮略有破损，也要使机器的运转发生故障的… 本篇文章用到的开源项目地址是：https://toscode.gitee.com/leshalv/screw 2. 特点 简介、轻量、设计良好 多数据库支持 已支持：Mysql、Mariadb、Tidb、oracle、sqlserver、postgreSql、Cache DB（2016） 开发中：H2、DB2、HSQL、SQLite、达梦、虚谷、人大金仓、瀚高 多种格式文档 灵活扩展 支持自定义模板 3. 文档生成支持 html word markdown 4. 使用案例-pom引入 引入依赖 123456789101112131415161718192021222324252627282930313233343536373839&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.freemarker&lt;/groupId&gt; &lt;artifactId&gt;freemarker&lt;/artifactId&gt; &lt;version&gt;2.3.31&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;cn.smallbun.screw&lt;/groupId&gt; &lt;artifactId&gt;screw-core&lt;/artifactId&gt; &lt;version&gt;1.0.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 配置数据库连接信息 12345spring.datasource.url=jdbc:mysql://localhost:3306/gang?useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;serverTimezone=Asia/Shanghai spring.datasource.username= rootspring.datasource.password= xiaoyugespring.datasource.driver-class-name=com.mysql.cj.jdbc.Driverspring.datasource.xa.properties.useInformationSchema=true 编写测试代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081import cn.smallbun.screw.core.Configuration;import cn.smallbun.screw.core.engine.EngineConfig;import cn.smallbun.screw.core.engine.EngineFileType;import cn.smallbun.screw.core.engine.EngineTemplateType;import cn.smallbun.screw.core.execute.DocumentationExecute;import cn.smallbun.screw.core.process.ProcessConfig;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.context.ApplicationContext;import org.springframework.test.context.junit4.SpringRunner;import javax.sql.DataSource;import java.util.ArrayList;import java.util.Arrays;import java.util.List; @SpringBootTest(classes = App.class)@RunWith(SpringRunner.class)public class AppTest &#123; @Autowired ApplicationContext applicationContext; @Test public void exportDbDoc() &#123; DataSource dataSourceMysql = applicationContext.getBean(DataSource.class); // 生成文件配置 EngineConfig engineConfig = EngineConfig.builder() // 生成文件路径，自己mac本地的地址，这里需要自己更换下路径 .fileOutputDir(&quot;/Users/xiaoyuge/Desktop/screw&quot;) // 打开目录 .openOutputDir(false) // 文件类型,我这用的是HTML, EngineFileType有以下三种类型： //HTML(&quot;.html&quot;, &quot;documentation_html&quot;, &quot;HTML文件&quot;), //WORD(&quot;.doc&quot;, &quot;documentation_word&quot;, &quot;WORD文件&quot;), //MD(&quot;.md&quot;, &quot;documentation_md&quot;, &quot;Markdown文件&quot;); .fileType(EngineFileType.HTML) // 生成模板实现 .produceType(EngineTemplateType.freemarker).build(); // 生成文档配置（包含以下自定义版本号、描述等配置连接） Configuration config = Configuration.builder() .version(&quot;1.0.5&quot;) .description(&quot;生成文档信息描述&quot;) .dataSource(dataSourceMysql) .engineConfig(engineConfig) .produceConfig(getProcessConfig()) .build(); // 执行生成 new DocumentationExecute(config).execute(); &#125; /** * 配置想要生成的表+ 配置想要忽略的表 * @return 生成表配置 */ public static ProcessConfig getProcessConfig()&#123; // 忽略表名 List&lt;String&gt; ignoreTableName = Arrays.asList(&quot;aa&quot;,&quot;test_group&quot;); // 忽略表前缀，如忽略a开头的数据库表 List&lt;String&gt; ignorePrefix = Arrays.asList(&quot;a&quot;,&quot;t&quot;); // 忽略表后缀 List&lt;String&gt; ignoreSuffix = Arrays.asList(&quot;_test&quot;,&quot;czb_&quot;); return ProcessConfig.builder() //根据名称指定表生成 .designatedTableName(new ArrayList&lt;String&gt;()) //根据表前缀生成 .designatedTablePrefix(new ArrayList&lt;String&gt;()) //根据表后缀生成 .designatedTableSuffix(new ArrayList&lt;String&gt;()) //忽略表名 .ignoreTableName(ignoreTableName) //忽略表前缀 .ignoreTablePrefix(ignorePrefix) //忽略表后缀 .ignoreTableSuffix(ignoreSuffix).build(); &#125;&#125; 查看数据库文档 5. 使用案例-Maven插件 在pom.xml文件中添加以下配置即可 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;cn.smallbun.screw&lt;/groupId&gt; &lt;artifactId&gt;screw-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;lastVersion&#125;&lt;/version&gt; &lt;dependencies&gt; &lt;!-- HikariCP --&gt; &lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt; &lt;/dependency&gt; &lt;!--mysql driver--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.20&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;configuration&gt; &lt;!--username--&gt; &lt;username&gt;root&lt;/username&gt; &lt;!--password--&gt; &lt;password&gt;password&lt;/password&gt; &lt;!--driver--&gt; &lt;driverClassName&gt;com.mysql.cj.jdbc.Driver&lt;/driverClassName&gt; &lt;!--jdbc url--&gt; &lt;jdbcUrl&gt;jdbc:mysql://127.0.0.1:3306/xxxx&lt;/jdbcUrl&gt; &lt;!--生成文件类型--&gt; &lt;fileType&gt;HTML&lt;/fileType&gt; &lt;!--打开文件输出目录--&gt; &lt;openOutputDir&gt;false&lt;/openOutputDir&gt; &lt;!--生成模板--&gt; &lt;produceType&gt;freemarker&lt;/produceType&gt; &lt;!--文档名称 为空时:将采用[数据库名称-描述-版本号]作为文档名称--&gt; &lt;fileName&gt;测试文档名称&lt;/fileName&gt; &lt;!--描述--&gt; &lt;description&gt;数据库文档生成&lt;/description&gt; &lt;!--版本--&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; &lt;!--标题--&gt; &lt;title&gt;数据库文档&lt;/title&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;run&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://xiaoyuge5201.github.io/tags/%E5%B7%A5%E5%85%B7/"}]},{"title":"Magic-api Java接口快速开发框架","slug":"magic-api","date":"2023-01-30T14:57:56.000Z","updated":"2023-01-30T14:57:56.000Z","comments":false,"path":"magic-api/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/magic-api/index.html","excerpt":"","text":"1 简介 magic-api 是一个基于Java的接口快速开发框架，编写接口将通过magic-api提供的UI界面完成，自动映射为HTTP接口，无需定义Controller、Service、Dao、Mapper、XML、VO等Java对象即可完成常见的HTTP API接口开发 2 特性 支持MySQL、MariaDB、Oracle、DB2、PostgreSQL、SQLServer 等支持jdbc规范的数据库 支持非关系型数据库Redis、Mongodb 支持集群部署、接口自动同步。 支持分页查询以及自定义分页查询 支持多数据源配置，支持在线配置数据源 支持SQL缓存，以及自定义SQL缓存 支持自定义JSON结果、自定义分页结果 支持对接口权限配置、拦截器等功能 支持运行时动态修改数据源 支持Swagger接口文档生成 基于magic-script脚本引擎，动态编译，无需重启，实时发布 支持Linq式查询，关联、转换更简单 支持数据库事务、SQL支持拼接，占位符，判断等语法 支持文件上传、下载、输出图片 支持脚本历史版本对比与恢复 支持脚本代码自动提示、参数提示、悬浮提示、错误提示 支持导入Spring中的Bean、Java中的类 支持在线调试 支持自定义工具类、自定义模块包、自定义类型扩展、自定义方言、自定义列名转换等自定义操作 3 快速开始 引入依赖包 123456789101112131415161718192021222324252627282930313233&lt;!-- 父包 --&gt;&lt;parent&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;version&gt;2.3.5.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.25&lt;/version&gt; &lt;/dependency&gt; &lt;!-- magic-api依赖包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.ssssssss&lt;/groupId&gt; &lt;artifactId&gt;magic-api-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 修改application.yaml 1234567891011121314151617server: port: 8080 # 配置静态资源启用 gzip 压缩 compression: enabled: true min-response-size: 128#配置magic-apimagic-api: web: /magic/web #配置文件存储位置，当以classpath开头时，为只读模式 resource: #location: /data/magic-api type: database # 配置接口存储方式，这里选择存在数据库中 table-name: magic_api_file # 数据库中的表名 prefix: /magic-api # 前缀 # 其它配置请参考 https://ssssssss.org/magic-api/config/ 创建数据库以及数据表 12345678910111213-- api接口表 CREATE TABLE magic_api_file ( `file_path` varchar(512) NOT NULL, `file_content` mediumtext, PRIMARY KEY (`file_path`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;-- 测试表CREATE TABLE test_magic_api ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 启动程序 访问接口配置平台 1http://127.0.0.1:8080/magic/web/index.html 弹出一个类似与IDEA的页面，如下图所示： 4 配置Magic-api 创建分组 创建接口 查询列表命令为： 1234var sql = &quot;&quot;&quot; select * from magic_api_file; // magic_api_file为数据库表&quot;&quot;&quot;return db.select(sql); 单表保存命令 123456return db.table(&#x27;test_magic_api&#x27;) //表test_magic_api有两个字段， id 自增， name .primary(&#x27;id&#x27;) .save(&#123; id, // 当file_path不为null时做修改，否则做插入 name: name &#125;) 分页查询命令 1234var sql = &quot;&quot;&quot; select file_path from magic_api_file&quot;&quot;&quot;return db.page(sql,5,0) //每页5条，从第0开始 执行查询接口，结果如下图所示 这样一个简单的模拟查询数据库表记录的接口就完成了，省去了后台写的大量的测试代码，大大的节省了时间用来摸鱼。 具体需要查看更多关于magic-api的相关内容，可以查看 magic-api源码 以及 样例代码以及常用的脚本","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"MyPerf4J一个高性能、无侵入的Java性能监控和统计工具","slug":"myperf4j","date":"2023-01-28T14:12:23.000Z","updated":"2023-01-28T14:12:23.000Z","comments":false,"path":"myperf4j/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/myperf4j/index.html","excerpt":"","text":"1 简介 随着应用服务的规模不断扩大，原有的垂直应用架构已无法满足产品的发展，几十个工程师在一个项目里并行开发不同的功能，开发效率不断降低，于是公司开始全面推进服务化进程，把团队内的大部分工程师主要精力全部都集中到服务化中。 服务化可以让每个工程师仅在自己负责的子项目中进行开发，提高了开发的效率，但是服务化同时也带来了其他问题： 无法知道每个服务的运行情况，例如，某一台服务它目前的 QPS 是多少？它的平均延迟是多少，99% 的延迟是多少，99.9% 的延迟又是多少？ 某一个接口响应时间慢，如何定位是哪个方法引起的？ 每个服务的负载是否均衡？ 当服务出现抖动时，如何判断是 DB、Cache 还是下游服务引起的？ DB 和 Cache 响应延迟是多少？ 如何评估服务的容量，随着服务的调用量越来越大，这个服务需要多少机器来支撑？什么时候应该加机器？ 目前已有的监控工具要么过于重量级，要么没有我想要的性能指标，不能满足我的 监控需求，为此，MyPerf4J诞生了，它用来来帮助我们监控服务的运行情况以及快速定位问题。 1.1 需求 MyPerf4J最基本的需求： 能统计处方法的RPS、Avg、Min、Max、StdDev、TP90、TP95、TP99 等指标 可配置： 可指定统计某些类、某些方法 可指定不统计某些类、某些方法 拥有极致的性能 不影响应用的GC 不影响应用的RT 性能指标的处理可以定制化，例如：日志收集、上报给日志收集服务等。 1.2 指标 Method Metrics RPS: 每秒请求数 Count: 总请求数 RT: 方法响应时间 Avg: 方法平均响应时间 Min: 方法最小响应时间 Max: 方法最大响应时间 StdDev: 方法响应时间的标准差 TP50, TP90, TP95, TP99, TP999, TP9999, TP100 TP: Top 百分数(Top Percentile) TP90: 在一个时间段内（如1分钟），统计该方法每次调用所消耗的时间，并将这些时间按从小到大的顺序进行排序，取第 90% 的那个值作为 TP90 值 JVM GC Metrics YoungGcCount: 一个时间片内累计 YoungGC 次数 YoungGcTime: 一个时间片内累计 YoungGC 时间 AvgYoungGcTime: 一个时间片内 YoungGC 平均时间 FullGcCount: 一个时间片内累计 OldGC 次数 FullGcTime: 一个时间片内累计 OldGC 时间 JVM Memory Metrics EdenUsed: 当前已经使用的Eden区内存量（以 KB 为单位） EdenUsedPercent: 当前已经使用的Eden区内存量占比 SurvivorUsed: 当前已经使用的Survivor区内存量（以 KB 为单位） SurvivorUsedPercent: 当前已经使用的Survivor区内存量占比 OldGenUsed: 当前已经使用的老年代内存量（以 KB 为单位） OldGenUsedPercent: 当前已经使用的老年代内存量占比 HeapUsed: 当前已经使用的堆内内存量（以 KB 为单位） HeapUsedPercent: 当前已经使用的堆内内存量占比 NonHeapUsed: 当前已经使用的非堆内内存量（以 KB 为单位） NonHeapUsedPercent: 当前已经使用的非堆内内存量占比 PermGenUsed: 当前已经使用的永久代内存量（以 KB 为单位） PermGenUsedPercent: 当前已经使用的永久代内存量占比 MetaspaceUsed: 当前已经使用的元数据区内存量（以 KB 为单位） MetaspaceUsedPercent: 当前已经使用的元数据区内存量占比 CodeCacheUsed: 当前已经使用的 CodeCache区 内存量（以 KB 为单位） CodeCacheUsedPercent: 当前已经使用的 CodeCache区 内存量占比 JVM Thread Metrics TotalStarted: 自 JVM 启动以来启动过的线程数 Active: 当前存活的线程数，包括守护线程和非守护线程 Daemon: 当前存活的守护线程数 Runnable: 正在 JVM 中执行的线程 Blocked: 受阻塞并等待某个监视器锁的线程数 Waiting: 无限期地等待另一个线程来执行某一特定操作的线程数 TimedWaiting: 等待另一个线程来执行取决于指定等待时间的操作的线程处于这种状态数 Terminated: 已退出的线程数 Peak: 自 JVM 启动或峰值重置以来峰值活动线程计数 New: 至今尚未启动的线程数 JVM ByteBuff Metrics Name: 缓存池名称 Count: 缓存池中 buffer 的数量 MemoryUsed: JVM 用于此缓冲池的内存估计值 MemoryCapacity: 缓存池中所有 buffer 的总容量估计值 JVM Class Metrics Total: 自 JVM 开始执行到目前已经加载的类的总数 Loaded: 当前加载到 JVM 中的类的数量 Unloaded: 自 JVM 开始执行到目前已经卸载的类的总数 JVM Compilation Metrics Time: 一个时间片内累计编译时间 TotalTime: 自 JVM 开始执行到目前累计的的总编译时间 JVM FileDescriptor Metrics OpenCount: 当前打开的文件句柄数 OpenPercent: 当前打开的文件句柄数占最大文件句柄数的百分比 1.3 架构 MyPerf4j支持两种部署架构: 3.x及其之前 3.x 各组件说明 组件 说明 Java Application MyPerf4J 的运行容器 MyPerf4J Metrics 收集和统计 Collector 日志收集器 Storage 日志收集器 Collector 存储平台 Dashboard 可视化平台 各组件关系说明： MyPerf4j 定时把指定时间片内的统计数据写入日志文件 Collector 从日志文件中读取统计数据，并写入Storage Dashboard 从Storage 中读取数据并展示 注意，MyPerf4J项目 只提供MyPerf4J本身，其余组件需要用户自行选择。 这样做的优点如下： 保持 MyPerf4J 的精简 健壮性，不论是 Collector、Storage 还是 Dashboard 宕掉都不影响 MyPerf4J 的数据采集，也不丢失采集到的数据 多样性，Collector 可以是 Telegraf 也可以是 Filebeat；Storage 可以是 InfluxDB 也可以是 OpenTSDB; Dashboard 可以是 Grafana 也可以是 Chronograf 2 配置 这里主要是3.x配置，如果需要查看2.x配置，可以到 MyPerf4j 源码处查找。 MyPerf4J 默认提供了以下几个参数，用于控制 MyPerf4J 的行为: 属性 类型 必填 默认值 说明 app_name String Y 配置应用名称 debug boolean N false 配置是否开启 debug 模式，可配置为 true/false http.server.port int N 2048,2000,2040 配置 Http 服务器端口号，格式为：首选端口,备选最小端口,备选最大端口 http.server.min_workers int N 1 配置 Http 服务器的最小 work 线程数 http.server.max_workers int N 2 配置 Http 服务器的最大 work 线程数 http.server.accept_count int N 1024 配置 Http 服务器的最大排队请求数 metrics.exporter String Y log.stdout 配置用于导出监控指标的 Exporter 的类型log.stdout:以标准格式化结构输出到 stdout.log，log.standard: 以标准格式化结构输出到磁盘，log.influxdb: 以 InfluxDB LineProtocol 格式输出到磁盘，http.influxdb:以 InfluxDB LineProtocol 格式发送至 InfluxDB server metrics.log.method String N data/logs/MyPerf4J/metrics.log 配置方法性能监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.class_loading String N NULL 配置类加载监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.gc String N NULL 配置GC监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.memory String N NULL 配置内存监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.buff_pool String N NULL metrics.log.thread String N NULL 配置线程监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.file_desc String N NULL 配置文件描述符监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.compilation String N NULL 配置编译时间监控指标的日志路径，NULL 表示丢弃收集到的监控指标 metrics.log.rolling.time_unit String N DAILY 配置日志文件滚动时间间隔，分别有 MINUTELY、HOURLY 和 DAILY 三个值 metrics.log.reserve.count int N 7 配置历史日志文件保留个数 metrics.time_slice.method int N 60000 配置方法指标采集的时间片，单位为 ms，最小 1s，最大 600s metrics.time_slice.jvm int N 60000 配置方法指标采集的时间片，单位为 ms，最小 1s，最大 600s metrics.method.show_params boolean N false 是否展示方法参数类型 metrics.method.class_level_mapping String N 配置 Java类的层级映射关系 recorder.mode String N rough 配置 RecordMode，包含 accurate 和 rough 两个模式 recorder.size.timing_arr int N 1000 配置通用的方法响应时间阈值，单位为 ms recorder.size.timing_map int N 16 配置通用的方法响应时间超出指定阈值的次数，仅在 RecorderMode=accurate 时有效 recorders.backup_count Int N 1 配置备用 Recorders 的数量，最小 1，最大 8；当你的应用程序拥有非常多的方法需要监控并且你配置的MilliTimeSlice 比较小时，可以适当的提高 BackupRecordersCount 的数值。 filter.packages.include String Yes 配置需要进行监控的包的前缀，支持多个包路径，每个包路径用英文 ‘;’ 分隔；可以使用 [] 表示包/类的集合，形如：com.demo.[p1,p2,p3]；可以使用 * 表示通配符，形如：com..demo. filter.packages.exclude String N “” 配置不需要进行监控的包的前缀，支持多个包路径，每个包路径用英文 ‘;’ 分隔；可以使用 [] 表示包/类的集合，形如：com.demo.[p1,p2,p3]；可以使用 * 表示通配符，形如：com..demo. filter.methods.exclude String N “” 配置不需要进行监控的方法名，每个方法名用英文 ‘;’ 分隔 filter.methods.exclude_private boolean N true 配置是否要排除私有方法，可配置为true/false filter.class_loaders.exclude String N “” 配置不需要进行监控的 ClassLoader，支持多个 ClassLoader，每个 ClassLoader 路径用英文’;'分隔 influxdb.version String N 1.0 配置 InfluxDB 的 版本号 influxdb.orgName String N “” 配置 InfluxDB 所属的组织名称，当 InfluxDB 为 v2.x 时为必填项 influxdb.host String N 127.0.0.1 配置 InfluxDB 的 IP 地址 influxdb.port int N 8086 配置 InfluxDB 的端口号 influxdb.database String N “” 配置 InfluxDB 的数据库名 influxdb.username String N “” 配置 InfluxDB 的用户名 influxdb.password String N “” 配置 InfluxDB 的密码 influxdb.conn_timeout int N 3000 配置 InfluxDB 的连接超时时间，单位为 ms influxdb.read_timeout int N 5000 配置 InfluxDB 的读超时时间，单位为 ms 2.1 关于 Rough模式 与 Accurate模式 Rough 模式 精度略差，会把响应时间超过指定阈值的记录为’阈值+1’ 更加节省内存，只使用数组来记录响应时间 速度略快一些，但计算 Metrics 的速度略慢一些 在 MyPerf4JPropFile 配置文件中指定 recorder.mode = rough Accurate 模式 精度高，会记录所有的响应时间 相对耗费内存，使用数组 + Map 来记录响应时间 速度略慢一些，但计算 Metrics 的速度略快一些 默认 建议 对于有以下特征的应用，推荐使用 Rough 模式 内存敏感 精度要求不是特别高 对于有以下特征的应用，推荐使用 Accurate 模式 内存不敏感 精度要求特别高 方法响应时间范围比较广 MyPerf4J 的版本号大于等于 2.8.0 2.2 关于包路径规则 filter.packages.include 和 filter.packages.exclude 目前支持以下三种规则： com.demo.p1 代表包含以 com.demo.p1 为前缀的所有包和类 [] 表示集合的概念：例如，com.demo.[p1,p2,p3] 代表包含以 com.demo.p1、com.demo.p2 和 com.demo.p3 为前缀的所有包和类，等价于 com.demo.p1;com.demo.p2;com.demo.p3 * 表示通配符：可以指代零个或多个字符，例如: com.*.demo.* 2.3 关于包路径规则 filter.methods.exclude 目前支持以下三种规则： filter.methods.exclude = getId1 代表排除所有方法名为 getId1 的方法 filter.methods.exclude = DemoServiceImpl.getId1 代表排除类 DemoServiceImpl 中所有方法名为 getId1 的方法 filter.methods.exclude = DemoServiceImpl.getId1(long) 代表排除类 DemoServiceImpl 中方法签名为 getId1(long) 的方法 2.4 关于 metrics.method.class_level_mapping 的使用规则 metrics.method.class_level_mapping 用于配置 Class 层级映射关系，格式为：LevelA:[classNameExpA1,classNameExpA2];LevelB:[classNameExpB1,classNameExpB2]，以 metrics.method.class_level_mapping = Api:[*Api,*ApiImpl];Controller:[*Controller];为例： Api:[*Api,*ApiImpl] 代表所有以 Api 和 ApiImpl 结尾的类的层级为 Api Controller:[*Controller] 代表所有以 Controller 结尾的类的层级为 Controller","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Nginx版本升级流程","slug":"nginx-upgrade","date":"2023-01-18T12:07:35.000Z","updated":"2023-01-18T12:07:35.000Z","comments":false,"path":"nginx-upgrade/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-upgrade/index.html","excerpt":"","text":"旨在说明版本升级流程步骤，测试环境为nginx1.11.1升级至1.22.0稳定版本，目的是解决Nginx&lt;=1.21.5下的0day LDAP RCE 漏洞情报。 备份 安装路径目录不大的话可以直接备份整个目录，或者剔除日志文件不备份。 1[root@localhost nginxHome]# tar -zcvf nginx1.11.1.tar-gz nginx1.11.1/ 查看原有的 nginx 编译参数 到nginx安装目录下，通过nginx -V命令检查原有的 nginx 编译参数，即configure arguments的配置内容 1234[root@localhost sbin]# ./nginx -V nginx version: nginx/1.11.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) configure arguments: --prefix=/home/oneapm/nginxHome/nginx1.11.1 对新版本的源码包进行编译 解压新版本的源码包，通过上述检查原有的编译参数（–prefix=/home/oneapm/nginxHome/nginx1.11.1）对新版本源码包进行编译 12[root@localhost nginx-1.22.0]# ./configure --prefix=/home/oneapm/nginxHome/nginx1.11.1[root@localhost nginx-1.22.0]# make 特别说明： make完之后就不要再make install，没有必要，如果make install，但–prefix又没改路径，那就gg了，覆盖了原来的安装环境，所以备份也很重要 停止nginx进程 不停止正在运行的nginx进程在下一步做二进制文件覆盖的时候会报错***/sbin/nginx’: Text file busy 1./nginx -s stop Tips: 提前检查好自己当前使用的那个配置文件，别停了启动的时候就不晓得配置文件是哪个了 复制新的nginx源码包中二进制文件，覆盖原来的文件 1[root@localhost nginx-1.22.0]# cp -p /opt/nginx-1.22.0/objs/nginx /home/oneapm/nginxHome/nginx1.11.1/sbin/ 启动nginx服务 -c选项指定配置文件启动nginx服务 1./nginx -c /home/oneapm/nginxHome/nginx1.11.1/conf/nginx.conf 验证nginx升级是否成功 检查版本并测试配置是否正常 1234567[root@localhost sbin]# ./nginx -Vnginx version: nginx/1.22.0built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC)configure arguments: --prefix=/home/oneapm/nginxHome/nginx1.22.0[root@localhost sbin]# ./nginx -tnginx: the configuration file /home/oneapm/nginxHome/nginx1.11.1/conf/nginx.conf syntax is oknginx: configuration file /home/oneapm/nginxHome/nginx1.11.1/conf/nginx.conf test is successful","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"Spring注解装配方式","slug":"spring-injection","date":"2023-01-15T06:03:13.000Z","updated":"2023-01-15T06:03:13.000Z","comments":false,"path":"spring-injection/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/spring-injection/index.html","excerpt":"","text":"目录 1. 实例注入方式 2. @Autowired, @Resource, @Inject 三个注解的区别 3. @Autowired Field injection is not recommended 4. 总结 1. 实例注入方式 spring注入的方式有三种： 属性field注入 setter方法注入 构造方法constructor注入 下面就分别看下这三种方式注入的区别 1.1 属性field注入 属性注入是大家最为常见也是使用最多的一种注入方式，就是在bean的变量上使用注解进行依赖注入，本质上是通过反射的方式直接注入到field。 1234567@Servicepublic class BService&#123; @Autowired AService aService; //......&#125; 这里使用@Autowired注解注入，另外也有Resource以及@Inject等注解，都可以实现注入。 但是当我们使用@Autowired注解注入时，却出现Field injection is not recommended的警告，这是为什么？且看下文。 1.2 setter方法注入 set 方法注入太过于臃肿，实际上很少使用 123456789@Servicepublic class BService&#123; AService aService; @Autowired public void setaService(AService aService)&#123; this.aService = aService; &#125;&#125; 注：在 Spring 4.3 及以后的版本中，setter 上面的 @Autowired 注解是可以不写的。 1.3 构造方法注入 构造方法注入方式如下： 123456789@Servicepublic class BService&#123; AService aService; @Autowired public BService(AService aService)&#123; this.aService = aService; &#125;&#125; 如果类只有一个构造方法，那么@Autowired注解可以省略；如果有多个构造方法，那么需要添加上@Autowired注解来明确指定使用哪个构造方法 1.4 属性field注入优缺点 好处：方式简洁，代码简单； 坏处： 容易违背了单一职责原则，使用这种基于field注入的方式，添加依赖是很简单的，就算类中有十几个依赖你可能都觉得没有什么问题，但是拥有太多的依赖通常意味着你的类要承担更多的责任，明显违背了单一职责原则(SRP：Single responsibility principle) 依赖注入与容器本身耦合：依赖注入框架的核心思想之一就是受容器管理的类不应该去依赖容器所使用的依赖。换句话说，这个类应该是一个简单的POJO能够被单独实例化并且能为它提供所需的依赖；这个问题具体可以表现在 类和依赖容器强耦合，不能在容器外使用 类不能绕过反射（例如单元测试的时候）进行实例化，必须通过依赖容器才能实例化 不能使用属性注入的方式构建不可变对象（final修饰的变量） 1.5 Spring 开发团队的建议 Since you can mix constructor-based and setter-based DI, it is a good rule of thumb to use constructors for mandatory dependencies and setter methods or configuration methods for optional dependencies. 简单的说，就是 强制依赖就用构造器凡事 可选、可变的依赖就用setter注入 当然你可以在同一个类中使用这两种方法。构造器注入更适合强制性的注入旨在不变性，Setter注入更适合可变性的注入 基于构造方法注入 The Spring team generally advocates constructor injection as it enables one to implement application components as immutable objects and to ensure that required dependencies are not null. Furthermore constructor-injected components are always returned to client (calling) code in a fully initialized state. As a side note, a large number of constructor arguments is a bad code smell, implying that the class likely has too many responsibilities and should be refactored to better address proper separation of concerns. Spring 团队提倡使用基于构造方法的注入，因为这样一方面可以将依赖注入到一个不可变的变量中 (注：final 修饰的变量) ，另一方面也可以保证这些变量的值不会是 null 。此外，经过构造方法完成依赖注入的组件 (注：比如各个 service)，在被调用时可以保证它们都完全准备好了 。与此同时，从代码质量的角度来看，一个巨大的构造方法通常代表着出现了代码异味，这个类可能承担了过多的责任 。 基于setter注入 Setter injection should primarily only be used for optional dependencies that can be assigned reasonable default values within the class. Otherwise, not-null checks must be performed everywhere the code uses the dependency. One benefit of setter injection is that setter methods make objects of that class amenable to reconfiguration or re-injection later. 基于 setter 的注入，则只应该被用于注入非必需的依赖，同时在类中应该对这个依赖提供一个合理的默认值。如果使用 setter 注入必需的依赖，那么将会有过多的 null 检查充斥在代码中。使用 setter 注入的一个优点是，这个依赖可以很方便的被改变或者重新注入 。 2. @Autowired, @Resource, @Inject 三个注解的区别 Spring 支持使用@Autowired、@Resource、@Inject三个注解进行依赖注入，下面就来介绍一下这三个注解的区别。 2.1 @Autowired @Autowired为Spring框架提供的注解，需要导入包org.springframework.beans.factory.annotation.Autowired。 1234567@Target(&#123;ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired &#123;trueboolean required() default true;&#125; 从Autowired注解源码上看，可以作用在： 12345@Target(ElementType.CONSTRUCTOR) //构造函数@Target(ElementType.METHOD) //方法@Target(ElementType.PARAMETER) //方法参数@Target(ElementType.FIELD) //字段、枚举的常量@Target(ElementType.ANNOTATION_TYPE) //注解 @Autowired是Spring自带的注解，通过AutowiredAnnotationBeanPostProcessor类实现的依赖注入 下面就通过一段代码来了解它。 123456789101112public interface Svc &#123; void sayHello();&#125;@Servicepublic class SvcA implements Svc &#123; @Override public void sayHello() &#123; System.out.println(&quot;hello, this is service A&quot;); &#125;&#125; 测试类： 1234567891011121314@SpringBootTestpublic class SimpleTest &#123; @Autowired // @Qualifier(&quot;svcA&quot;) Svc svc; @Test void rc() &#123; Assertions.assertNotNull(svc); svc.sayHello(); &#125;&#125; @Qualifier(“XXX”) 中的 XX是 Bean 的名称，所以 @Autowired 和 @Qualifier 结合使用时，自动注入的策略就从 byType 转变成 byName 了 注意：使用@Qualifier 时候，如何设置的指定名称的Bean不存在，则会抛出异常，如果防止抛出异常，可以使用： 123@Qualifier(&quot;xxxxyyyy&quot;)@Autowired(required = false)private Svc svc; 装配顺序： 按照type在上下文查看匹配的bean 1查找type为Svc的bean 如果有多个bean, 则按照name进行匹配 判断是否有@Qualifier注解 a. 如果有@Qualifier注解，则按照@Qualifier指定的name进行匹配 1查找name为svcA的bean b. 如果没有，则按照变量名进行匹配 1查找name为svc的bean 匹配不到，则报错。（@Autowired(required=false)，如果设置required为false(默认为true)，则注入失败时不会抛出异常） 2.2 @Inject @Inject是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包，才能实现注入 @Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named 12345@Target(&#123;ElementType.METHOD, ElementType.CONSTRUCTOR, ElementType.FIELD&#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Inject &#123;&#125; 从@Inject注解源码上看，可以使用在以下地方： 123@Target(ElementType.CONSTRUCTOR) //构造函数@Target(ElementType.METHOD) //方法@Target(ElementType.FIELD) //字段、枚举的常量 在Spring 的环境下，@Inject和@Autowired 是相同的 ，因为它们的依赖注入都是使用AutowiredAnnotationBeanPostProcessor来处理的。 123456789101112131415161718public class AutowiredAnnotationBeanPostProcessor extends InstantiationAwareBeanPostProcessorAdapter implements MergedBeanDefinitionPostProcessor, PriorityOrdered, BeanFactoryAware &#123; //........省略........truepublic AutowiredAnnotationBeanPostProcessor() &#123;truetruethis.autowiredAnnotationTypes.add(Autowired.class);truetruethis.autowiredAnnotationTypes.add(Value.class);truetruetry &#123;truetruetruethis.autowiredAnnotationTypes.add((Class&lt;? extends Annotation&gt;)truetruetruetruetrueClassUtils.forName(&quot;javax.inject.Inject&quot;, AutowiredAnnotationBeanPostProcessor.class.getClassLoader()));truetruetruelogger.info(&quot;JSR-330 &#x27;javax.inject.Inject&#x27; annotation found and supported for autowiring&quot;);truetrue&#125;truetruecatch (ClassNotFoundException ex) &#123;truetruetrue// JSR-330 API not available - simply skip.truetrue&#125;true&#125; //........省略........ &#125; 简单使用代码： 123@Inject@Named(&quot;svcA&quot;)private SvcA asvc; @Named的作用类似 @Qualifier！ 区别：@Inject是Java EE包里的，在SE环境需要单独引入。另一个区别在于@Autowired可以设置required=false而@Inject并没有这个属性 2.3 @Resource 123456789101112131415@Target(&#123;TYPE, FIELD, METHOD&#125;)@Retention(RUNTIME)public @interface Resource &#123; String name() default &quot;&quot;; String lookup() default &quot;&quot;; Class&lt;?&gt; type() default java.lang.Object.class; enum AuthenticationType &#123; CONTAINER, APPLICATION &#125; AuthenticationType authenticationType() default AuthenticationType.CONTAINER; boolean shareable() default true; String mappedName() default &quot;&quot;; String description() default &quot;&quot;;&#125; 从@Resource注解源码上看，可以使用在以下地方： 123@Target(ElementType.TYPE) //Class, interface (including annotation type), or enum declaration@Target(ElementType.METHOD) //方法@Target(ElementType.FIELD) //字段、枚举的常量 @Resource是JSR-250定义的注解。Spring 在 CommonAnnotationBeanPostProcessor实现了对JSR-250的注解的处理，其中就包括@Resource。 1234567891011121314151617181920public class CommonAnnotationBeanPostProcessor extends InitDestroyAnnotationBeanPostProcessor implements InstantiationAwareBeanPostProcessor, BeanFactoryAware, Serializable &#123; //........省略........ @Override public PropertyValues postProcessPropertyValues( PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) &#123; InjectionMetadata metadata = findResourceMetadata(beanName, bean.getClass(), pvs); try &#123; metadata.inject(bean, beanName, pvs); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, &quot;Injection of resource dependencies failed&quot;, ex); &#125; return pvs; &#125; //........省略........ &#125; @Resource有两个重要的属性：name和type，而Spring 将@Resource注解的name属性解析为bean的名字，而type属性则解析为bean的类型。 装配顺序： 如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常。 如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常。 如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或是找到多个，都会抛出异常。 如果既没有指定name，又没有指定type，则默认按照byName方式进行装配；如果没有匹配，按照byType进行装配。 3. @Autowired Field injection is not recommended 在使用IDEA进行Spring开发时，使用@Autowired注解的时候，会发现IDEA会有警告显示： 123Field injection is not recommendedInspection info: Spring Team Recommends: &quot;Always use constructor based dependency injection in your beans. Always use assertions for mandatory dependencies&quot;. 大致的意思就是： 不建议使用基于 field 的注入方式。 Spring 开发团队建议：在你的Spring Bean 永远使用基于constructor 的方式进行依赖注入。对于必须的依赖，永远使用断言来确认。 比如以下代码： 12345678910111213141516171819202122@Servicepublic class HelpService &#123; @Autowired @Qualifier(&quot;svcB&quot;) private Svc svc; public void sayHello() &#123; svc.sayHello(); &#125;&#125;public interface Svc &#123; void sayHello();&#125;@Servicepublic class SvcB implements Svc &#123; @Override public void sayHello() &#123; System.out.println(&quot;hello, this is service B&quot;); &#125;&#125; 修改为Constructor的注入方式： 12345678910111213@Servicepublic class HelpService &#123; private final Svc svc; @Autowired public HelpService(@Qualifier(&quot;svcB&quot;) Svc svc) &#123; Assert.notNull(svc, &quot;svc must not be null&quot;); this.svc = svc; &#125; public void sayHello() &#123; svc.sayHello(); &#125;&#125; 如果按照Spring团段的建议，如果svc是必须的依赖，应该使用Assert.notNull(svc, &quot;svc must not be null&quot;);来确认 4. 总结 4.1 @Autowired总结 @Autowired注解总结 可用于构造函数，成员变量以及set方法 从Spring 4.3开始，如果目标Bean只有一个构造函数，则在该构造函数上可以省略@Autowired注解；如果目标Bean有多个构造函数则不可省略 @Autowired注入方式： 按照type查找bean，如果使用@Qualifier注解声明了name，则从结果集中取出与该name相匹配的bean返回（此时可以视为通过name和type获取bean，但实质是先通过type获取所有bean，然后通过name筛选，详情见后文findAutowireCandidates()方法源码分析） 如果没有使用@Qualifier注解，且找到多个bean，则判断这些bean中是否有使用@Primary注解和@Priority注解，有就返回优先级最高的哪一个bean，没有就按照字段名称去匹配bean，匹配成功返回，不成功抛出异常。（详情见后文determineAutowireCandidate()方法源码解析） 4.2 @Resource总结 可用于成员变量以及set方法 若不指定name属性，则会把name属性值处理为字段名或set方法标识的字段名称 若指定type属性，则type属性值必须与字段类型或set方法返回值类型为父子关系（type属性值可以是子类，也可以是超类），否则会抛出异常 @Resource先按照name属性值注入，若未找到，则按type属性值注入。即默认的name或指定的name找不到 bean ，就会按 type 注入 4.3 @Inject总结 @Inject是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包 ，才能实现注入 @Inject可以作用CONSTRUCTOR、METHOD、FIELD上 @Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； 4.4 总结 1、@Autowired是Spring自带的，@Resource是JSR250规范实现的，@Inject是JSR330规范实现的 2、@Autowired、@Inject用法基本一样，不同的是@Inject没有一个request属性 3、@Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的，@Autowired默认是byType可以使用@Qualifier指定Name，@Resource默认ByName如果找不到则ByType 4、@Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Name一起使用，@Resource则通过name进行指定 5、@Autowired可以对构造器、方法、参数、字段使用，@Resource只能对方法、字段使用， @Inject可以对构造函数、方法、字段、枚举的常量使用","categories":[{"name":"02 Spring","slug":"02-Spring","permalink":"https://xiaoyuge5201.github.io/categories/02-Spring/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"服务定位模式 Service Locator Pattern","slug":"service-locator-pattern","date":"2023-01-14T12:28:56.000Z","updated":"2023-01-14T12:28:56.000Z","comments":false,"path":"service-locator-pattern/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/service-locator-pattern/index.html","excerpt":"","text":"1. 前言 不知道大家在项目中有没有遇到这样的场景，根据传入的类型，调用接口不同的实现类或者服务，比如根据文件的类型使用CSV解析器或者JSON解析器，在调用的客户端 一般都是用if else去做判断，比如类型为JSON，就用JSON解析器，那么如果新加一个类型的解析器，是不是调用的客户端还要修改？这显然太耦合了。 本文就介绍一种方法，服务定位模式Service Locator Pattern来解决，帮助我们消除紧耦合实现及其依赖性，并提出将服务与其具体类解耦。 2. 文件解析器 接下来通过一个例子来介绍如何使用Service Locator Pattern。 假设有一个从各种来源获取数据的应用程序，我们必须解析不同类型的文件，比如解析CSV文件和JSON 文件。 定义一个类型的枚举 1234public enum ContentType &#123; JSON, CSV&#125; 定义一个解析的接口 1234public interface Parser &#123; List parse(Reader reader);&#125; 根据不同的文件类型编写不同的实现类 1234567891011121314151617@Componentpublic class CSVParser implements Parser&#123; @Override public List parse(Reader reader) &#123; return null; &#125;&#125;@Componentpublic class JSONParser implements Parser&#123; @Override public List parse(Reader reader) &#123; return null; &#125;&#125; 编写一个客户端，通过switch case根据不同类型调用不同的实现类 123456789101112131415161718192021222324@Servicepublic class Client &#123; private Parser csvParser, jsonParser; @Autowired public Client(Parser csvParser, Parser jsonParser) &#123; this.csvParser = csvParser; this.jsonParser = jsonParser; &#125; public List getAll(ContentType contentType, Reader reader) &#123; //...... switch (contentType) &#123; case CSV: return csvParser.parse(reader); case JSON: return jsonParser.parse(reader); default: break; &#125; // ........... return null; &#125;&#125; 可能大部份人首先想到的都是像上面一样的方式去实现，那么这样存在怎样的问题呢？ 现在加入提出一个新需求支持XML文件类型，是不是客户端也要修改代码，然后在switch case中添加新的类型，这就导致客户端和不同的解析器紧密耦合。 那么有什么更好的方式呢？ 3. 应用Service Locator Pattern 接下来使用服务定位模式Service Locator Pattern来改造上面的方法 定义服务定位器接口ParserFactory，根据参数类型返回Parser 123public interface ParserFactory &#123; Parser getParser(ContentType contentType);&#125; 配置ServiceLocatorFactoryBean使用ParserFactory作为服务定位器接口，ParserFactory这个接口不需要写实现类 1234567891011@Configurationpublic class ParserConfig &#123; @Bean(&quot;parserFactory&quot;) public FactoryBean serviceLocatorFactoryBean() &#123; ServiceLocatorFactoryBean factory = new ServiceLocatorFactoryBean(); //设置服务定位接口 factory.setServiceLocatorInterface(ParserFactory.class); return factory; &#125;&#125; 设置解析器Bean的名称为类型名称，方便服务定位 123456789101112131415161718192021@Component(&quot;CSV&quot;)public class CSVParser implements Parser &#123; @Override public List parse(Reader reader) &#123; return null; &#125;&#125;@Component(&quot;JSON&quot;)public class JSONParser implements Parser &#123; @Override public List parse(Reader reader) &#123; return null; &#125;&#125;@Component(&quot;XML&quot;)public class XmlParser implements Parser&#123; @Override public List parse(Reader reader) &#123; return null; &#125;&#125; 修改枚举，添加XML类型 12345public enum ContentType &#123; JSON, CSV, XML&#125; 最后修改客户端调用，直接根据类型调用对应的解析器，去掉了switch case 12345678910111213141516@Servicepublic class Client &#123; private ParserFactory parserFactory; @Autowired public Client(ParserFactory parserFactory) &#123; this.parserFactory = parserFactory; &#125; public List getAll(ContentType contentType, Reader reader) &#123; //.............. //关键点，直接根据类型获取 return parserFactory.getParser(contentType).parse(reader); &#125;&#125; 这样就实现了，如果再添加新的类型，只需要扩展添加新的解析器就行，再也不用修改客户端了，满足开闭原则。 如果觉得Bean的名称直接使用类型怪怪的，可以建议按照下面的方式来 123456789101112131415161718192021222324252627282930public enum ContentType &#123; JSON(TypeConstants.JSON_PARSER), CSV(TypeConstants.CSV_PARSER), XML(TypeConstants.XML_PARSER); private final String parserName; ContentType(String parserName) &#123; this.parserName = parserName; &#125; @Override public String toString() &#123; return this.parserName; &#125; public interface TypeConstants &#123; String CSV_PARSER = &quot;csvParser&quot;; String JSON_PARSER = &quot;jsonParser&quot;; String XML_PARSER = &quot;xmlParser&quot;; &#125;&#125;@Component(TypeConstants.CSV_PARSER)public class CSVParser implements Parser &#123; &#125;@Component(TypeConstants.JSON_PARSER)public class JSONParser implements Parser &#123; &#125;@Component(TypeConstants.XML_PARSER)public class XMLParser implements Parser &#123; &#125; 4. 剖析Service Locator Pattern 服务定位器模式消除了客户端对具体实现的依赖。以下引自 Martin Fowler 的文章总结了核心思想：“服务定位器背后的基本思想是拥有一个知道如何获取应用程序可能需要的所有服务的对象。因此，此应用程序的服务定位器将有一个在需要时返回“服务”的方法。” Spring 的ServiceLocatorFactoryBean实现了 FactoryBean接口，创建了Service Factory服务工厂Bean。 5. 总结 我们通过使用服务定位器模式实现了一种扩展 Spring 控制反转的绝妙方法。它帮助我们解决了依赖注入未提供最佳解决方案的用例。也就是说，依赖注入仍然是首选，并且在大多数情况下不应使用服务定位器来替代依赖注入。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"设计模式","slug":"设计模式","permalink":"https://xiaoyuge5201.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"}]},{"title":"断网了，还能ping通127.0.0.1吗","slug":"interview-1","date":"2023-01-10T04:12:01.000Z","updated":"2023-01-10T04:12:01.000Z","comments":false,"path":"interview-1/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/interview-1/index.html","excerpt":"","text":"1. 前言 网通不通， 你ping一下就知道了，如果把127.0.0.1换成0.0.0.0或则localhost会怎样呢？这几个IP有什么区别？ 话不多说，拔掉网线，断开WI-FI，然后在控制台ping 127.0.0.1 说明，断网了还是能够ping 通127.0.0.1的。 2. 什么是127.0.0.1 首先，这个是IPV4地址，IPV4地址有 32 位， 一个字节8位，共4个字节 其中127开头的都属于回环地址，也是IPV4的特殊地址，人为规定的。而127.0.0.1是众多回环地址中的一个，之所以不是127.0.0.2，是因为源码就是这么定义的，没什么道理。 IPV4的地址是32位，2的32次方，大概是40+亿。这点数量不够用，目前IP已经用完，所以就有了IPV6，IPV6地址是128位的也就是2的128次方≈10的38次方 IPV4以8位一组，每组之间用 . 号隔开。 IPV6就以16位为一组，每组之间用 : 号隔开。如果全是0，那么可以省略不写。 IPV4的回环地址是127.0.0.1, 在IPV6下，表达式为：::1,中间把连续的0给省略了，之所以不是7个冒号，而是2个冒号，是因为IPV6地址只允许出现一次两个连续的冒号。 在IPV4下用 ping 127.0.0.1 命令, 在IPV6下用Ping :: 1命令 3. 什么是ping ping 是应用层命令，ping作为一个小软件，它的功能是尝试发送一个小小的消息到目标机器上，判断目标机器是否可达，也就是判断目标机器网络是否可以联通。 ping应用的底层用户的是网络层ICMP协议 虽然ICMP协议和IP协议都属于网络层协议，但其实ICMP也是利用了IP协议进行消息的传输 简单的理解为 ping 某个IP 就是往某个IP地址发个消息 4. TCP发数据和ping的区别 一般情况下，我们会使用TCP进行网络数据传输，那么可以看下它和ping的区别。 ping 和其他应用软件都属于应用层。 那么我们横向对比一下，比如说聊天软件，如果用的是TCP的方式去发送消息，那么就得先知道往哪发，Linux里万物皆文件，那么你要发消息的目的地，也就是个文件，这里就需要使用socket。 在TCP传输中创建socket的方式是socket(AF_INET, SOCK_STREAM, 0) AF_INET: 表示将使用IPV4里host:port的方式去解析你输入的网络地址 SOCK_STREAM: 指使用面向字节流的TCP协议，工作在传输层。 创建好socket之后，就可以把要传输的数据写到这个文件里，调用socket的sendto接口的过程中进程会从用户态进入到内核态，最后会调用到socket_sendmsg方法。 进入传输层，带上TCP头，网络层带上IP头，数据链路层带上MAC头等一系列操作后，进入网卡的发送队列ringbuffer， 顺着网卡就发出去了。 再回到ping，整个过程基本跟TCP发数据类似，差异的地方主要在于： 创建socket的时候用的是socket(AF_INET,SOCK_RAW,IPPROTO_ICMP)，SOCK_RAW是原始套接字，工作在网络层，所以构建ICMP网络层协议的数据，是再合适不过了。 ping 在进入内核态最后也是调用的sock_sengmsg方法，计入到网络层后加上ICMP和IP头，数据链路层加上MAC头，也是顺着网卡发出 因此，本质上ping 和普通应用发消息在程序流上没有太大的差别。 这也解释了为什么当你发现怀疑网络有问题的时候，别人第一时间是问你能ping通吗？因为可以简单理解为ping就是自己组了个数据包，让系统按着其他软件发送数据的路径往外发一遍，能通的话说明其他软件发的数据也能通 5. 卫生棉断网了还能ping通127.0.0.1 前面提到，有网的情况下，ping最后是通过网卡将数据发出去的，那么在断网的情况下，网卡已经不工作了，ping回环地址却一切正常，可以看下这种情况下的工作原理。 从应用层到出书层再到网络层，这段路径跟ping外网的时候几乎是一样的，到了网络层，系统会根据目的IP，在路由表中获取对应的路由信息，这其中就包括选择哪个网卡把消息发送出去。 当发现目标IP是外网IP时，会从&quot;真网卡&quot;发出 当发现目标IP是回环地址时，会选择本地网卡 本地网卡会把数据推到一个叫input_pkt_queue的链表中，这个链表是所有网卡共享的，上面挂着发给本机的各种消息，消息被发送到这个链表后，会在触发一个软中断。 专门处理软中断的工具人ksoftirqd(内核线程)，它在收到软中断后就会立即去链表将消息取出，然后顺着数据链路层、网络层等层层往上传递最后给到应用程序。 ping 回环地址和通过TCP等各种协议发送数据到回环地址都是走这条路径。整条路径从发到收，都没有经过&quot;真网卡&quot;。之所以127.0.0.1叫本地回环地址，可以理解为，消息发出到这个地址上的话，就不会出网络，在本机打个转就又回来了。所以断网，依然能 ping 通 127.0.0.1。 6. ping回环地址和ping本机地址有什么区别 查看本机地址和回环地址 lo0 : 表示本地回环接口对应的地址，也就是回环地址 en0 : 表示本地第一块网卡，对应的地址是192.168.1.100，也叫做本地IP 之前一直人为ping 本地IP的话会通过&quot;真网卡&quot;出去，然后遇到第一个路由器，再发回到本机。 为了验证这个说法，可以进行抓包。 ping 127.0.0.1 ping 本机IP 可以看到 ping 本机IP 跟 ping 回环地址一样，相关的网络数据，都是走的lo0，本地回环接口。 只要走了本地回环接口，那数据都不会发送到网络中，在本机网络协议栈中兜一圈，就发回来了。因此ping回环地址和ping本机地址没有区别。 7. 127.0.0.1和localhost以及0.0.0.0区别 访问nginx的时候，发现用这几个 IP，都能正常访问到nginx的欢迎网页。一度认为这几个IP都是一样的。 但本质上还是有些区别的。 首先localhost就不叫IP，它是一个域名，就跟 “baidu.com”,是一个形式的东西，只不过默认会把它解析为127.0.0.1 ，当然这可以在/etc/hosts文件下进行修改。 所以默认情况下，使用localhost跟使用127.0.0.1确实是没区别的。 其次就是 0.0.0.0，执行 ping 0.0.0.0 ，是会失败的，因为它在IPV4中表示的是无效的目标地址。 但它还是很有用处的，回想下，我们启动服务器的时候，一般会 listen 一个 IP 和端口，等待客户端的连接。 如果此时listen的是本机的0.0.0.0, 那么它表示本机上的所有IPV4地址。 举个例子。刚刚提到的 127.0.0.1 和 192.168.1.100 ，都是本机的IPV4地址，如果监听0.0.0.0 ，那么用上面两个地址，都能访问到这个服务器。 当然， 客户端connect时，不能使用0.0.0.0 。必须指明要连接哪个服务器IP。 8. 总结 127.0.0.1是回环地址。localhost是域名，但默认等于127.0.0.1。 ping 回环地址和 ping 本机地址，是一样的，走的是lo0 “假网卡”，都会经过网络层和数据链路层等逻辑，最后在快要出网卡前狠狠拐了个弯， 将数据插入到一个链表后就软中断通知 ksoftirqd 来进行收数据的逻辑，压根就不出网络。所以断网了也能ping通回环地址。 如果服务器listen的是 0.0.0.0，那么此时用127.0.0.1和本机地址都可以访问到服务。","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"interview","slug":"interview","permalink":"https://xiaoyuge5201.github.io/tags/interview/"}]},{"title":"说说wait()和sleep的区别","slug":"interview-2","date":"2023-01-07T02:26:38.000Z","updated":"2023-01-07T02:26:38.000Z","comments":false,"path":"interview-2/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/interview-2/index.html","excerpt":"","text":"1. 得分点 所属的类型不同、对锁的依赖不同、返回的条件不同、使用方式不同 2. 标准回答 wait()和sleep()方法主要有以下3个区别： 所属类型不同： wait()是Object类的实例方法，调用该方法的线程将进入WAITING状态 sleep()是Thread类的静态方法，调用该方法的线程将进入TIMED_WAITING 对锁的依赖不同： wait()依赖于synchronized锁，它必须通过监视器进行调用，在调用后会释放锁 sleep()不依赖任何锁，所以在调用后不会释放锁 返回的条件不同： 调用wait()进入等待状态的线程，需要由notify()/ notifyAll()唤醒，进入就绪状态，从而返回 调用sleep()进入睡眠的线程，在超时时间到达后自动返回或者使用interrupt()方法中断，进入就绪状态（sleep()）方法只是让出CPU，并不会让出同步资源锁 使用方式不同： wait()只能在同步方法或者同步代码块中调用，否则会报illegalMonitorStateException异常，如果没有设置超时时间，使用notify()来唤醒 sleep()能在任何地方调用 3. 加分项 wait()方法也支持超时参数，线程调用带有超时参数的wait()会进入TIMED_WAITING状态，在此状态下的线程可以通过notify()/ notifyAll()唤醒从而返回，若在超时时间结束后仍未被唤醒则自动返回. 如果采用Lock进行线程同步,则不存在同步监视器,此时需要使用Condition的方法实现等待。 Condition对象是通过Lock对象创建出来的,它的await()方法会导致线程进入WTING状态,它的带超时参数的await()方法会导致线程进入TIMED_WAITING状态,当调用它的signal()/signalAll()方法时,线程会被唤醒从而返回。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"SpringBoot+Disruptor","slug":"springboot-disruptor","date":"2023-01-04T04:01:43.000Z","updated":"2023-01-04T04:01:43.000Z","comments":false,"path":"springboot-disruptor/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/springboot-disruptor/index.html","excerpt":"","text":"1. Disruptor介绍 Disruptor 是英国外汇交易公司LMAX开发的一个高性能队列，研发的初衷是解决内存队列的延迟问题（在性能测试中发现竟然与I/O操作处于同样的数量级）。基于 Disruptor 开发的系统单线程能支撑每秒 600 万订单，2010 年在 QCon 演讲后，获得了业界关注 Disruptor是一个开源的Java框架，它被设计用于在生产者—消费者（producer-consumer problem，简称PCP）问题上获得尽量高的吞吐量（TPS）和尽量低的延迟 从功能上来看，Disruptor 是实现了“队列”的功能，而且是一个有界队列。那么它的应用场景自然就是“生产者-消费者”模型的应用场合了 Disruptor是LMAX在线交易平台的关键组成部分，LMAX平台使用该框架对订单处理速度能达到600万TPS，除金融领域之外，其他一般的应用中都可以用到Disruptor，它可以带来显著的性能提升 其实Disruptor与其说是一个框架，不如说是一种设计思路，这个设计思路对于存在“并发、缓冲区、生产者—消费者模型、事务处理”这些元素的程序来说，Disruptor提出了一种大幅提升性能（TPS）的方案 Disruptor的github主页：https://github.com/LMAX-Exchange/disruptor 2. Disruptor核心概念 先从连接Disruptor的核心概念开始，来了解它是如何运作的，下面介绍的概念模型，既是领域对象，也是映射到代码实现的核心对象 2.1 Ring Buffer 环形的缓冲区，曾经RingBuffer是Disruptor中最主要的对象，但从3.0 版本开始，其职责被简化为仅仅负责对通过Disruptor进行交换的数据（事件）进行存储和更新。在一些更高级的应用场景中， RingBuffer可以由用户的自定义实现来完全替代 2.2 Sequence Disruptor 通过顺序递增的序号来管理通过其进行交换的数据（事件），对数据（事件）的处理过程总是沿着序号逐个递增处理。一个sequence用于跟踪标识某个特定的事件处理者（RingBuffer/Consumer）的处理进度，虽然一个 AtomicLong也可以用于标识进度，但定义sequence来负责该问题还有另一个目的，就是防不同的sequence之间的CPU缓存伪共享（False Sharing）问题。 2.3 Sequencer Sequencer是Disruptor的真正核心。此接口有两个实现类SingleProducerSequencer和MultiProducerSequencer，他们定义在生产者和消费者之间快速、正确地传递数据的并发算法。 2.4 Sequence Barrier 用于保持对RingBuffer的 main published Sequence 和Consumer依赖的其它Consumer的 Sequence 的引用。Sequence Barrier 还定义了决定 Consumer 是否还有可处理的事件的逻辑。 2.5 Wait Strategy 定义 Consumer 如何进行等待下一个事件的策略。（注：Disruptor 定义了多种不同的策略，针对不同的场景，提供了不一样的性能表现） 2.6 Event 在 Disruptor 的语义中，生产者和消费者之间进行交换的数据被称为事件(Event)。它不是一个被 Disruptor 定义的特定类型，而是由 Disruptor 的使用者定义并指定。 2.7 EventProcessor EventProcessor 持有特定消费者(Consumer)的 Sequence，并提供用于调用事件处理实现的事件循环(Event Loop)。 2.8 EventHandler Disruptor 定义的事件处理接口，由用户实现，用于处理事件，是 Consumer 的真正实现。 2.9 Producer 即生产者，只是泛指调用 Disruptor 发布事件的用户代码，Disruptor 没有定义特定接口或类型。 3. 案例Demo 添加pom.xml依赖 1234567891011121314151617181920212223242526272829303132&lt;parent&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;version&gt;2.3.5.RELEASE&lt;/version&gt;&lt;/parent&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 主要的 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.lmax&lt;/groupId&gt; &lt;artifactId&gt;disruptor&lt;/artifactId&gt; &lt;version&gt;3.4.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.22&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 消息体Model 1234@Datapublic class MessageModel &#123; private String message;&#125; 构造EventFactory 123456public class HelloEventFactory implements EventFactory&lt;MessageModel&gt; &#123; @Override public MessageModel newInstance() &#123; return new MessageModel(); &#125;&#125; 构造EventHandler - 消费者 123456789101112131415161718@Slf4jpublic class HelloEventHandler implements EventHandler&lt;MessageModel&gt; &#123; @Override public void onEvent(MessageModel messageModel, long l, boolean b) &#123; try &#123; //这里停顿1000ms是为了确定消费消息是异步的 Thread.sleep(1000); log.info(&quot;消费者处理消息开始&quot;); if (messageModel != null) &#123; log.info(&quot;消费者消费的消息是：&#123;&#125;&quot;, messageModel); &#125; &#125; catch (Exception e) &#123; log.info(&quot;消费者处理消息失败&quot;); &#125; log.info(&quot;消费者处理消息结束&quot;); &#125;&#125; 构造BeanManager 1234567891011121314151617181920212223242526/** * 获取实例化对象 * @author xiaoyuge */@Componentpublic class BeanManager implements ApplicationContextAware &#123; private static ApplicationContext applicationContext = null; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; BeanManager.applicationContext = applicationContext; &#125; public static ApplicationContext getApplicationContext() &#123; return applicationContext; &#125; public static Object getBean(String name) &#123; return applicationContext.getBean(name); &#125; public static &lt;T&gt; T getBean(Class&lt;T&gt; clazz) &#123; return applicationContext.getBean(clazz); &#125;&#125; 构造MQManager 12345678910111213141516171819202122@Configurationpublic class MQManager &#123; @Bean(&quot;messageModel&quot;) public RingBuffer&lt;MessageModel&gt; messageModelRingBuffer() &#123; ExecutorService executor = Executors.newFixedThreadPool(2); //指定事件工厂 HelloEventFactory factory = new HelloEventFactory(); //指定ringBuffer字节大小，必须为2的N次方，能将求模运算转为位运算提高效率，否则将影响效率 int bufferSize = 1024 * 256; Disruptor&lt;MessageModel&gt; disruptor = new Disruptor&lt;&gt;(factory, bufferSize, executor, ProducerType.SINGLE, new BlockingWaitStrategy()); //设置事件业务处理器---消费者 disruptor.handleEventsWith(new HelloEventHandler()); //启动disruptor线程 disruptor.start(); //获取ringBuffer环， 用于接收生产者生产的事件 RingBuffer&lt;MessageModel&gt; ringBuffer = disruptor.getRingBuffer(); return ringBuffer; &#125;&#125; 构造MqService和实现类- 生产者 12345678public interface DisruptorMqService &#123; /** * 消息 * @param message 消息内容 */ void sayHelloMq(String message);&#125; 实现类： 123456789101112131415161718192021222324252627282930@Slf4j@Component@Servicepublic class DsiruptorMqServiceImpl implements DisruptorMqService &#123; @Autowired private RingBuffer&lt;MessageModel&gt; messageModelRingBuffer; /** * 消息 * @param message 消息内容 */ @Override public void sayHelloMq(String message) &#123; log.info(&quot;record the message:&#123;&#125;&quot;, message); //获取下一个Event槽的下标 long sequence = messageModelRingBuffer.next(); try &#123; MessageModel event = messageModelRingBuffer.get(sequence); event.setMessage(message); log.info(&quot;网消息队列中添加消息:&#123;&#125;&quot;,event); &#125;catch (Exception e) &#123; log.error(&quot;添加失败:&#123;&#125;&quot;,e); &#125;finally &#123; //发布Event，激活观察者去消费，将sequence传递给消费者 //注意最后的publish方法必须放在finally中以确保必须调用；如果某个请求的sequence未被提交将会堵塞后续的发布操作或者其他的producer messageModelRingBuffer.publish(sequence); &#125; &#125;&#125; 构造测试类以及方法 12345678910111213141516@Slf4j@SpringBootTest(classes = App.class)@RunWith(SpringRunner.class)public class AppTest &#123; @Autowired private DisruptorMqService disruptorMqService; @Test public void sayHelloTest() throws Exception &#123; disruptorMqService.sayHelloMq(&quot;消息到了， hello world !&quot;); log.info(&quot;消息队列以发送完毕&quot;); //这里停顿2000ms是为了确定处理消息是异步的 Thread.sleep(2000); &#125;&#125; 运行结果： 1234562023-01-10 20:44:15.461 INFO 7643 --- [ main] org.example.DsiruptorMqServiceImpl : record the message:消息到了， hello world !2023-01-10 20:44:15.463 INFO 7643 --- [ main] org.example.DsiruptorMqServiceImpl : 网消息队列中添加消息:MessageModel(message=消息到了， hello world !)2023-01-10 20:44:15.463 INFO 7643 --- [ main] org.example.AppTest : 消息队列以发送完毕2023-01-10 20:44:16.467 INFO 7643 --- [pool-1-thread-1] org.example.handler.HelloEventHandler : 消费者处理消息开始2023-01-10 20:44:16.468 INFO 7643 --- [pool-1-thread-1] org.example.handler.HelloEventHandler : 消费者消费的消息是：MessageModel(message=消息到了， hello world !)2023-01-10 20:44:16.468 INFO 7643 --- [pool-1-thread-1] org.example.handler.HelloEventHandler : 消费者处理消息结束 4. 总结 其实生产者-消费者模式是很常见的，通过一些消息队列也可以轻松做到上述的效果，不同的地方在于：Disruptor是在内存中以队列的方式去实现的，而且是无锁的。这也是 Disruptor 为什么高效的原因","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"SpringBoot代码混淆","slug":"code-obfuscation","date":"2022-12-30T02:35:43.000Z","updated":"2022-12-30T02:35:43.000Z","comments":false,"path":"code-obfuscation/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/code-obfuscation/index.html","excerpt":"","text":"作为一个有经验的程序员，我们都知道 jar/war里面的.class文件都可以逆向还回来，可以通过 JD-GUI 或者IDEA，那么我们该怎么样预防源码暴露呢？ 可以混淆代码，扰乱敌人视线。接下来就一起来看看怎么去实现 代码混淆 首先得弄个springBoot项目吧 在项目根路径新建proguard.cfg文件，然后键入以下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445#指定Java的版本-target 1.8#proguard会对代码进行优化压缩，他会删除从未使用的类或者类成员变量等-dontshrink#是否关闭字节码级别的优化，如果不开启则设置如下配置-dontoptimize#混淆时不生成大小写混合的类名，默认是可以大小写混合-dontusemixedcaseclassnames# 对于类成员的命名的混淆采取唯一策略-useuniqueclassmembernames#混淆时不生成大小写混合的类名，默认是可以大小写混合-dontusemixedcaseclassnames#混淆类名之后，对使用Class.forName(&#x27;className&#x27;)之类的地方进行相应替代-adaptclassstrings #对异常、注解信息予以保留-keepattributes Exceptions,InnerClasses,Signature,Deprecated,SourceFile,LineNumberTable,*Annotation*,EnclosingMethod# 此选项将保存接口中的所有原始名称（不混淆）--&gt;-keepnames interface ** &#123; *; &#125;# 此选项将保存所有软件包中的所有原始接口文件（不进行混淆）#-keep interface * extends * &#123; *; &#125;#保留参数名，因为控制器，或者Mybatis等接口的参数如果混淆会导致无法接受参数，xml文件找不到参数-keepparameternames# 保留枚举成员及方法-keepclassmembers enum * &#123; *; &#125;# 不混淆所有类,保存原始定义的注释--keepclassmembers class * &#123; @org.springframework.context.annotation.Bean *; @org.springframework.beans.factory.annotation.Autowired *; @org.springframework.beans.factory.annotation.Value *; @org.springframework.stereotype.Service *; @org.springframework.stereotype.Component *; &#125; #忽略warn消息-ignorewarnings#忽略note消息-dontnote#打印配置信息-printconfiguration #这里要看清，不要混淆springboot的启动类-keep public class com.yugb.Application &#123; public static void main(java.lang.String[]); &#125; 添加proguard混淆插件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;com.github.wvengen&lt;/groupId&gt; &lt;artifactId&gt;proguard-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;executions&gt; &lt;!-- 以下配置说明执行mvn的package命令时候，会执行proguard--&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;proguard&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;!-- 就是输入Jar的名称，我们要知道，代码混淆其实是将一个原始的jar，生成一个混淆后的jar，那么就会有输入输出。 --&gt; &lt;injar&gt;$&#123;project.build.finalName&#125;.jar&lt;/injar&gt; &lt;!-- 输出jar名称，输入输出jar同名的时候就是覆盖，也是比较常用的配置。 --&gt; &lt;outjar&gt;$&#123;project.build.finalName&#125;.jar&lt;/outjar&gt; &lt;!-- 是否混淆 默认是true --&gt; &lt;obfuscate&gt;true&lt;/obfuscate&gt; &lt;!-- 配置一个文件，通常叫做proguard.cfg,该文件主要是配置options选项，也就是说使用proguard.cfg那么options下的所有内容都可以移到proguard.cfg中 --&gt; &lt;proguardInclude&gt;$&#123;project.basedir&#125;/proguard.cfg&lt;/proguardInclude&gt; &lt;!-- 额外的jar包，通常是项目编译所需要的jar --&gt; &lt;libs&gt; &lt;lib&gt;$&#123;java.home&#125;/lib/rt.jar&lt;/lib&gt; &lt;lib&gt;$&#123;java.home&#125;/lib/jce.jar&lt;/lib&gt; &lt;lib&gt;$&#123;java.home&#125;/lib/jsse.jar&lt;/lib&gt; &lt;/libs&gt; &lt;!-- 对输入jar进行过滤比如，如下配置就是对META-INFO文件不处理。 --&gt; &lt;inLibsFilter&gt;!META-INF/**,!META-INF/versions/9/**.class&lt;/inLibsFilter&gt; &lt;!-- 这是输出路径配置，但是要注意这个路径必须要包括injar标签填写的jar --&gt; &lt;outputDirectory&gt;$&#123;project.basedir&#125;/target&lt;/outputDirectory&gt; &lt;!--这里特别重要，此处主要是配置混淆的一些细节选项，比如哪些类不需要混淆，哪些需要混淆--&gt; &lt;options&gt; &lt;!-- 可以在此处写option标签配置，不过我上面使用了proguardInclude，故而我更喜欢在proguard.cfg中配置 --&gt; &lt;/options&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!-- 这个是启动类路径 --&gt; &lt;mainClass&gt;com.yugb.Application&lt;/mainClass&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 然后就可以看到： 正常执行编译打包流程 打包编译+混淆 打开jar包，反编译class 这打开一看，这啥东西啊，谁看谁不迷糊啊。接下来反编译走一波。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://xiaoyuge5201.github.io/tags/springboot/"}]},{"title":"SQL语句执行过程","slug":"sql-execution-process","date":"2022-12-26T06:23:24.000Z","updated":"2022-12-26T06:23:24.000Z","comments":false,"path":"sql-execution-process/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/sql-execution-process/index.html","excerpt":"","text":"1. Mysql 逻辑架构 作为一个有经验的程序员，每天都在和mysql打交道，那么你知道执行一条简单的select语句，需要经历哪些过程？这个就是今天要讲的内容。 在讲sql语句执行过程之前需要了解mysql的架构是怎样的以及它包含哪些组件？ 分为Server层和存储引擎两部分 Server 层包括：连接器、查询缓存、分析器、优化器、执行器等，涵盖Mysql的大多数核心服务功能，以及所有的内置函数（如日期、时间、数据以及加密函数等）,所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 存储引擎层：负责数据的存储和提取，其架构模式是插件式的，支持Innodb、MyISAM、Memory等多个存储引擎，Mysql5.5.5后默认存储引擎为Innodb 2. SQL语句执行过程 连接器 负责将mysql客户端和服务端建立连接，连接成功后，获取的当前连接用户的权限，这里获取到的权限对整个连接都有效，一旦连接成功后，如果中途管理员更改了权限，那么只有等到该用户下次重连才会更新权限 查询缓存 连接成功后，即开始要正式执行select 语句了，但是在执行查询之前，mysql会看下有没有该条语句的缓存内容，如果有缓存直接读取缓存并返回结果，不在执行后面的步骤。 如果没有缓存则继续往后执行，并将执行结果和语句保存到缓存中 注意： mysql8 后已经没有查询缓存这个功能了，因为缓存非常容易被清空掉，命中率低，只要对表有一个更新呢，这个表上的所有缓存都会被清空 如果缓存查到结果，在返回结果前需要做权限校验，判断该用户是否有权限查询 分析器 既然没有查到缓存，就要执行sql语句了，在执行之前肯定需要先对sql语句进行解析，分析器主要对sql语句进行语法和语义分析，检查单词是否拼写错误，还有检查这个表或者字段是否存在。 假如分析器检测出错误，会出现以下错误信息： 12ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ‘xxxxxx’ at line 1一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 并且停止查询操作。 优化器 在经过分析器后，mysql就理解了你所要执行的操作了，基本上对于同一个sql语句，mysql内部或许有好几种执行方案，例如存在多个索引的情况下，应该选择哪个索引？多张表有关联查询时，如何确认各表的连接顺序。 这些方案的执行结果都一样，但是执行效率不一样，所以在mysql执行之前都要尝试找出一个最优的方案来，这就是优化器主要工作。 执行器 经过优化器选定一个方案后，执行器就按照这个方案执行sql语句，在执行语句之前会先判断是否有权限，如果没有权限就会return，并提示没有权限 权限校验完毕，继续打开表，然后调用存储引擎所提供的接口去查询，最后返回结果数据。 到这，一条sql语句就执行完了，这里知识讲了大致的流程，具体在mysql底层实现是非常复杂的。","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"sql","slug":"sql","permalink":"https://xiaoyuge5201.github.io/tags/sql/"}]},{"title":"注解@ConditionalOnClass的源码实现","slug":"conditional-on-class","date":"2022-12-19T13:34:57.000Z","updated":"2022-12-19T13:34:57.000Z","comments":false,"path":"conditional-on-class/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/conditional-on-class/index.html","excerpt":"","text":"在Springboot中，支持了很多条件注解，@ConditionalOnClass注解就是其中之一。它主要是用来判断该注解所指定的某个类或者某些类是否在ClassPath中存在，如果存在则符合条件，如果不存在则不符合。 注解源码： 12345678@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)@Retention(RetentionPolicy.RUNTIME)@Documented@Conditional(OnClassCondition.class)public @interface ConditionalOnClass &#123; Class&lt;?&gt;[] value() default &#123;&#125;; String[] name() default &#123;&#125;;&#125; 通过value和name来指定要判断的类，而真正执行判断的逻辑在OnClassCondition类中。 ConditionalOnClass注解逻辑在OnClassCondition类中 OnClassCondition继承了SpringBootCondition类 SpringBootCondition类实现了Condition接口 所以Spring在解析条件注解时，就会调用Condition接口的matches()方法，SpringBootCondition类中实现了matches()方法，所以会被先调用 1234567891011121314151617181920212223242526272829303132public abstract class SpringBootCondition implements Condition &#123; private final Log logger = LogFactory.getLog(getClass()); @Override public final boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; String classOrMethodName = getClassOrMethodName(metadata); try &#123; ConditionOutcome outcome = getMatchOutcome(context, metadata); logOutcome(classOrMethodName, outcome); recordEvaluation(context, classOrMethodName, outcome); return outcome.isMatch(); &#125; catch (NoClassDefFoundError ex) &#123; throw new IllegalStateException( &quot;Could not evaluate condition on &quot; + classOrMethodName + &quot; due to &quot; + ex.getMessage() + &quot; not &quot; + &quot;found. Make sure your own configuration does not rely on &quot; + &quot;that class. This can also happen if you are &quot; + &quot;@ComponentScanning a springframework package (e.g. if you &quot; + &quot;put a @ComponentScan in the default package by mistake)&quot;, ex); &#125; catch (RuntimeException ex) &#123; throw new IllegalStateException( &quot;Error processing condition on &quot; + getName(metadata), ex); &#125; &#125; //....................省略..............&#125; 在matches方法中，会调用getMatchOutcome()方法，并得到ConditionOutcome对象，它表示的就是条件判断的结果。 12345678public class ConditionOutcome &#123; //表示条件是否匹配 private final boolean match; private final ConditionMessage message; //....................省略..............&#125; getMatchOutcome()方法在SpringBootCondition类中是一个抽象方法，在子类OnClassCondition类中才真正实现了getMatchOutcome方法 进行条件判断. 所以核心就是这个getMatchOutcome()方法，在这个方法中会先获取@ConditionalOnClass注解的value和name属性的值，这些值就是待判断的类名集合。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960//OnClassCondition 类@Order(Ordered.HIGHEST_PRECEDENCE)class OnClassCondition extends SpringBootCondition implements AutoConfigurationImportFilter, BeanFactoryAware, BeanClassLoaderAware &#123; //..................省略................ @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; ClassLoader classLoader = context.getClassLoader(); ConditionMessage matchMessage = ConditionMessage.empty(); //重点：找到所有包含ConditionalOnClass 注解的类 List&lt;String&gt; onClasses = getCandidates(metadata, ConditionalOnClass.class); if (onClasses != null) &#123; //然后根据classLoader去加载，判断类是否存在 List&lt;String&gt; missing = getMatches(onClasses, MatchType.MISSING, classLoader); if (!missing.isEmpty()) &#123; //如果集合不为空，表示类有不存在的，返回不匹配对象 return ConditionOutcome .noMatch(ConditionMessage.forCondition(ConditionalOnClass.class) .didNotFind(&quot;required class&quot;, &quot;required classes&quot;) .items(Style.QUOTE, missing)); &#125; matchMessage = matchMessage.andCondition(ConditionalOnClass.class) .found(&quot;required class&quot;, &quot;required classes&quot;).items(Style.QUOTE, getMatches(onClasses, MatchType.PRESENT, classLoader)); &#125; List&lt;String&gt; onMissingClasses = getCandidates(metadata, ConditionalOnMissingClass.class); if (onMissingClasses != null) &#123; List&lt;String&gt; present = getMatches(onMissingClasses, MatchType.PRESENT, classLoader); if (!present.isEmpty()) &#123; return ConditionOutcome.noMatch( ConditionMessage.forCondition(ConditionalOnMissingClass.class) .found(&quot;unwanted class&quot;, &quot;unwanted classes&quot;) .items(Style.QUOTE, present)); &#125; matchMessage = matchMessage.andCondition(ConditionalOnMissingClass.class) .didNotFind(&quot;unwanted class&quot;, &quot;unwanted classes&quot;).items(Style.QUOTE, getMatches(onMissingClasses, MatchType.MISSING, classLoader)); &#125; //如果都存在则返回匹配的ConditionOutcome对象 return ConditionOutcome.match(matchMessage); &#125; private List&lt;String&gt; getCandidates(AnnotatedTypeMetadata metadata, Class&lt;?&gt; annotationType) &#123; MultiValueMap&lt;String, Object&gt; attributes = metadata.getAllAnnotationAttributes(annotationType.getName(), true); if (attributes == null) &#123; return Collections.emptyList(); &#125; List&lt;String&gt; candidates = new ArrayList&lt;&gt;(); addAll(candidates, attributes.get(&quot;value&quot;)); addAll(candidates, attributes.get(&quot;name&quot;)); return candidates; &#125;&#125; 接下来就会逐个判断类名集合中的每个类名，判断逻辑为：利用MatchType.MISSING来判断某个类是否存在。 1List&lt;String&gt; missing = getMatches(onClasses, MatchType.MISSING, classLoader); 12345678910private List&lt;String&gt; getMatches(Collection&lt;String&gt; candidates, MatchType matchType, ClassLoader classLoader) &#123; List&lt;String&gt; matches = new ArrayList&lt;&gt;(candidates.size()); for (String candidate : candidates) &#123; if (matchType.matches(candidate, classLoader)) &#123; matches.add(candidate); &#125; &#125; return matches;&#125; MatchType.MISSING就是利用ClassLoader来加载类，如果加载到了表示类存在，没加载到表示类不存在 1234567891011121314151617181920212223242526272829303132333435363738private enum MatchType &#123; PRESENT &#123; @Override public boolean matches(String className, ClassLoader classLoader) &#123; return isPresent(className, classLoader); &#125; &#125;, MISSING &#123; @Override public boolean matches(String className, ClassLoader classLoader) &#123; return !isPresent(className, classLoader); &#125; &#125;; private static boolean isPresent(String className, ClassLoader classLoader) &#123; if (classLoader == null) &#123; classLoader = ClassUtils.getDefaultClassLoader(); &#125; try &#123; forName(className, classLoader); return true; &#125; catch (Throwable ex) &#123; return false; &#125; &#125; private static Class&lt;?&gt; forName(String className, ClassLoader classLoader) throws ClassNotFoundException &#123; if (classLoader != null) &#123; return classLoader.loadClass(className); &#125; return Class.forName(className); &#125; public abstract boolean matches(String className, ClassLoader classLoader);&#125; 判断完之后，只要missing集合不为空，那就表示待判断的类中有不存在的，就返回条件不匹配的ConditionOutCome对象，否则就返回条件匹配的ConditionOutCome对象。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"cglib 底层源码分析","slug":"cglib","date":"2022-12-14T14:13:31.000Z","updated":"2022-12-14T14:13:31.000Z","comments":false,"path":"cglib/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/cglib/index.html","excerpt":"","text":"1. cglib 介绍 cglib是一个功能强大，高性能的代码生成包，它为没有实现接口的类提供代理，为JDK的动态代理提供了很好的补充。 1.1 cglib原理 原理：动态生成一个要代理类的子类，子类重写要代理的类的所有不是final的方法。在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。它比使用java反射的JDK动态代理要快。 底层：使用字节码处理框架ASM，来转换字节码并生成新的类。不鼓励直接使用ASM，因为它要求你必须对JVM内部结构包括class文件的格式和指令集都很熟悉。 缺点：对于final方法，无法进行代理。 1.2 cglib的应用 广泛的被许多AOP的框架使用，例如Spring AOP和dynaop。Hibernate使用CGLIB来代理单端single-ended(多对一和一对一)关联。 1.3 为什么使用cglib CGLIB代理主要通过对字节码的操作，为对象引入间接级别，以控制对象的访问。我们知道Java中有一个动态代理也是做这个事情的，那我们为什么不直接使用Java动态代理，而要使用CGLIB呢？答案是CGLIB相比于JDK动态代理更加强大，JDK动态代理虽然简单易用，但是其有一个致命缺陷是，只能对接口进行代理。如果要代理的类为一个普通类、没有接口，那么Java动态代理就没法使用了 1.4 cglib 组成结构 CGLIB底层使用了ASM（一个短小精悍的字节码操作框架）来操作字节码生成新的类。除了CGLIB库外，脚本语言（如Groovy何BeanShell）也使用ASM生成字节码。ASM使用类似SAX的解析器来实现高性能。我们不鼓励直接使用ASM，因为它需要对Java字节码的格式足够的了解。 1.5 cglib的API 1、Jar包： cglib-nodep-2.2.jar：使用nodep包不需要关联asm的jar包,jar包内部包含asm的类. cglib-2.2.jar：使用此jar包需要关联asm的jar包,否则运行时报错. 2、CGLIB类库： 由于基本代码很少，学起来有一定的困难，主要是缺少文档和示例，这也是CGLIB的一个不足之处。 本系列使用的CGLIB版本是2.2。 net.sf.cglib.core: 底层字节码处理类，他们大部分与ASM有关系。 net.sf.cglib.transform: 编译期或运行期类和类文件的转换 net.sf.cglib.proxy: 实现创建代理和方法拦截器的类 net.sf.cglib.reflect: 实现快速反射和C#风格代理的类 net.sf.cglib.util: 集合排序等工具类 net.sf.cglib.beans: JavaBean相关的工具类 2. cglib使用 上面讲了一大段cglib的基本概念以及Api等，那么下面开始介绍通过MethodInterceptor和Enhancer实现一个动态代理，看下cglib底层是如何实现动态代理的。 被代理类： 123456789101112public class TargetObject&#123; public String method()&#123; return &quot;method&quot;; &#125; public void voidMethod()&#123; System.out.println(&quot;-----void-----&quot;); &#125; public String method1(String name)&#123; return name; &#125;&#125; 定义一个拦截器，在调用目标方法时，cglib会回调MethodInterceptor接口方法拦截，来实现自己的代理逻辑，类似于JDK中的InvocationHandler接口 12345678910111213141516171819202122232425/** * 目标对象拦截器实现, 实现MethodInterceptor * * @author xiaoyuge */public class TargetInterceptor implements MethodInterceptor &#123; /** * 重写方法拦截在方法前和方法后加入业务 * * @param o 目标对象, 由cglib动态生成的代理类示例 * @param method 目标方法，实体类所调用的被代理的方法引用 * @param objects 参数值列表 * @param methodProxy cglib方法代理对象 * @return 结果 * @throws Throwable 异常信息 */ @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println(&quot;--------调用前-------&quot;); //调用代理类示例上的proxy 方法的弗雷方法（即实体类TargetObject中对应的方法） Object result = methodProxy.invokeSuper(o, objects); System.out.println(&quot;------调用后:&quot; + result); return result; &#125;&#125; 生成动态代理类 123456789101112131415import net.sf.cglib.proxy.Enhancer;public class TestCglib &#123; public static void main(String[] args) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TargetObject.class); enhancer.setCallback(new TargetInterceptor()); TargetObject target = (TargetObject) enhancer.create(); System.out.println(target); System.out.println(target.method1(&quot;xiaoyuge&quot;)); System.out.println(target.method()); target.voidMethod(); &#125;&#125; 输出： 123456789org.example.TargetObject$$EnhancerByCGLIB$$8db9696e@5e91993f--------调用前-------------调用后:xiaoyugexiaoyuge--------调用前-------------调用后:method--------调用前------------void-----------调用后:null Enhancer类：cglib中的一个字节码增强器，他可以方便的对你想要处理的类进行扩展。 首先将被代理类设置为父类，然后设置拦截器TargetInterceptor，最后执行enhancer.create()动态生成一个代理类，并从Object强转为弗类型TargetObject，最后在代理类上调用方法 回调过滤器CallbackFilter 在cglib回调时可以设置不同方法执行不同的回调逻辑，或者根本不执行回调，在JDK动态代理中并没有类似的功能，对InvocationHandler接口方法的调用对代理类内的所以方法都有效。 定义实现过滤器CallbackFilter接口的类： 1234567891011121314151617181920212223242526272829303132333435import net.sf.cglib.proxy.CallbackFilter;import java.lang.reflect.Method;/** * 回调方法过滤器 * * @author xiaoyuge */public class TargetMethodCallbackFilter implements CallbackFilter &#123; /** * 过滤方法， * 返回的值为数字，代表类Callback数组中的索引位置 * * @param method 方法 * @return 结果 */ @Override public int accept(Method method) &#123; if (&quot;method1&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter method1 ==0&quot;); return 0; &#125; if (&quot;method&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter method1 ==1&quot;); return 1; &#125; if (&quot;voidMethod&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter method1 ==2&quot;); return 2; &#125; return 0; &#125;&#125; 其中return值为被代理类的各个方法在回调数组Callback[]中的位置 12345678910111213141516171819import net.sf.cglib.proxy.FixedValue;/** * 表示锁定方法返回值，无论被代理类的方法返回什么值，回调方法都返回固定值。 * @author xiaoyuge * */public class TargetResultFixed implements FixedValue&#123; /** * 该类实现FixedValue接口，同时锁定回调值为123 * (整型，CallbackFilter中定义的使用FixedValue型回调的方法为getConcreteMethodFixedValue，该方法返回值为整型)。 */ @Override public Object loadObject() throws Exception &#123; System.out.println(&quot;锁定结果&quot;); return 123; &#125; &#125; 12345678910111213141516171819202122232425262728import net.sf.cglib.proxy.Callback;import net.sf.cglib.proxy.CallbackFilter;import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.NoOp;public class TestCglib &#123; public static void main(String[] args) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TargetObject.class); CallbackFilter callbackFilter = new TargetMethodCallbackFilter(); //NoOp.INSTANC:表示no operator， 即什么操作也不做，代理类直接调用被代理的方法不进行拦截 Callback noopCb = NoOp.INSTANCE; Callback callback = new TargetInterceptor(); //FixedValue：表示锁定方法返回值，无论被代理类的方法返回什么值，回调方法都返回固定值。 Callback fixedValue = new TargetResultFixed(); Callback[] array = new Callback[]&#123;callback, noopCb, fixedValue&#125;; enhancer.setCallbacks(array); enhancer.setCallbackFilter(callbackFilter); TargetObject targetObject = (TargetObject) enhancer.create(); System.out.println(targetObject); System.out.println(targetObject.method()); System.out.println(targetObject.method1(&quot;xiaoyuge&quot;)); targetObject.voidMethod(); &#125;&#125; 输出： 12345678910111213filter method1 ==2filter method1 ==0filter method1 ==1--------调用前---------------调用前-------------调用后:359023572------调用后:org.example.TargetObject$$EnhancerByCGLIB$$92522e49@156643d4org.example.TargetObject$$EnhancerByCGLIB$$92522e49@156643d4method--------调用前-------------调用后:xiaoyugexiaoyuge锁定结果 延迟加载对象 LazyLoader接口实现了Callback，因此也算是cglib中的一种Callback类型 另一种延迟加载接口Dispatcher，同样也继承于Callback，也是一种回调类型。 两者的区别在于：LazyLoader只在第一次访问延迟加载属性时除法代理类的回调方法，而Dispatcher在每次访问延迟加载属性时都会触发代理类回调方法 定义一个实体类LoaderBean,该bean内又一个需要延迟加载的属性PropertyBean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import net.sf.cglib.proxy.Enhancer;public class LazyBean &#123; private String name; private int age; /** * 延迟加载属性 */ private PropertyBean propertyBean; private PropertyBean propertyBeanDispatcher; public LazyBean(String name, int age, PropertyBean propertyBean, PropertyBean propertyBeanDispatcher) &#123; this.name = name; this.age = age; this.propertyBean = propertyBean; this.propertyBeanDispatcher = propertyBeanDispatcher; &#125; /** * 只有第一次懒加载 * 使用cglib进行懒加载，对需要延迟加载的对象添加代理，在获取该对象属性时先通过代理类回调方法进行对象初始化 * 在不需要加载该对象时，只要不去获取该对象内属性，该对象就不会补初始化类（在cglib的实现中只要去访问该对象内属性的getter方法就会触发代理类回调） * @return 对象 */ private PropertyBean createPropertyBean() &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(PropertyBean.class); return (PropertyBean) Enhancer.create(PropertyBean.class, new ConcreteClassLazyLoader()); &#125; /** * 每次都懒加载 * * @return 对象 */ private PropertyBean createPropertyBeanDispatcher() &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(PropertyBean.class); return (PropertyBean) Enhancer.create(PropertyBean.class, new ConcreteClassDispatcher()); &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public PropertyBean getPropertyBean() &#123; return propertyBean; &#125; public void setPropertyBean(PropertyBean propertyBean) &#123; this.propertyBean = propertyBean; &#125; public PropertyBean getPropertyBeanDispatcher() &#123; return propertyBeanDispatcher; &#125; public void setPropertyBeanDispatcher(PropertyBean propertyBeanDispatcher) &#123; this.propertyBeanDispatcher = propertyBeanDispatcher; &#125; @Override public String toString() &#123; return &quot;LazyBean&#123;&quot; + &quot;name=&#x27;&quot; + name + &#x27;\\&#x27;&#x27; + &quot;, age=&quot; + age + &quot;, propertyBean=&quot; + propertyBean + &quot;, propertyBeanDispatcher=&quot; + propertyBeanDispatcher + &#x27;&#125;&#x27;; &#125;&#125; 延迟加载类 PropertyBean : 1234567891011121314151617181920212223public class PropertyBean &#123; private String key; private Object value; public String getKey() &#123; return key; &#125; public void setKey(String key) &#123; this.key = key; &#125; public Object getValue() &#123; return value; &#125; @Override public String toString() &#123; return &quot;PropertyBean&#123;&quot; + &quot;key=&#x27;&quot; + key + &#x27;\\&#x27;&#x27; + &quot;, value=&quot; + value + &#x27;&#125;&#x27;; &#125; public void setValue(Object value) &#123; this.value = value; &#125;&#125; 延迟加载器： 12345678910111213141516171819202122import net.sf.cglib.proxy.LazyLoader;/** * @author xiaoyuge */public class ConcreteClassLazyLoader implements LazyLoader &#123; /** * 对于需要延迟加载的对象添加代理，在获取对象属性时先通过代理类回调方法进行对象初始化 * 在不需要加载该对象时，只要不去获取该对象内属性，该对象就不会补初始化类（在cglib的实现中只要去访问该对象内属性的getter方法就会触发代理类回调） * @return 延迟加载对象 * @throws Exception 异常信息 */ @Override public Object loadObject() throws Exception &#123; System.out.println(&quot;before lazyLoader...&quot;); PropertyBean propertyBean = new PropertyBean(); propertyBean.setKey(&quot;xiaoyuge&quot;); propertyBean.setValue(new TargetObject()); System.out.println(&quot;after lazyLoader...&quot;); return propertyBean; &#125;&#125; 1234567891011121314/** * @author xiaoyuge */public class ConcreteClassDispatcher implements Dispatcher &#123; @Override public Object loadObject() throws Exception &#123; System.out.println(&quot;before Dispatcher...&quot;); PropertyBean propertyBean = new PropertyBean(); propertyBean.setKey(&quot;nb&quot;); propertyBean.setValue(new TargetObject()); System.out.println(&quot;after Dispatcher...&quot;); return propertyBean; &#125;&#125; 测试方法： 123456789101112public class TestCglib2 &#123; public static void main(String[] args) &#123; LazyBean bean = new LazyBean(&quot;xiaoyuge&quot;, 18); //LazyLoader只在第一次访问延迟加载属性时触发代理类回调方法 System.out.println(&quot;延迟加载&quot; + bean.getPropertyBean().toString()); System.out.println(&quot;延迟加载&quot; + bean.getPropertyBean().toString()); //Dispatcher在每次访问延迟加载属性时都会触发代理类回调方法。 Object obj = bean.getPropertyBeanDispatcher().getValue(); Object obj1 = bean.getPropertyBeanDispatcher().getValue(); &#125;&#125; 输出： 12345678910before lazyLoader...after lazyLoader...before Dispatcher...after Dispatcher...延迟加载PropertyBean&#123;key=&#x27;xiaoyuge&#x27;, value=org.example.TargetObject@45fe3ee3&#125;延迟加载PropertyBean&#123;key=&#x27;xiaoyuge&#x27;, value=org.example.TargetObject@45fe3ee3&#125;before Dispatcher...after Dispatcher...before Dispatcher...after Dispatcher... 从上面可以看出：Dispatcher在每次访问延迟加载属性时都会触发代理类回调方法，LazyLoader只在第一次访问延迟加载属性时触发代理类回调方法 接口生成器 InterfaceMaker 会动态生成一个接口，该接口包含指定类的所有方法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import net.sf.cglib.proxy.Enhancer;import net.sf.cglib.proxy.InterfaceMaker;import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import org.junit.jupiter.api.Test;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class InterfaceMakerDemo &#123; @Test public void test() throws NoSuchMethodException, InvocationTargetException, IllegalAccessException &#123; InterfaceMaker interfaceMaker = new InterfaceMaker(); //抽取某个类的方法生成接口方法 interfaceMaker.add(TargetObject.class); Class&lt;?&gt; targetInterface = interfaceMaker.create(); for (Method method : targetInterface.getMethods()) &#123; System.out.println(method.getName()); &#125; //接口代理并设置代理接口方法拦截 Object object = Enhancer.create(Object.class, new Class[]&#123;targetInterface&#125;, new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; if (&quot;method&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter method&quot;); return &quot;method&quot;; &#125; if (&quot;method1&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter method1&quot;); return &quot;method1&quot;; &#125; if (&quot;voidMethod&quot;.equals(method.getName())) &#123; System.out.println(&quot;filter voidMethod&quot;); return &quot;filter voidMethod&quot;; &#125; return &quot;default&quot;; &#125; &#125;); Method targetMethod = object.getClass().getMethod(&quot;method1&quot;, String.class); String value = (String) targetMethod.invoke(object, new Object[]&#123;&quot;xiaoyuge&quot;&#125;); System.out.println(&quot;----&quot;+value); Method targetMethod1 = object.getClass().getMethod(&quot;method&quot;); String value1 = (String) targetMethod1.invoke(object, new Object[] &#123;&#125;); System.out.println(&quot;----&quot;+value1); &#125;&#125; 输出： 1234567method1voidMethodmethodfilter method1----method1filter method----method 3. 底层源码分析 上面讲了通过MethodInterceptor和Enhancer实现一个动态代理，那么它的底层是如何实现的呢？ 我们都知道，既然要生成一个代理对象，那么肯定需要一个代理类，只不过当我们使用cglib时，这个代理类是由cglib生成的，那么我们要看到这个代理类是怎么生成的，只需要在运行时加上： 1-Dcglib.debugLocation=/Users/xiaoyuge/Desktop/classes cglib就会将生成的代理类放到上面所指定的路径上。 先看原来的类： 12345public class TargetObject &#123; public String method() &#123; return &quot;method&quot;; &#125;&#125; 测试方法： 12345678910111213141516171819202122@Test public static void testTargetObject()&#123; final TargetObject target = new TargetObject(); Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TargetObject.class); enhancer.setCallbacks(new Callback[]&#123;new MethodInterceptor() &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; //test 方法增加了其他的逻辑 if (&quot;method1&quot;.equals(method.getName())) &#123; System.out.println(&quot;------before------&quot;); Object result = method.invoke(target, objects); System.out.println(&quot;------after-------&quot;); return result; &#125; //其他方法正常执行 return method.invoke(target, objects); &#125; &#125;&#125;); TargetObject t = (TargetObject) enhancer.create(); t.method(); &#125; cglib代理类： 1234567891011121314public class TargetObject$$EnhancerByCGLIB$$8db9696e extends TargetObject implements Factory &#123; //............省略......... final String CGLIB$method1$0(String var1) &#123; return super.method1(var1); &#125; public final String method1(String var1) &#123; MethodInterceptor var10000 = this.CGLIB$CALLBACK_0; if (var10000 == null) &#123; CGLIB$BIND_CALLBACKS(this); var10000 = this.CGLIB$CALLBACK_0; &#125; return var10000 != null ? (String)var10000.intercept(this, CGLIB$method1$0$Method, new Object[]&#123;var1&#125;, CGLIB$method1$0$Proxy) : super.method1(var1); &#125;&#125; 可以看到代理类继承了TargetObject，然后重写了TargetObject里面的方法，通过代理对象去调用，代理对象中包含了一个CGLIB$method1$0和一个method1方法。 method1方法内会调用所设置的Callbacks中的intercept(), 相当于执行增强逻辑，如果没有Callbacks，则会执行super.method1(); 那么如果不设置Callbacks，是否能够正常执行，接着往下看 1234567@Testpublic void test()&#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TargetObject.class); TargetObject t = (TargetObject) enhancer.create(); t.method();&#125; 运行时，cglib在构造对象时就会报一个没有Callbacks的控制针异常。 12345678java.lang.NullPointerExceptiontrueat net.sf.cglib.proxy.CallbackInfo.determineTypes(CallbackInfo.java:39)trueat net.sf.cglib.proxy.Enhancer.preValidate(Enhancer.java:350)trueat net.sf.cglib.proxy.Enhancer.createHelper(Enhancer.java:471)trueat net.sf.cglib.proxy.Enhancer.create(Enhancer.java:305)trueat org.example.TestBitwise.test(TestBitwise.java:17)trueat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)true//..... 再来看代理类中的另一个方法： 123final String CGLIB$method1$0(String var1) &#123; return super.method1(var1);&#125; 这个方法我们不能直接调用，要通过所设置的Callback，也就是MethodInterceptor中MethodProxy对象来调用，MethodProxy对象表示方法代理。 假如TargetObject代理对象在执行method1方法，那么当执行流程进入到intercept()方法时，MethodProxy对象表示的就是method1()方法，但是我们现在知道了在TargetObject类和代理类都有method1方法，所以MethodProxy对象代理的就是这两个method1方法。 比如： 1234567891011121314// o 表示当前代理对象， target表示被代理对象//执行TargetObject代理对象的CGLIB$method1$0方法，也就是执行TargetObject代理对象的父类的method1方法Object result1 = methodProxy.invokeSuper(o, objects);//执行TargetObject代理对象的CGLIB$method1$0方法，会报错，调用invokeSuper只能传入代理对象Object result2 = methodProxy.invokeSuper(target, objects);//执行TargetObject对象method1方法Object result3 = methodProxy.invoke(o, objects);//执行TargetObject代理对象的method1方法，又会进入拦截器，栈溢出Object result4 = methodProxy.invokeSuper(target, objects); 所以在执行methodProxy.invokeSuper()方法时，就会去执行CGLIB$method1$0()方法。 总结一下cglib的大概工作原理是：cglib会根据所设置的Superclass，生成代理类作为其子类，并且会重写Superclass中的方法，Superclass中的某一个方法，比如method1()，相应的在代理类中会对应两个方法，一个是重写的method1()，用来执行增强逻辑，一个是CGLIB$method1$0()，会直接调用super.test()，是让MethodProxy对象来用的。 4. 流程总结 首先生成代理对象。创建增强类Enhancer，设置代理类的父类，设置回调拦截方法，返回创建的代理对象； 调用代理类中的方法。这里调用的代理类中的方法实际上是重写的父类的拦截。重写的方法中会去调用intercept方法; 调用intercept，方法中会对调用代理方法中的invokeSuper方法。而在 invokeSuper 中维护了一个 FastClassInfo类，其包含四个属性字段: FastClass f1(目标类)、 FastClass f2 (代理类)、 int i1(目标类要执行方法的下标)、 int i2(代理类要执行方法的下标); 123456789101112131415161718private static class FastClassInfo &#123; FastClass f1; FastClass f2; int i1; int i2; private FastClassInfo() &#123; &#125; &#125;public Object invokeSuper(Object obj, Object[] args) throws Throwable &#123; try &#123; this.init(); MethodProxy.FastClassInfo fci = this.fastClassInfo; return fci.f2.invoke(fci.i2, obj, args); &#125; catch (InvocationTargetException var4) &#123; throw var4.getTargetException(); &#125; &#125; invokeSuper中会调用的为代理类中的对应方法（代理类继承于父类的时候，对于其父类的方法，自己会生成两个方法，一个是重写的方法，一个是代理生成的方法，这里调用的即是代理生成的方法）； 调用代理类中的代理方法。代理方法中通过super.xxxx(string)来实际真正的调用要执行的方法； 5. JDK与cglib的区别 JDK动态代理 123456789public static Object getProxy(Object proxyObj) &#123; return Proxy.newProxyInstance(Thread.currentThread().getContextClassLoader(), proxyObj.getClass().getInterfaces(), new MyInvocationHandler(proxyObj));&#125;public static void main(String[] args) &#123; UserDetailService service = (UserDetailService) getProxy(new UserDetailServiceImpl()); service.method1(&quot;xiaoyuge&quot;);&#125; cglib动态代理 12345678910111213private static Enhancer enhancer = new Enhancer();private static TargetInterceptor cglibProxy = new TargetInterceptor();public static Object getProxy(Class cls) &#123; enhancer.setSuperclass(cls); enhancer.setCallback(cglibProxy); return enhancer.create();&#125;public static void main(String[] args) throws InterruptedException &#123; TargetObject target = (TargetObject) getProxy(TargetObject.class); target.method1(&quot;xiaoyuge&quot;);&#125; 个人理解两者的区别在于：JDK动态代理是基于委托思想，而CGLib动态代理是基于继承的思想！！！ 基于委托思想，JDK生成动态代理类的时候，需要传入被代理类（被委托类）的对象，可以看作是对象级别的重用机制 基于继承思想，动态代理类继承了被代理类，理论上父类的所有开放方法对于子类都是可见的，可以看作是类级别的重用机制； 动态代理 = 拦截器机制 + 回溯到被代理类的能力 对于JDK动态代理： JDK动态代理 = 拦截器机制（InvocationHandler) + 回溯到被代理类的能力(反射调用被代理类对象相关方法） 在JDK动态代理中，生成的代理类的全限定类名是com.sun.proxy.ProxyN（N是正整数，比如ProxyN（N是正整数，比如ProxyN（N是正整数，比如Proxy0），它继承了com.sun.proxy类，该类中存在一个InvocationHandler类型的h成员变量，它就是拦截器。但这里会存在一个问题，由于我们希望代理类和被代理类在行为上是一致的（具有相同的类型），所以JDK动态代理需要引入接口的概念，代理对象和被代理对象需要具有相同的接口定义。 所以，在我们使用JDK动态代理的过程中，我们需要自定义拦截器，实现InvocationHandler 接口，然后将被代理对象（被委托对象）注入到拦截器中。当调用接口方法时，会首先调用拦截器的invoke方法，拦截器invoke方法内部，会经过反射去调用被代理对象的相应方法。 123456789101112public class MyInvocationHandler implements InvocationHandler &#123; private Object target; public MyInvocationHandler(Object target) &#123; this.target = target; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //执行目标方法对象 Object result = method.invoke(target, args); return result; &#125;&#125; 对于CGLIB动态代理： CGLIB动态代理 = 拦截器机制（MethodInterceptor) + 回溯到被代理类的能力 (FastClass辅助类、MethodProxy类） 在CGLIB动态代理中，生成的代理类的全限定类名是很自由的。因为它是基于继承机制，代理类继承了被代理类。 在生成的代理类中，会存在一个MethodInterceptor类型的CGLIB$CALLBACK_0成员变量，它就是拦截器。由于是继承，代理类天然就可以调用到父类（被代理类）的方法，因此这里不再需要注入被代理类的对象实例了。但这里仍然存在一个很核心的问题：代理类看起来，既要能够调用到拦截器，又要可以回溯到父类（被代理类）的原始方法，这看起来很矛盾。怎么解决呢？ 其实很简单，CGLIB生成的代理，对于被代理类的原有方法（比如上面的method1方法），会调用到拦截器。而与此同时，CGLIB还增加了隐藏的能够回溯到原始方法的传送门方法（比如CGLIB$method1$0） 123456789public class CglibProxy implements MethodInterceptor &#123; @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; //执行目标方法对象 Object result = methodProxy.invokeSuper(o, objects); return result; &#125;&#125; 两者区别 JDK动态代理基于接口实现，必须先定义接口； CGLib动态代理基于被代理类实现，可以直接定义类或者实现接口的类； JDK动态代理需要实现InvocationHanlder接口，加上反射机制实现代理类 CGLib动态代理需要实现MethodInterceptor接口，对于代理类不可使用final修饰 JDK动态代理是委托机制，委托hanlder，生成新的委托类，调用实现类方法； CGLib动态代理则使用继承机制，被代理类和代理类是继承关系，直接调用其中方法；","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"一文搞明白位运算、补码、反码、原码","slug":"bitwise-operation","date":"2022-12-12T13:29:59.000Z","updated":"2022-12-12T13:29:59.000Z","comments":false,"path":"bitwise-operation/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/bitwise-operation/index.html","excerpt":"","text":"在平时看各种框架的源码过程中，经常会看到一些位移运算，所以作为一个有经验的Java开发者是一定要掌握位移运算的。 1. 正数位移运算 java中有三个位移运算： &lt;&lt;：左移 &gt;&gt;：右移 &gt;&gt;&gt;：无符号右移 123456System.out.println(2 &lt;&lt; 1); //4System.out.println(2 &gt;&gt; 1); //1System.out.println(2 &gt;&gt;&gt; 1); //1System.out.println(-2 &lt;&lt; 1); //-4System.out.println(-2 &gt;&gt; 1); //-1System.out.println(-2 &gt;&gt;&gt; 1); //2147483647 下面就来详细解释以下结果是如何运算出来的。 上面Demo中有 2和 -2 ，这两个十进制数，并且是int类型（Java中占4个字节），位元算是基于二进制bit来的，所以我们需要将十进制转化为二进制之后再进行运算： 2 &lt;&lt; 1：十进制2转化为二进制为00000000 00000000 00000000 00000010,再将二进制左移 1 位，高位丢弃，低位补0，所以结果为00000000 00000000 00000000 00000100，换算成十进制为4（2n-1 其中n为非0为下标 ） 2 &gt;&gt; 1：十进制2转化为二进制为00000000 00000000 00000000 00000010,再将二进制右移 1 位，高位补0，低位丢弃，所以结果为00000000 00000000 00000000 00000001，换算成十进制为1 对于这两种情况都非常好理解，那么什么是无符号右移，以及负数是怎么运算的？我们先来看-2 &lt;&lt; 1 和 -2 &gt;&gt; 1，这两个负数的左移与右移操作其实和正数类似，都是先将十进制数转换成二进制数，再将二进制数进行移动，所以现在的关键是负数如何用二进制数进行表示。 2. 原码、反码、补码 接下来我们主要介绍十进制数用二进制表示的不同方法，所以为了简洁，我们用一个字节，也就是8个bit来表示二进制数。 2.1 原码 十进制 原码 2 0000 0010 -2 1000 0010 原码其实是最容易理解的，只不过需要利用二进制中的第一位来表示符号位，0表示正数，1表示负数，所以可以看到，一个数字用二进制原码表示的话，取值范围是-111 1111 ~ +111 1111， 换成十进制就是 -127 ~ 127 2.2 反码 在数学中我们有加减乘除，而对于计算机来说最好只有加法，这样计算机会更加简单高效，我们知道在数学中5-3=2，可以使用加法表示：5+(-3)=2，而乘法是加法的累积，除法是减法的累积，所以在计算机中只要有加法就够。 一个数字用原码表示是容易理解的，但是需要单独的一个bit来表示符号位。并且在进行加法时，计算机需要先识别某个二进制原码是正数还是负数，识别出来之后再进行相应的运算。这样效率不高，能不能让计算机在进行运算时不用去管符号位，也就是说让符号位也参与运算，这就要用到反码。 十进制 原码 反码 2 0000 0010 0000 0010 -2 1000 0010 1111 1101 正数的反码和原码一样，负数的反码就是在原码的基础上符号位保持不变，其他位取反。 那么我们来看下，用反码直接运算会怎么样，以5-3为例。 5 - 3等于5 + （-3） 十进制 原码 反码 5 0000 0101 0000 0101 -3 1000 0011 1111 1100 1234565-3=5 + (-3)= 0000 0101(反码) + 1111 1100 (反码)= 0000 0001 (反码)= 0000 0001 （原码）= 1 结果算出来是1，不合理啊，差了1。 接着看一个特殊的运算： 1234561-1=1 + (-1)= 0000 0001(反码) + 1111 1110 (反码)= 1111 1111 (反码)= 1000 0000 （原码）= -0 再看一个特殊的运算： 123450+0= 0000 0000(反码) + 0000 0000 (反码)= 0000 0000 (反码)= 0000 0000 （原码）= 0 我们可以看到1000 0000表示-0，0000 0000表示0，虽然-0和0是一样的，但是在用原码和反码表示时是不同的，我们可以理解为在用一个字节表示数字取值范围时，这些数字中多了一个-0，所以导致我们在用反码直接运算时符号位可以直接参加运算，但是结果会不对。 2.3 补码 为了解决反码的问题就出现来补码 十进制 原码 反码 补码 2 0000 0010 0000 0010 0000 0010 -2 1000 0010 1111 1101 1111 1110 正数的补码和原码、反码一样，负数的补码就是反码+1（满2进1）。 十进制 原码 反码 补码 5 0000 0101 0000 0101 0000 0101 -3 1000 0011 1111 1100 1111 1101 1234565-3=5 + (-3)= 0000 0101(补码) + 1111 1101 (补码)= 0000 0010 (补码)= 0000 0010 （原码）= 2 5 - 3= 2 ! 正确 再来看特殊的 1234561-1=1 + (-1)= 0000 0001(补码) + 1111 1111 (补码)= 0000 0000 (补码)= 0000 0000 （原码）= 0 1 - 1 = 0 ! 正确 123450+0= 0000 0000(补码) + 0000 0000 (补码)= 0000 0000 (补码)= 0000 0000 （原码）= 0 0 + 0 = 0 ! 正确 所以，我们可以看到补码解决来反码的问题，对于数字，我们可以使用补码的形式进行二进制表示。 3. 负数位移运算 那么接着来看 -2 &lt;&lt; 1 与 -2 &gt;&gt; 1 其中十进制-2原码、反码、补码分别为： 原码：1000000 00000000 00000000 00000010 反码：1111111 11111111 11111111 11111101 补码：1111111 11111111 11111111 11111110 那么-2 &lt;&lt; 1表示 -2的补码左移一位后结果为:1111111 11111111 11111111 11111100 该补码对应的反码为 123451111111 11111111 11111111 11111100-1= 1111111 11111111 11111111 11111011对应的原码为：1000000 00000000 00000000 00000100， 转化为十进制为： -4 所以 -2 &lt;&lt; 1 = -4， 同理-2 &gt;&gt; 1也是一样的计算方法。 4. 无符号右移 上面在进行左移和右移时，我有一点没讲到，就是在对补码进行移动时，符号位是固定不动的，而无符号右移是指在进行移动时，符号位也会跟着一起移动。 比如-2 &gt;&gt;&gt; 1: -2原码：1000000 00000000 00000000 00000010 -2反码：1111111 11111111 11111111 11111101 -2补码：1111111 11111111 11111111 11111110 右移后的补码对应的反码、原码为：0111111 11111111 11111111 11111111 (因为现在的符号位是0表示正数，正数的补、原、反码都是一样的) 所以对应的十进制为2147483647，也就是 -2 &gt;&gt;&gt; 1 = 2147483647 5. 总结 相信看完上面写的小伙伴们，都以发现： 2 &lt;&lt; 1 = 4 = 2 * 21 2 &lt;&lt; 2 = 8 = 2 * 22 2 &lt;&lt; n = 2 * 2n m &lt;&lt; n = m * 2n 右移则相反，所以大家以后在源码中再看到位运算时，可以参考上面的公式。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"线程池底层源码分析","slug":"thread-pool-source","date":"2022-12-06T13:56:29.000Z","updated":"2022-12-06T13:56:29.000Z","comments":false,"path":"thread-pool-source/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/thread-pool-source/index.html","excerpt":"","text":"1. 线程池创建 先使用ThreadPoolExecutor手动创建一个线程池 根据阿里巴巴Java开发手册里面的要求，线程池不允许使用Executors创建，而是通过ThreadPoolExecutor的方式 这样的处理方式更加明确线程池的运行规则，规避资源耗尽的风险 说明：Executors返回的线程池对象的弊端如下： FixedThreadPool和SingleThreadPool: 允许的请求队列长度为Integer.MAX_VALUE,可能会堆积大量请求，从而导致OOM CachedThreadPool和 ScheduledThreadPool: 允许的创建线程数长度为Integer.MAX_VALUE,可能会创建大量线程，从而导致OOM 1234567ThreadPoolExecutor executor = new ThreadPoolExecutor(2, 5, 3, TimeUnit.SECONDS, new LinkedBlockingQueue&lt;&gt;(3), Executors.defaultThreadFactory(), new ThreadPoolExecutor.DiscardOldestPolicy()); ThreadPoolExecutor七大参数： corePoolSize:核心线程池大小 maximumPoolSize:最大线程池大小 keepAliveTime:空闲线程存活时间 unit:时间单位 workQueue:阻塞队列 threadFactory:线程工厂：创建线程的，一般不用动 handler:拒绝策略 new ThreadPoolExecutor.AbortPolicy() // 不执行新任务，直接抛出异常，提示线程池已满 new ThreadPoolExecutor.CallerRunsPolicy() // 哪来的去哪里！由调用线程处理该任务 new ThreadPoolExecutor.DiscardPolicy() //不执行新任务，也不抛出异常 new ThreadPoolExecutor.DiscardOldestPolicy() //丢弃队列最前面的任务，然后重新提交被拒绝的任务。 ThreadPoolExecutor构造方法源码 123456789101112131415161718192021222324public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.acc = System.getSecurityManager() == null ? null : AccessController.getContext(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 2. 线程池的基础属性和方法 在线程池的源码中，会通过一个 AtomicInteger类型的变量 ctl 来表示线程池的状态和当前线程池中的工作线程数量。 1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); 一个Integer占4个字节，也就是 32 个bit,线程池有5个状态： RUNNING SHUTDOWN STOP TIDYING TERMINATED 12345678910111213141516171819202122232425262728 源码关于5中状态的说明：/* ******* The runState provides the main lifecycle control, taking on values:** RUNNING: Accept new tasks and process queued tasks* SHUTDOWN: Don&#x27;t accept new tasks, but process queued tasks* STOP: Don&#x27;t accept new tasks, don&#x27;t process queued tasks,* and interrupt in-progress tasks* TIDYING: All tasks have terminated, workerCount is zero,* the thread transitioning to state TIDYING* will run the terminated() hook method* TERMINATED: terminated() has completed** The numerical order among these values matters, to allow* ordered comparisons. The runState monotonically increases over* time, but need not hit each state. The transitions are:** RUNNING -&gt; SHUTDOWN* On invocation of shutdown(), perhaps implicitly in finalize()* (RUNNING or SHUTDOWN) -&gt; STOP* On invocation of shutdownNow()* SHUTDOWN -&gt; TIDYING* When both queue and pool are empty* STOP -&gt; TIDYING* When pool is empty* TIDYING -&gt; TERMINATED* When the terminated() hook method has completed*/ 2个 bit 能表示4种状态, 那5种状态就至少需要3个bit位，比如在线程池的源码中就是这么表示的 12345678private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; Integer.SIZE 为 32， 所以 COUNT_BITS = 29， 最终各个状态对应的二进制为： RUNNING: 11100000 00000000 00000000 00000000 SHUTDOWN: 00000000 00000000 00000000 00000000 STOP: 00100000 00000000 00000000 00000000 TIDYING: 01000000 00000000 00000000 00000000 TERMINATED: 01100000 00000000 00000000 00000000 所以，只需要使用一个Integer数字的最高3个bit，就可以表示5种线程池状态，而剩下的29个bit就可以用来表示工作线程数， 比如：假设ctl为：11100000 00000000 00000000 00001010 就表示线程池的状态为RUNNING，线程池池目前在工作的线程有10个， 这里说的&quot;在工作&quot;指的是线程活着，要么在执行任务，要么在阻塞等待任务。 同时，线程池中也提供一些方法来获取线程池状态和工作线程数，比如： 1234567891011121314151617181920private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));//29 二进制为：00000000 00000000 00000000 00011101private static final int COUNT_BITS = Integer.SIZE - 3;//二进制为：00011111 11111111 11111111 11111111private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// Packing and unpacking ctl//～CAPACITY： 11100000 00000000 00000000 00000000// &amp; 操作之后， 得到的就是 c 的高3位private static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;//CAPACITY： 00011111 11111111 11111111 11111111// &amp; 操作之后， 得到的就是 c 的低29位private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;/** * 把运行状态和线程数量进行合并，传入的两个int 数字有限制， rs的低29位都必须是9 * wc的高3位都必须位0，这样经过或运算之后，才能得到准确的 ctl */private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; execute方法 当执行线程池的execute方法时 123456789101112131415161718192021222324252627282930313233343536public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取ctl, ctl初始值时 ctlOf(RUNNING, 0), 表示线程池处于运行中，工作线程数位0 int c = ctl.get(); //工作线程数于 corePoolSize， 则添加工作线程数，并把command作为该线程要执行的任务 if (workerCountOf(c) &lt; corePoolSize) &#123; //true 表示添加的时核心工作线程，在addWorker内部会判断当前工作线程数是不是超过了corePoolSize //如果超过了就会添加失败，addWorker返回false，表示不能开启新的线程来执行任务，而是应该先 入队 if (addWorker(command, true)) return; //如果添加核心工作线程失败，要重新获取ctl, 可能是线程池状态被其他线程修改了 // 也可能是其他线程也在向线程池提交任务，导致核心工作线程已经超出了corePoolSize c = ctl.get(); &#125; // 线程池状态是否还是RUNNING，如果是就把任务添加到阻塞队列中 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //在任务入队时，线程池的状态也可能发生改变 int recheck = ctl.get(); // 再次检查线程池的状态，如果不是RUNNING，就不能再接受任务了，就把任务从队列中移除，并进行拒绝策略 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) //不过为了确保刚刚入队的任务有线程回去处理它，需要判断以下工作线程数，如果位0，则添加一个非核心的工作线程 addWorker(null, false); &#125; //如果线程池状态不是RUNNING，或者线程池状态时RUNNING但是队列满了，则去添加一个非核心工作线程 //实际上，addWorker中会判断线程池状态不是RUNNING，是不会添加工作线程的 //false表示非核心工作线程，作用是，在addWorker内部会判断当前工作线程数已经超过了maximumPoolSize //如果超过了则会添加不成功，执行拒绝策略 else if (!addWorker(command, false)) reject(command); &#125; addWorker方法 addWorker方法是核心方法，用于添加线程的， core参数表示条件的是核心线程还是非核心线程 添加线程：实际上就是开启一个线程，不管是核心线程还是非核心线程都只是一个普通的线程，而核心和非核心的区别在于： 如果要添加核心工作线程，那么就要判断目前的工作线程数是否超过corePoolSize a. 如果没超过，则直接开启新的工作线程执行任务 b. 如果超过了，则不会开启新的工作线程，而是把任务进行入队 如果添加非核心线程，那么就要判断目前的工作线程数是否超过maximumPoolSize a. 如果没超过，则直接开启新的工作线程执行任务 b. 如果超过了，则拒绝执行任务 所以在addWorker方法中，首先就要判断工作线程有没有超过限制，如果没有超过限制再去开启一个线程。 并且在addWorker方法中，还得判断线程池的状态，如果线程池的状态不是RUNNING状态了，那就没必要要去添加线程了，当然有一种特例，就是线程池的状态是SHUTDOWN，但是队列中有任务，那此时还是需要添加添加一个线程的。 我们前面提到的都是开启新的工作线程，那么工作线程怎么回收呢？不可能开启的工作线程一直活着，因为如果任务由多变少，那也就不需要过多的线程资源，所以线程池中会有机制对开启的工作线程进行回收，如何回收的，后文会提到，我们这里先分析，有没有可能线程池中所有的线程都被回收了，答案的是有的。 首先非核心工作线程被回收是可以理解的，那核心工作线程要不要回收掉呢？其实线程池存在的意义，就是提交生成好线程资源，需要线程的时候直接使用就可以，而不需要临时去开启线程，所以正常情况下，开启的核心工作线程是不用回收掉的，就算暂时没有任务要处理，也不用回收，就让核心工作线程在那等着就可以了。 但是，在线程池中有这么一个参数：allowCoreThreadTimeOut表示是否允许核心工作线程超时，意思就是**是否允许核心工作线程回收。**默认这个参数为false，但是我们可以调用allowCoreThreadTimeOut(boolean value)来把这个参数改为true，只要改了，那么核心工作线程也就会被回收了，那这样线程池中的所有工作线程都可能被回收掉，那如果所有工作线程都被回收掉之后，阻塞队列中来了一个任务，这样就形成了特例情况。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); //线程池状态不是RUNNING，并且不是特例情况（线程池状态是SHUTDOWN并且队列不为空） //如果是RUNNING或者是特例情况，就准备新建工作线程 // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //判断工作线程数是否超过了限制 //如果超过了，则return false if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) //如果没有超过限制，则修改ctl，增加工作线程数，cas成功则退出外层retry循环，去创建新的工作线程 break retry; //如果cas失败，则表示有其他线程也在提交任务，也在增加工作线程数，此时重新获取ctl c = ctl.get(); // Re-read ctl //如果发现线程池的状态发生了变化，则继续回到retry，重新判断线程池的状态是否是RUNNING //如果没有发生变化，则继续利用CAS来增加工作线程数，直到cas成功 if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; //ctl 修改成功，也就是工作线程数 +1 成功 //接下来就要开启一个新的工作线程了 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //Workker实现了Runnable接口，构造一个Worker对象时，就会利用ThreadFactory新建一个线程 //Worker对象有两个属性： //Runnable firstTask: 表示worker待执行的第一个任务，第二个任务会从阻塞队列中获取 // Thread thread： 表示Worker对应的线程，就是这个线程来获取并执行任务的 w = new Worker(firstTask); //拿出线程对象，还没有start final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); // 如果线程池的状态是RUNNING //或者线程池的状态变成了SHUTDOWN，但是当前线程没有自己的第一个任务，那就表示当前调用addWorker方法是为了从队列中获取任务来执行 // 正常情况下线程池的状态如果是SHUTDOWN，是不能创建新的工作线程的，但是队列中如果有任务，那就是上面说的特例情况 if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); // workers用来记录当前线程池中工作线程 workers.add(w); //largestPoolSize 用来跟踪线程池在运行中工作线程数据的峰值 int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; //运行线程 if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; //在上述过程中如果抛出了异常，需要从workers中移除添加的work,并且还要修改ctl , 工作线程数据 -1，表示新建工作线程失败 if (! workerStarted) addWorkerFailed(w); &#125; //表示添加工作线程成功 return workerStarted; &#125; 所以，对于addWorker方法，核心逻辑就是： 先判断工作线程数是否超过了限制 修改ctl，使工作线程数 +1 构造Work对象，并把它添加到workers集合中 启动Work对象对应的工作线程","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"}]},{"title":"Mybatis批量插入的5种方式","slug":"mybatis-batch-insert","date":"2022-12-04T12:52:30.000Z","updated":"2022-12-04T12:52:30.000Z","comments":false,"path":"mybatis-batch-insert/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mybatis-batch-insert/index.html","excerpt":"","text":"1. 目录 1. 准备工作 2. MyBatis利用For循环批量插入 3. MyBatis的手动批量提交 4. MyBatis以集合方式批量新增（推荐） 5. MyBatis-Plus提供的SaveBatch方法 6. MyBatis-Plus提供的InsertBatchSomeColumn方法（推荐） 1. 准备工作 导入pom.xml依赖 12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--Mybatis依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.2.2&lt;/version&gt;&lt;/dependency&gt;&lt;!--Mybatis-Plus依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.baomidou&lt;/groupId&gt; &lt;artifactId&gt;mybatis-plus-boot-starter&lt;/artifactId&gt; &lt;version&gt;3.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt; 配置yml文件 123456789101112server: port: 8080 spring: datasource: username: mysql用户名 password: mysql密码 url: jdbc:mysql://localhost:3306/数据库名字?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&amp;serverTimezone=UTC driver-class-name: com.mysql.cj.jdbc.Driver mybatis: mapper-locations: classpath:mapping/*.xml 公用的User类 1234567@Datapublic class User &#123; private int id; private String username; private String password;&#125; 2. MyBatis利用For循环批量插入 编写UserService服务类，测试插入一万条数据耗时情况 12345678910111213141516171819@Servicepublic class UserService &#123; @Resource private UserMapper userMapper; public void InsertUsers()&#123; long start = System.currentTimeMillis(); for(int i = 0 ;i &lt; 10000; i++) &#123; User user = new User(); user.setUsername(&quot;name&quot; + i); user.setPassword(&quot;password&quot; + i); userMapper.insertUsers(user); &#125; long end = System.currentTimeMillis(); System.out.println(&quot;一万条数据总耗时：&quot; + (end-start) + &quot;ms&quot; ); &#125; &#125; 编写UserMapper接口以及xml文件 1234@Mapperpublic interface UserMapper &#123; Integer insertUsers(User user);&#125; 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.ithuang.demo.mapper.UserMapper&quot;&gt; &lt;insert id=&quot;insertUsers&quot;&gt; INSERT INTO user (username, password) VALUES(#&#123;username&#125;, #&#123;password&#125;) &lt;/insert&gt;&lt;/mapper&gt; 进行单元测试 1234567891011@SpringBootTestclass DemoApplicationTests &#123; @Resource private UserService userService; @Test public void insert()&#123; userService.InsertUsers(); &#125;&#125; 结果输出 一万条数据总耗时：26348ms 3. MyBatis的手动批量提交 其他保持不变，Service层修改 12345678910111213141516171819202122232425@Servicepublic class UserService &#123; @Resource private UserMapper userMapper; @Resource private SqlSessionTemplate sqlSessionTemplate; public void InsertUsers()&#123; //关闭自动提交 SqlSession sqlSession = sqlSessionTemplate.getSqlSessionFactory().openSession(ExecutorType.BATCH, false); UserMapper userMapper = sqlSession.getMapper(UserMapper.class); long start = System.currentTimeMillis(); for(int i = 0 ;i &lt; 10000; i++) &#123; User user = new User(); user.setUsername(&quot;name&quot; + i); user.setPassword(&quot;password&quot; + i); userMapper.insertUsers(user); &#125; sqlSession.commit(); long end = System.currentTimeMillis(); System.out.println(&quot;一万条数据总耗时：&quot; + (end-start) + &quot;ms&quot; ); &#125;&#125; 结果输出 一万条数据总耗时：24516ms 4. MyBatis以集合方式批量新增（推荐） 编写UserService服务类 12345678910111213141516171819202122@Servicepublic class UserService &#123; @Resource private UserMapper userMapper; public void InsertUsers()&#123; long start = System.currentTimeMillis(); List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user; for(int i = 0 ;i &lt; 10000; i++) &#123; user = new User(); user.setUsername(&quot;name&quot; + i); user.setPassword(&quot;password&quot; + i); userList.add(user); &#125; userMapper.insertUsers(userList); long end = System.currentTimeMillis(); System.out.println(&quot;一万条数据总耗时：&quot; + (end-start) + &quot;ms&quot; ); &#125; &#125; 编写UserMapper接口以及xml文件 1234@Mapperpublic interface UserMapper &#123; Integer insertUsers(List&lt;User&gt; userList);&#125; 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; ?&gt;&lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;&gt;&lt;mapper namespace=&quot;com.ithuang.demo.mapper.UserMapper&quot;&gt; &lt;insert id=&quot;insertUsers&quot;&gt; INSERT INTO user (username, password) VALUES &lt;foreach collection =&quot;userList&quot; item=&quot;user&quot; separator =&quot;,&quot;&gt; (#&#123;user.username&#125;, #&#123;user.password&#125;) &lt;/foreach&gt; &lt;/insert&gt;&lt;/mapper&gt; 输出结果 一万条数据总耗时：521ms 5.MyBatis-Plus提供的SaveBatch方法 编写UserService服务 123456789101112131415161718@Servicepublic class UserService extends ServiceImpl&lt;UserMapper, User&gt; implements IService&lt;User&gt; &#123; public void InsertUsers()&#123; long start = System.currentTimeMillis(); List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user; for(int i = 0 ;i &lt; 10000; i++) &#123; user = new User(); user.setUsername(&quot;name&quot; + i); user.setPassword(&quot;password&quot; + i); userList.add(user); &#125; saveBatch(userList); long end = System.currentTimeMillis(); System.out.println(&quot;一万条数据总耗时：&quot; + (end-start) + &quot;ms&quot; ); &#125;&#125; 编写UserMapper接口 1234@Mapperpublic interface UserMapper extends BaseMapper&lt;User&gt; &#123; &#125; 测试结果 一万条数据总耗时：24674ms 6.MyBatis-Plus提供的InsertBatchSomeColumn方法（推荐） 编写EasySqlInjector自定义类 1234567891011public class EasySqlInjector extends DefaultSqlInjector &#123; @Override public List&lt;AbstractMethod&gt; getMethodList(Class&lt;?&gt; mapperClass, TableInfo tableInfo) &#123; // 注意：此SQL注入器继承了DefaultSqlInjector(默认注入器)，调用了DefaultSqlInjector的getMethodList方法，保留了mybatis-plus的自带方法 List&lt;AbstractMethod&gt; methodList = super.getMethodList(mapperClass, tableInfo); methodList.add(new InsertBatchSomeColumn(i -&gt; i.getFieldFill() != FieldFill.UPDATE)); return methodList; &#125;&#125; 定义核心配置类注入Bean 12345678@Configurationpublic class MybatisPlusConfig &#123; @Bean public EasySqlInjector sqlInjector() &#123; return new EasySqlInjector(); &#125;&#125; 编写UserService服务类 12345678910111213141516171819public class UserService&#123; @Resource private UserMapper userMapper; public void InsertUsers()&#123; long start = System.currentTimeMillis(); List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user; for(int i = 0 ;i &lt; 10000; i++) &#123; user = new User(); user.setUsername(&quot;name&quot; + i); user.setPassword(&quot;password&quot; + i); userList.add(user); &#125; userMapper.insertBatchSomeColumn(userList); long end = System.currentTimeMillis(); System.out.println(&quot;一万条数据总耗时：&quot; + (end-start) + &quot;ms&quot; ); &#125;&#125; 编写EasyBaseMapper接口 123456789public interface EasyBaseMapper&lt;T&gt; extends BaseMapper&lt;T&gt; &#123; /** * 批量插入 仅适用于mysql * * @param entityList 实体列表 * @return 影响行数 */ Integer insertBatchSomeColumn(Collection&lt;T&gt; entityList);&#125; 编写UserMapper接口 1234@Mapperpublic interface UserMapper&lt;T&gt; extends EasyBaseMapper&lt;User&gt; &#123; &#125; 测试结果 一万条数据总耗时：575ms","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"String.intern()使用总结","slug":"intern","date":"2022-12-02T13:14:18.000Z","updated":"2022-12-02T13:14:18.000Z","comments":false,"path":"intern/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/intern/index.html","excerpt":"","text":"1. 知识点一 123String s = new String(&quot;1&quot;);String s1 = s.intern();System.out.println(s == s1); //输出结果为： false 对于new String(&quot;1&quot;)会生成两个对象，一个是String类型的对象，它将存储在Java Heap中，另一个字符串常量对象1，它将存储在字符串常量池中、。 s.inertn()方法会先去字符串常量池中查找是否存在字符串常量1，如果存在则返回该对象的地址，如果不存在则在字符串常量池中生成一个1字符串常量，并返回该对象的地址。 如下图： 变量s指向的是String类型对象，变量s1对象指向的是&quot;1&quot;字符串常量对象，所以 s ==s1结果为false。 2. 知识点二 在上面的基础上再定一个s2，如下： 12345String s = new String(&quot;1&quot;);String s1 = s.intern();String s2 = &quot;1&quot;;System.out.println(s == s1); //falseSystem.out.println(s1 == s2); //true s1 == s2 为true，表示变量s2是直接指向的字符串常量，如下图所示： 3. 知识点三 1234String s = new String(&quot;1&quot;);String t = new String(&quot;1&quot;);System.out.println(s == t); //falseSystem.out.println(s.intern() == t.intern()); //true s == t为false，这个很明显，变量s 和变量t 指向的是不同的两个String类型的对象。 s.intern() == t.intern()为true，因为intern()方法返回的是字符串常量池中的同一个对象1，所以为true 4. 知识点四 1234String x = new String(&quot;1&quot;) + new String(&quot;1&quot;);String s3 = &quot;11&quot;;System.out.println(x == s3); //falseSystem.out.println(x.intern() == s3.intern()); //true 变量x为两个String类型对象相加，那x依旧还是一个String类型对象，所以x != s3；调用x.intern()方法将返回11对应的字符串常量，所以x.intern() == s3.intern()为true 5. 知识点五 12345678String x = new String(&quot;1&quot;) + new String(&quot;1&quot;);String x1 = &quot;1&quot; + new String(&quot;1&quot;);String x2 = &quot;1&quot; + &quot;1&quot;;String s3 = &quot;11&quot;;System.out.println(x == s3); //falseSystem.out.println(x1 == s3); //falseSystem.out.println(x2 == s3); //true x == s3为false，表示x指向String类型对象， s3指向字符串常量； x1 == s3为false，表示x1指向String类型对象， s3指向字符串常量； x2 == s3为true，表示x2指向字符串常量， s3指向字符串常量； 所以可以看到new String(&quot;1&quot;) + &quot;1&quot;返回的是String类型的对象 6. 总结 现在我们知道intern()方法就是将字符串保存到常量池中，在保存字符串到常量池的过程中会先查看常量池中是否已经存在相等的字符串，如果存在则直接使用该字符串。 所以我们在写业务代码的时候，应该尽量使用字符串常量中的字符串，比如使用String s = &quot;1&quot; 比使用 new String(&quot;1&quot;)更节省内存； 我们也可以使用String s = String类型对象.intern()方法来间接使用字符串常量。 这种做法通常用在你接收到一个String类型的对象而又想节省内存的情况下，当然你完全可以 String s = String类型对象，但是这么用可能会因为变量s的引用而影响String类型对象的垃圾回收，所以我们可以使用intern方法进行优化，但是需要注意的是intern能节省 内存，但是会影响运行速度，因为该方法需要求常量池中查询是否存在某个字符串。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"13种锁的实现方式","slug":"lock-granularity","date":"2022-11-30T14:56:19.000Z","updated":"2022-11-30T14:56:19.000Z","comments":false,"path":"lock-granularity/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/lock-granularity/index.html","excerpt":"","text":"1. 悲观锁 它是指对数据修改时持保守态度，认为其他人也会修改数据。因此在操作数据时，会把数据锁住，直到操作完成。悲观锁大多数情况下依靠数据库的锁机制实现，以保证操作最大程度的独占性。如果加锁的时间过长，其他用户长时间无法访问，影响程序的并发访问性，同时这样对数据库性能开销影响也很大，特别是长事务而言，这样的开销往往无法承受 如果时单机系统，我们可以采用synchronized关键字，添加到方法或者同步代码块上锁住资源，如果时分布式系统，可以借助数据库自身的锁机制来实现 1select * from 表名 where id= #&#123;id&#125; for update 使用悲观锁的时候，我们要注意锁的级别，MySQL innodb 在加锁时，只有明确的指定主键或（索引字段）才会使用行锁；否则，会执行表锁，将整个表锁住，此时性能会很差。在使用悲观锁时，我们必须关闭 MySQL 数据库的自动提交属性，因为mysql默认使用自动提交模式。 悲观锁适用于写多的场景，而且并发性能要求不高 2. 乐观锁 在操作数据时非常乐观，认为别人不会同时修改数据，因此乐观锁不会上锁 只是在 提交更新 时，才会正式对数据的冲突与否进行检测。如果发现冲突了，则返回错误信息，让用户决定如何去做，fail-fast 机制 。否则，执行本次操作。 分为三个阶段：数据读取、写入校验、数据写入 单机系统：可以基于Java的CAS来实现，它是一种原子操作，借助硬件的比较并交换来实现 分布式系统: 可以在数据库增加一个版本号version字段 123update 表set ... , version = version +1where id= #&#123;id&#125; and version = #&#123;version&#125; 操作前，先读入记录的版本号，更新时，比较版本号是否一致，如果一致则更新数据，否则再次读取版本号，重复上面的操作。 3. 分布式锁 Java中的 synchronized、ReentrantLock等，都是解决单体应用单机部署的资源互斥问题；当单体应用演化为分布式集群后，多线程、多进程分布在不同的机器上，原来的并发控制策略实效 此时，我们需要引入分布式锁，解决跨机器的互斥机制来控制共享资源的访问 分布式锁需要具备以下条件： 与单机系统一样的资源互斥功能 高性能获取、释放锁 高可用 可重入性 有锁实效机制，防止死锁 非阻塞，不管是否获得锁，要能快速返回 实现的方式有多种，基于数据库、Redis、以及Zookeeper等；这里主要讲下主路的基于Redis的实现方式： 加锁 1SET key unique_value [EX seconds] [PX milliseconds] [NX|XX] 通过原子命令，如果执行成功返回1，表示加锁成功哦难过，注意：unique_value是客户端生成的唯一标识，区分来自不同客户端的锁操作， 解锁要注意，先判断unique_value是不是加锁的客户端，是的化才允许解锁； 解锁：有两个命令操作，需要借助Lua脚本来保证原子性 123456// 先比较 unique_value 是否相等，避免锁的误释放if redis.call(&quot;get&quot;,KEYS[1]) == ARGV[1] then return redis.call(&quot;del&quot;,KEYS[1])else return 0end 借助 Redis 的高性能，Redis 实现分布式锁也是目前主流实现方式。但任何事情有利有弊，如果加锁的服务器宕机了，当slave 节点还没来得及数据备份，那不是别的客户端也可以获得锁。 为了解决这个问题，Redis 官方设计了一个分布式锁 Redlock。 基本思路：让客户端与多个独立的 Redis 节点并行请求申请加锁，如果能在半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。 4. 可重入锁 可重入锁，也较递归锁，是指在同一个线程在掉外层方法获取锁的时候，在进入内层方法会自动获取锁 对象锁或类锁内部有计数器，一个线程每获得一次锁，计数器 +1；解锁时，计数器 -1。 有多少次加锁，就要对应多少次解锁，加锁与解锁成对出现。 JAVA 中的 ReentrantLock 和 synchronized 都是 可重入锁。可重入锁的一个好处是可一定程度避免死锁。 5. 自旋锁 自旋锁是采用让当前线程不停地在循环体内执行，当循环的条件被其他线程改变时才进入临界区。自旋锁只是将当前线程不停地执行循环体， 不进行线程状态的改变，所以响应速度更快。但当线程数不断增加时，性能下降明显，因为每个新城都需要执行，占用CPU时间片，如果线程竞争不激烈，并且保持锁的时间短，适合使用自旋锁 自旋锁缺点： 可能引发死锁 可能占用CPU时间过长 可以设置一个循环时间或循环次数，超出阈值时，让线程进入阻塞状态，防止长时间占用CPU资源，JUC并发包中的CAS就是采用自旋锁，compareAndSet是CAS操作的核心，底层采用Unsafe对象实现的 1234567public final int getAndAddInt(Object var1, long var2, int var4) &#123; int var5; do &#123; var5 = this.getIntVolatile(var1, var2); &#125; while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;&#125; 如果内存中 var1 对象的var2字段值等于预期的 var5，则将该位置更新为新值（var5 + var4），否则不进行任何操作，一直重试，直到操作成功为止。 CAS包含了Compare和Swap两个操作，CAS是由CPU支持的原子操作，其原子性在硬件层面进行控制 特别注意，CAS可能会导致ABA问题，我们可以引入递增版本号来解决 6. 独享锁/排他锁 独享锁也叫排他锁，无论是读操作还是写操作，只能有一个线程获得锁，其他线程处于阻塞状态 缺点：读操作不会修改数据，而且大部份的系统都是读多写少，如果读读之间互斥，大大降低系统的性能 像Java中的Reentrantlock、Synchronized都是独享锁 7. 共享锁 共享锁是指允许多个线程同时持有锁，一般在读锁上。读锁的共享锁可保证并发读是非常搞笑的，读写，写读，写写之间则是互斥的。 独享锁和共享锁是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享 读写锁ReentrantReadWriteLock 中读锁是共享锁，写锁是独享锁 8. 读锁/写锁 如果对某个资源是读操作，那多个线程之间并不会相互影响，可以通过添加读锁实现共享，如果有修改操作，为了保证数据的并发安全 ， 此时只能有一个线程获得锁我们称之为写锁。 读读是共享的，读写、写读 、写写 则是互斥的 9. 公平锁/非公平锁 公平锁：多个线程按照申请锁的顺序去多额锁，所有线程都在队列中排队，先来先获取的公平性原则 优点： 所有的线程都能得到资源，不会饿死在队列中 缺点：吞吐量会下降很多，队列里面处理第一个线程，其他的线程都会阻塞，CPU唤醒下一个阻塞线程有系统开销 非公平锁：多个线程不按照申请锁的顺序去获得锁，而是同时以插队方式直接尝试获取锁，获取不到（插队失败），会进入队列等待（失败则乖乖排队），如果能获取到（插队成功），就直接获取到锁。 优点：可以减少CPU唤醒线程的开销，整体的吞吐效率会高点 缺点：可能导致线程中排队的线程一直获取不到锁或者长时间获取不到锁 Java多线程并发操作，大多是基于Sync本身去实现的，而sync本身却是ReentrantLock的一个内部类，sync继承AbstractQueuedSynchronizer， 像ReentrantLock默认是非公平锁，我们可以在构造函数中传入true来创建公平锁 123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 10. 可中断锁/不可中断锁 可中断锁：指一个线程因为没有获得锁在阻塞等待过程中，可以中断自己阻塞的状态 不可中断锁：如果锁被其他线程获取后，当前线程只能阻塞等待，入股持有锁的线程一直不释放锁，那其他想获取锁的线程就会一直阻塞 内置锁 synchronized 是不可中断锁，而 ReentrantLock 是可中断锁。 ReentrantLock获取锁定有三种方式： lock()， 如果获取了锁立即返回，如果别的线程持有锁，当前线程则一直处于阻塞状态，直到该线程获取锁 tryLock()， 如果获取了锁立即返回true，如果别的线程正持有锁，立即返回false tryLock(long timeout,TimeUnit unit)， 如果获取了锁定立即返回true，如果别的线程正持有锁，会等待参数给定的时间，在等待的过程中，如果获取了锁定，就返回true，如果等待超时，返回false； lockInterruptibly()，如果获取了锁定立即返回；如果没有获取锁，线程处于阻塞状态，直到获取锁或者线程被别的线程中断 11. 分段锁 分段锁其实是一种锁的设计，目的是细化锁的粒度，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的失信就是通过分段锁的形式来实现搞笑的并发操作。 ConcurrentHashMap中的分段锁称为segment，历史与Hashmap（Jdk7）的结构，即内部拥有一个Entry数组，数组中的每一个元素又是一个链表，同时又是一个ReentrantLock（Segment继承来ReentrantLock. 当需要put元素的时候，并不是对整个HashMap加锁，而是先通过hashcode知道要放在哪一个分段中，然后对这个分段加锁，所以当多线程put时，只要不是放在同一个分段中，可支持并行插入。 12. 锁升级（无锁|偏向锁|轻量级锁|重量级锁） JDK 1.6之前，synchronized 还是一个重量级锁，效率比较低。但是在JDK 1.6后，JVM为了提高锁的获取与释放效率对 synchronized 进行了优化，引入了偏向锁和轻量级锁 ，从此以后锁的状态就有了四种：无锁、偏向锁、轻量级锁、重量级锁。这四种状态会随着竞争的情况逐渐升级，而且是不可降级。 无锁 无锁并不会对资源锁定，所有的线程都可以访问并修改同一个资源，但同时只有一个线程能修改成功。也就是我们常说的乐观锁。 偏向锁 偏向于第一个访问锁的线程，初次执行synchronized代码块时，通过 CAS 修改对象头里的锁标志位，锁对象变成偏向锁。 当一个线程访问同步代码块并获取锁时，会在 Mark Word 里存储锁偏向的线程 ID。在线程进入和退出同步块时不再通过 CAS 操作来加锁和解锁，而是检测 Mark Word 里是否存储着指向当前线程的偏向锁。轻量级锁的获取及释放依赖多次 CAS 原子指令，而偏向锁只需要在置换 ThreadID 的时候依赖一次 CAS 原子指令即可。 执行完同步代码块后，线程并不会主动释放偏向锁。当线程第二次再执行同步代码块时，线程会判断此时持有锁的线程是否就是自己（持有锁的线程ID也在对象头里），如果是则正常往下执行。由于之前没有释放锁，这里不需要重新加锁，偏向锁几乎没有额外开销，性能极高。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程是不会主动释放偏向锁的。关于偏向锁的撤销，需要等待全局安全点，即在某个时间点上没有字节码正在执行时，它会先暂停拥有偏向锁的线程，然后判断锁对象是否处于被锁定状态。如果线程不处于活动状态，则将对象头设置成无锁状态，并撤销偏向锁，恢复到无锁（标志位为01）或轻量级锁（标志位为00）的状态。 偏向锁是指当一段同步代码一直被同一个线程所访问时，即不存在多个线程的竞争时，那么该线程在后续访问时便会自动获得锁，从而降低获取锁带来的消耗。 轻量级锁 当前锁是偏向锁，此时有多个线程同时来竞争锁，偏向锁会升级为轻量级锁。轻量级锁认为虽然竞争是存在的，但是理想情况下竞争的程度很低，通过自旋方式来获取锁。 轻量级锁的获取有两种情况： 当关闭偏向锁功能 多个线程竞争偏向锁导致偏向锁升级为轻量级锁，一旦有第二个线程加入锁竞争，偏向锁就会升级为轻量级锁（自旋锁） 在轻量级锁状态下继续锁竞争，没有抢到锁的线程将自旋，不停地循环判断是否能够被成功获取。获取锁的操作，其实就是通过CAS修改对象里面的锁标识位，先比较当前锁标志位是否位&quot;释放&quot;，如果是则将其设置为&quot;锁定&quot;，此过程是原子性；如果抢到锁，线程将当前锁的持有者信息修改为自己。 重量级锁 如果线程的竞争很激励，线程的自旋超过了一定次数（默认循环10次，可以通过虚拟机参数更改），将轻量级锁升级为重量级锁（依然是 CAS 修改锁标志位，但不修改持有锁的线程ID），当后续线程尝试获取锁时，发现被占用的锁是重量级锁，则直接将自己挂起（而不是忙等），等待将来被唤醒。 重量级锁是指当有一个线程获取锁之后，其余所有等待获取该锁的线程都会处于阻塞状态。简言之，就是所有的控制权都交给了操作系统，由操作系统来负责线程间的调度和线程的状态变更。而这样会出现频繁地对线程运行状态的切换，线程的挂起和唤醒，从而消耗大量的系统资。 13. 锁优化（锁粗化、锁消除） 锁粗化 就是告诉我们任何事情都有个度，有些情况下我们反而希望把很多次锁的请求合并成一个请求，以降低短时间内大量锁请求、同步、释放带来的性能损耗。 举个例子：有个循环体 12345for(int i=0;i&lt;size;i++)&#123; synchronized(lock)&#123; ...业务处理，省略 &#125;&#125; 经过锁粗化的代码如下： 123456synchronized(lock)&#123; for(int i=0;i&lt;size;i++)&#123; ...业务处理，省略 &#125;&#125; 锁消除 指的是在某些情况下，JVM 虚拟机如果检测不到某段代码被共享和竞争的可能性，就会将这段代码所属的同步锁消除掉，从而到底提高程序性能的目的。 锁消除的依据是逃逸分析的数据支持，如 StringBuffer 的 append() 方法，或 Vector 的 add() 方法，在很多情况下是可以进行锁消除的，比如以下这段代码： 1234567public String method() &#123; StringBuffer sb = new StringBuffer(); for (int i = 0; i &lt; 10; i++) &#123; sb.append(&quot;i:&quot; + i); &#125; return sb.toString();&#125; 以上代码经过编译之后的字节码如下： 从上述结果可以看出，之前我们写的线程安全的加锁的StringBuffer对象，在生成字节码之后就被替换成了不加锁不安全的 StringBuilder 对象了，原因是 StringBuffer 的变量属于一个局部变量，并且不会从该方法中逃逸出去，所以我们可以使用锁消除（不加锁）来加速程序的运行。 博客摘录于Tom哥的聊聊13种锁的实现方式","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"接口设计看这一篇就够了","slug":"special-interface","date":"2022-11-23T12:58:03.000Z","updated":"2022-11-23T12:58:03.000Z","comments":false,"path":"special-interface/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/special-interface/index.html","excerpt":"","text":"目录 1. 接口幂等性 2. SpringBoot 防重Token令牌方案 3. 接口数据安全方案 4. 接口性能优化方案 5. 接口设计的锦囊 1. 接口幂等性 1.1 幂等性描述 幂等是一个数据和计算机学概念，在数学中某一元运算为幂等时，作用多次和作用一次的结果相同 在数学中，幂等用函数表达式就是：f(x) = f(f(x)) 1.2 接口幂等性 在HTTP/1.1中，对幂等性进行类定义，它描述一次和多次请求某个资源对资源本身应该具有同样的结果（网络超时等问题除外）， 即多次调用方法或者接口不会改变业务状态，可以保证重复调用的结果和单次调用的结果一致。 幂等性指的是作用于结果而非资源本身。例如，HTTP GET方法可能会每次得到不同的返回内容，但并不影响资源。 1.3 为什么需要实现幂等性 在接口调用时一般情况下都能正常返回信息不会出现重复提交，不过出现以下几种情况会有问题，如： 前端重复提交表单：比如用户注册时，因网络波动没有及时对用户做出提交成功响应，致使用户认为没有提交成功，然后多次进行提交操作，这时就会发生重复提交请求 用户恶意刷单：比如用户投票，如果用户针对一个内容重复提交投票，接口接收到用户重复提交的投票信息，影响实际的计算结果 接口超时重复提交：如果存在超时重试机制，尤其是第三方调用接口时，为了防止网络波动超时等造成的请求失败，都会添加重试机制，导致一个请求多次提交 消息进行重复消费：当使用MQ消息中间件时，如果发生消息中间件出现错误为即使提交消费消息，导致发生重复消费； 使用幂等性最大的优势在于使接口保证任何幂等性操作，避免因重试等造成系统未知问题。 1.4 幂等性对系统的影响 幂等性是为了简化客户端逻辑处理，能防止重复提交等操作，但也额外增加了服务端业务逻辑复杂性，主要是 把并行执行的功能改成了串行，降低了执行效率 增加了额外控制幂等的业务代码，使原本的业务功能复杂化 所以我们需要根据实际的业务场景来考虑是否引入幂等性 1.5 Restful API 接口的幂等性 现在流行的Restful 推荐的几种HTTP方法中幂等性如下： 类型 是否幂等 描述 HEAD 是 Head不含有呈现数据，仅时HTTP头信息，head方法常用来做探活使用 GET 是 Get 方法用于获取资源。其一般不会也不应当对系统资源进行改变，所以是幂等的 POST 否 Post 方法一般用于创建新的资源。其每次执行都会新增数据，所以不是幂等的 PUT - Put 方法一般用于更新资源。该操作则分情况来判断是不是满足幂等，更新操作中直接根据某个值进行更新，也能保持幂等。不过执行累加操作的更新是非幂等 DELETE - Delete 方法一般用于删除资源。该操作则分情况来判断是不是满足幂等，当根据唯一值进行删除时，删除同一个数据多次执行效果一样。不过需要注意，带查询条件的删除则就不一定满足幂等了。例如在根据条件删除一批数据后，这时候新增加了一条数据也满足条件，然后又执行了一次删除，那么将会导致新增加的这条满足条件数据也被删除。 OPTIONS 是 主要用于获取当前URL所支持的方法，也是有点像查询 OPTIONS 是 主要用于获取当前URL所支持的方法，也是有点像查询 1.6 如何设计幂等 幂等意味着一条请求的唯一性，无论上面哪个方，都需要一个全局ID去标记这个请求是独一无二的。 数据库唯一索引控制幂等，那唯一索引是唯一的 数据库主键控制幂等， 那么主键是唯一的 全局唯一ID 下面Demo防重Token令牌方案使用的是UUID，但是UUID的缺点比较明显，字符串占用的空间比较大，生成的ID过于随机，可读性差，而且没有递增。 我们还可以使用Snowflake雪花算法 生成唯一ID 雪花算法是一种生成分布式全局ID的算法，生成的ID称为Snowflakes IDs 一个SnowflakesID有64位： 第1位： Java中long的最高位是符号位，正数为0，负数是1，一般生成的ID都是正数，所以默认为0 第2-42位：时间戳，表示了自选定的时期以来的毫秒数 第43-52位：计算机ID，防止冲突 第53-64位：每台及其上生成ID的序列号，这允许在同一毫秒内创建多个Snowflake ID 全局唯一ID还可以使用百度的Uidgenerator、美团的leaf 幂等设计的基本流程 幂等处理的过程，其实就是过滤以下已经收到的请求，然后判断请求是否之前收到过，把请求存储起来，收到请求时，先查下存储记录，记录存在就返回上次的结果，不存在就处理请求。 1.7 实现幂等的方案 方案1：数据库唯一主键 描述 利用数据库主键唯一约束的特性，依赖来说唯一主键比较适用于插入时的幂等性，其能保证一张表只能存在一条带该唯一主键的记录 使用数据库唯一主键完成幂等性时需要注意的是，该主键一般来说并不是使用数据库自增主键，而是使用分布式ID作为主键，这样才能保证在分布式环境下ID的全局一致性 使用操作 插入 删除 使用限制 需要生成全局唯一主键ID 主要流程 分布式ID服务可以使用Snowflake算法、数据库号段模式、Redis自增等方式生成； 方案2：数据库乐观锁 乐观锁：在操作数据时，非常乐观，认为别人不再同时在修改数据，因此乐观锁不会上锁，只是在执行更新的时候判断以下，在此期间是否别人修改了数据 描述 一般只适用于更新操作的过程，在表中增加version版本字段，每次对该表的这条数据更新时，都会带上上次更新后的version值 使用操作 更新 使用限制 需要在业务表中添加额外字段 主要流程 更新数据前，先查下数据，查出版本号为version = 5 然后使用version=5 和 order_id=1010101一起作为条件去更新 为什么版本号建议自增呢？ 因为乐观锁存在ABA的问题，如果version版本一直是自增就不会出现ABA的情况了。 ABA问题：一个线程先读取共享内存数据值A，随后因某种原因，线程暂时挂起，同时另一个线程临时将共享内存数据值先改为B，随后又改回为A。随后挂起线程恢复，并通过CAS比较，最终比较结果将会无变化。这样会通过检查，这就是ABA问题。 在CAS比较前会读取原始数据，随后进行原子CAS操作。这个间隙之间由于并发操作，最终可能会带来问题 相当于是只关心共享变量的起始值和结束值，而不关心过程中共享变量是否被其他线程动过。 方案3：防重Token令牌 描述 针对客户端连续点击或者调用方的超时重试等情况，例如提交订单，此种操作就可以用 Token 的机制实现防止重复提交。简单的说就是调用方在调用接口的时候先向后端请求一个全局 ID（Token），请求的时候携带这个全局 ID 一起请求（Token 最好将其放到 Headers 中），后端需要对这个 Token 作为 Key，用户信息作为 Value 到 Redis 中进行键值内容校验，如果 Key 存在且 Value 匹配就执行删除命令，然后正常执行后面的业务逻辑。如果不存在对应的 Key 或 Value 不匹配就返回重复执行的错误信息，这样来保证幂等操作 使用操作 更新 插入 使用限制 需要生成全局唯一 Token串 需要使用Redis进行数据校验 主要流程 Token可以是一个序列号，也可以是分布式ID或者UUID串 验证成功：说明存在该token，是第一次调用接口，可以执行后面的业务代码，同时在redis中删除该token 验证失败：说明存在该token，是重复调用接口，不可以执行后面的业务代码； 注意，在并发情况下，执行 Redis 查找数据与删除需要保证原子性，否则很可能在并发下无法保证幂等性。其实现方法可以使用分布式锁或者使用 Lua 表达式来注销查询与删除操作。 方案4：下游传递唯一序列号 描述 所谓请求序列号，其实就是每次向服务端请求时候附带一个短时间内唯一不重复的序列号，该序列号可以是一个有序 ID，也可以是一个订单号，一般由下游生成，在调用上游服务端接口时附加该序列号和用于认证的 ID。 当上游服务器收到请求信息后拿取该 序列号 和下游 认证ID 进行组合，形成用于操作 Redis 的 Key，然后到 Redis 中查询是否存在对应的 Key 的键值对，根据其结果： 如果存在，就说明已经对该下游的该序列号的请求进行了业务处理，这时可以直接响应重复请求的错误信息。 如果不存在，就以该 Key 作为 Redis 的键，以下游关键信息作为存储的值（例如下游商传递的一些业务逻辑信息），将该键值对存储到 Redis 中 ，然后再正常执行对应的业务逻辑即可。 使用操作 更新 插入 删除 使用限制 需要第三方传递唯一序列号 需要使用Redis进行数据校验 主要流程 ① 下游服务生成分布式 ID 作为序列号，然后执行请求调用上游接口，并附带“唯一序列号”与请求的“认证凭据ID”。 ② 上游服务进行安全效验，检测下游传递的参数中是否存在“序列号”和“凭据ID”。 ③ 上游服务到 Redis 中检测是否存在对应的“序列号”与“认证ID”组成的 Key，如果存在就抛出重复执行的异常信息，然后响应下游对应的错误信息。如果不存在就以该“序列号”和“认证ID”组合作为 Key，以下游关键信息作为 Value，进而存储到 Redis 中，然后正常执行接来来的业务逻辑。 上面步骤中插入数据到 Redis 一定要设置过期时间。这样能保证在这个时间范围内，如果重复调用接口，则能够进行判断识别。如果不设置过期时间，很可能导致数据无限量的存入 Redis，致使 Redis 不能正常工作。 方案5：状态机幂等 描述 很多业务表都是有状态的，比如转账流水表就会有0-待处理，1-处理中，2-成功，3失败，转账流水更新时，都会涉及流水状态更新，即涉及状态机。 比如转账成功后，把处理中的流水更新为成功状态 1update trans_flow set status = 2 where biz_seq=&#x27;123&#x27; and status = 1; 主要流程 第1次请求来时，bizSeq流水号是 666，该流水的状态是处理中，值是 1，要更新为2-成功的状态，所以该update语句可以正常更新数据，sql执行结果的影响行数是1，流水状态最后变成了2。 第2请求也过来了，如果它的流水号还是 666，因为该流水状态已经2-成功的状态了，所以更新结果是0，不会再处理业务逻辑，接口直接返回。 伪代码 123456789101112131415Rsp idempotentTransfer（Request req）&#123; String bizSeq = req.getBizSeq(); int rows= &quot;update transfr_flow set status=2 where biz_seq=#&#123;bizSeq&#125; and status=1;&quot; if(rows==1)&#123; log.info(“更新成功,可以处理该请求”); //其他业务逻辑处理 return rsp; &#125;else if(rows==0)&#123; log.info(“更新不成功，不处理该请求”); //不处理，直接返回 return rsp; &#125; log.warn(&quot;数据异常&quot;) return rsp：&#125; 方案6：悲观锁 悲观锁：通俗的讲，就是每次去操作数据时，都会觉得别人中途会修改，所以每次拿到数据的时候都会上锁；官方点讲就是：共享资源每次只给一个线程使用，其他线程阻塞，用完后再把资源转为其他线程。 业务场景 1假设先查处订单，如果查到的时处理中状态，就处理完业务，然后更新订单状态为完成，如果查到订单状态不是处理中状态，则直接返回 伪代码 12345678910begin; # 1.开始事务select * from order where order_id=&#x27;666&#x27; -- 查询订单，判断状态-- 0-待处理，1-处理中，2-成功，3失败if（status != 处理中）&#123; //非处理中状态，直接返回； return ;&#125;## 处理业务逻辑update order set status=&#x27;完成&#x27; where order_id=&#x27;666&#x27; # 更新完成commit; -- 5.提交事务 这种场景时非原子操作的，在高并发环境下，可能会造成一个业务被执行两次的问题 当一个请求A在执行时，而另一个请求B也开始状态判断的操作，因为请求A还未来得及更改状态，所以请求B也能执行成功，这就导致一个业务被执行了两次。 可以使用数据库悲观锁（select … for update）来解决这个问题 12345678910begin; -- 1.开始事务select * from order where order_id=&#x27;666&#x27; for update -- 查询订单，判断状态,锁住这条记录-- 0-待处理，1-处理中，2-成功，3失败if（status != 处理中）&#123; -- 非处理中状态，直接返回； return ;&#125;## 处理业务逻辑update order set status=&#x27;完成&#x27; where order_id=&#x27;666&#x27; -- 更新完成commit; -- 5.提交事务 这里面order_id需要时索引或主键，如果不是索引或主键，会锁表；相关内容可以查看博客《 select … for update表锁还是行锁 》 悲观锁在统一事务操作过程中，锁住了一行数据，别的请求只能等待，如果当前事务耗时比较长，就很影响接口性能，所以一般不建议使用悲观锁来做幂等 方案7：分布式锁 描述 分布式锁实现幂等性的逻辑就是：请求过来时，先去尝试获取分布式锁，如果获得成功就执行业务逻辑，反之获取失败的话，就舍弃请求直接返回成功 主要流程 分布式锁可以使用redis、zookeeper; redis可能会好一点，轻量级 redis分布式锁，可以使用命令set ex px nx + 唯一流水号实现，分布式锁的key 必须为业务的唯一标识 Redis执行设置key的动作时，需要设置过期时间，太长会占存储空间，太短拦截不了重复请求 总结 对于下单等存在唯一主键的可以使用&quot;唯一主键方案&quot;的方式实现 对于更新订单状态等相关的更新场景操作，可以使用&quot;乐观锁方案&quot; 对于上下游这种，下游请求上游，上游服务可以使用&quot;下游传递唯一序列号方案&quot;更为合理 类似于前端重复提交、重复下单、没有唯一ID号的场景，可以通过token与Redis配置的&quot;防重Token方案&quot;更为快捷 方案 适用方法 复杂度 缺点 数据库唯一主键 插入、删除 简单 只能用于存在唯一主键的场景 数据库乐观锁 更新 简单 只能用于更新操作，表中需要添加额外字段 请求序列号 插入、删除、更新 简单 1. 需要保证下游生成唯一序列号； 2. 需要Redis存储序列号 防重Token令牌 插入、更新、删除 适中 需要Redis存储序列号 悲观锁 更新、删除 适中 如果当前事务耗时比较长，就很影响接口性能 2. SpringBoot 防重Token令牌方案 该方案能保证在不同请求动作下的幂等性，实现逻辑可以看上面写的”防重Token令牌”方案; 2.1 引入相关依赖 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;idempotent-token&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;idempotent-token&lt;/name&gt; &lt;url&gt;http://www.example.com&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;parent&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;version&gt;2.3.5.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;pluginManagement&gt;&lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;/build&gt;&lt;/project&gt; 2.2 配置文件 123456789101112131415# 配置redis连接参数spring: redis: ssl: false host: localhost port: 6379 database: 0 timeout: 1000 password: lettuce: pool: max-active: 100 max-wait: -1 min-idle: 0 max-idle: 20 2.3 Token获取/验证接口 创建用于操作Token相关的Service类，包含创建token以及验证方法，其中： Token创建： 使用UUID工具创建token串，设置IDEMPOTENT_TOKEN_PREFIX:+token串作为key,以用户信息作为value，存入Redis; Token验证：接口Token串参数，加上前缀生成key，再传入用户信息value,使用Lua表达式（Lua表达式能保证命令执行的原子性）进行查找对应的key和value，执行完成后验证命令的返回结果，如果不为空且非0则验证成功，反之则失败; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/** * @author xiaoyuge */@Slf4j@Servicepublic class TokenUtilService &#123; @Autowired private StringRedisTemplate redisTemplate; /** * redis 的token键前缀 */ private static final String IDEMPOTENT_TOKEN_PREFIX = &quot;IDEMPOTENT_TOKEN:&quot;; /** * 创建token存入redis， 并返回token * @param value 用于辅助验证的value * @return token串 */ public String generateToken(String value) &#123; String token = UUID.randomUUID().toString(); //拼接redis key String key = IDEMPOTENT_TOKEN_PREFIX + token; //存储到redis 中，设置过期时间为5分钟 redisTemplate.opsForValue().set(key, value, 5, TimeUnit.MINUTES); return token; &#125; /** * 验证token的正确性 * @param token token字符串 * @param value 辅助验证信息 * @return 验证结果 */ public boolean validateToken(String token, String value) &#123; //设置Lua脚本，其中KEY[1] 是key, KEYS[2] 是value； 如果根据key获取到的值是value,那么删除key否则返回0 String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == KEYS[2] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; RedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(script, Long.class); //拼接key String key = IDEMPOTENT_TOKEN_PREFIX + token; //执行lua脚本，传递数组参数[key, value] Long result = redisTemplate.execute(redisScript, Arrays.asList(key, value)); if (result == null &amp;&amp; result != 0L) &#123; log.info(&quot;验证token= &#123;&#125;,key=&#123;&#125;, value=&#123;&#125;成功&quot;, token, key, value); return true; &#125; return false; &#125;&#125; 2.4 创建Controller 创建用于测试的 Controller 类，里面有获取 Token 与测试接口幂等性的接口，内容如下： 123456789101112131415161718192021222324252627282930313233343536/** * @author xiaoyuge */@Slf4j@RestControllerpublic class TokenController &#123; @Resource private TokenUtilService tokenUtilService; /** * 获取token * @return token串 */ @GetMapping(&quot;/token&quot;) public String getToken()&#123; //获取用户信息 String username = &quot;xiaoyuge&quot;; //使用用户信息作为辅助验证 //获取token并返回 return tokenUtilService.generateToken(username); &#125; /** * 接口幂等性测试接口 * @param token token串 * @return 执行结果 */ @PostMapping(&quot;/test&quot;) public String testIdempotence(@RequestHeader(value = &quot;token&quot;) String token)&#123; //获取用户信息，和上面保持一样的业务逻辑 String username = &quot;xiaoyuge&quot;; //根据token和用户相关信息到redis验证是否存在对应的信息 boolean result = tokenUtilService.validateToken(token, username); return result ? &quot;正常调用&quot;:&quot;重复调用&quot;; &#125;&#125; 2.5 创建Springboot启动类 123456@SpringBootApplicationpublic class IdempotentApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(IdempotentApplication.class, args); &#125;&#125; 2.6 创建测试类 测试多次访问同一个接口，是否只有第一次执行成功 12345678910111213141516171819202122232425262728@Slf4j@SpringBootTest@RunWith(SpringRunner.class)public class IdempotentTest &#123; @Autowired private WebApplicationContext webApplicationContext; @Test public void interfaceIdempotenceTest() throws Exception &#123; //初始化MockMvc MockMvc mockMvc = MockMvcBuilders.webAppContextSetup(webApplicationContext).build(); //调用获取 token 接口 String token = mockMvc.perform(MockMvcRequestBuilders.get(&quot;/token&quot;) .accept(MediaType.TEXT_HTML)) .andReturn() .getResponse().getContentAsString(); log.info(&quot;获取的token串：&#123;&#125;&quot;, token); for (int i = 0; i &lt; 5; i++) &#123; log.info(&quot;第&#123;&#125;次调用接口&quot;, i+1); String result = mockMvc.perform(MockMvcRequestBuilders.post(&quot;/test&quot;) .header(&quot;token&quot;, token) .accept(MediaType.TEXT_HTML)) .andReturn().getResponse().getContentAsString(); log.info(&quot;调用结果:&#123;&#125;&quot;, result); &#125; &#125;&#125; 调用结果返回如下： 1234567891011[main] org.example.IdempotentTest : 获取的token串：ed965e9e-42ce-4865-a1fd-25d13ad5544b[main] org.example.IdempotentTest : 第1次调用接口[main] org.example.IdempotentTest : 调用结果:正常调用[main] org.example.IdempotentTest : 第2次调用接口[main] org.example.IdempotentTest : 调用结果:重复调用[main] org.example.IdempotentTest : 第3次调用接口[main] org.example.IdempotentTest : 调用结果:重复调用[main] org.example.IdempotentTest : 第4次调用接口[main] org.example.IdempotentTest : 调用结果:重复调用[main] org.example.IdempotentTest : 第5次调用接口[main] org.example.IdempotentTest : 调用结果:重复调用 3. 接口数据安全方案 3.1 数据加密，防止报文明文传输 数据在网络传输过程中，很容易被抓包，如果使用的时http协议，因为他是明文传输的，用户的数据很容易被别人获取，所以需要对数据加密。 3.1.1 加密方式 常见搭的实现方式，就是对关键字段加密，比如登陆接口对密码加密。一般采用的是：对称加密算法（AES 来加解密，或者哈希算法（MD5） 对称加密：加密和揭秘使用相同的密钥的加密算法 非对称加密：非对称加密算法需要两个密钥（公开密钥和私有密钥）。公钥与私钥是成对存在的，如果用公钥对数据进行加密，只有对应的私钥才能解密。 更安全的做法，就是用非对称加密算法（RSA、SM2），公钥加密、私钥解密。 3.1.2 HTTPS安全协议 如果想对所有字段都加密的话，一般都推荐使用HTTPS协议，https就是在http和tcp之间添加一层加密层SSL。 客户端发起Https请求，连接到服务器的443端口。 服务器必须要有一套数字证书（证书内容有公钥、证书颁发机构、失效日期等）。 服务器将自己的数字证书发送给客户端（公钥在证书里面，私钥由服务器持有）。 客户端收到数字证书之后，会验证证书的合法性。如果证书验证通过，就会生成一个随机的对称密钥，用证书的公钥加密。 客户端将公钥加密后的密钥发送到服务器。 服务器接收到客户端发来的密文密钥之后，用自己之前保留的私钥对其进行非对称解密，解密之后就得到客户端的密钥，然后用客户端密钥对返回数据进行对称加密，酱紫传输的数据都是密文啦。 服务器将加密后的密文返回到客户端。 客户端收到后，用自己的密钥对其进行对称解密，得到服务器返回的数据。 基本的日常业务，数据传输加密这块的话，用https就可以，如果安全性要求较高的，比如登陆注册这些，需要传输密码的，密码就可以使用RSA等非对称加密算法，对密码加密。如果你的业务，安全性要求很高，你可以模拟https这个流程，对报文，再做一次加解密。 3.2 数据加签验签 数据报文加签验签，就是保证数据传输安全的常用手段，它可以保证数据在传输过程中不给篡改。 3.2.1 什么是加签验签 数据加签：用Hash算法（md5、SHA-256）把原始请求参数生成报文摘要，然后用私钥对这个摘要加密，就得到这个报文对应的数字签名sign（这个过程就是加签）。通常来说，请求方会把数字签名和报文原文一并发送给接收方 验签：接收防拿到原始报文和数字签名sign后，用同一个hash算法（比如都用MD5）从报文中生成摘要A，然后用对方提供的公钥对数字签名进行解密，得到摘要B，对比A和B是否相同，就可以知道报文是否被篡改过。 通俗一点讲：就是把请求参数，按照一定规则，利用hash算法+加密算法生成一个唯一标签sign。验签的话，就是把请求参数按照相同的规则处理，再用相同的hash算法，和对应的密钥解密处理，以对比这个签名是否一致。 3.2.2 有了Https，为什么还要加签验签 加签验签主要是防止数据在传输过程中被篡改，那如果都用了Https协议加密数据了，为啥还需要加签验签？ 数据在传输过程中被加密了，理论上，即使被抓包，数据也不会被篡改，但是HTTPS不是绝对的安全，另外Https加密的部分只是在外网，然后很对服务是内网相互跳转的，捡钱也可以保证在这里不被中间人篡改； 3.3 token 授权认证机制 日常开发中，我们的网站或者App都是需要用户登录的，那么如果是非登录接口，如何确保安全，如何确认用户身份？可以使用token授权机制 用户在客户端输入用户名和密码，点击登录后，服务器会校验密码，然后返回客户端一个token，并将token 以键值对的形式存放在缓存中（一般为Redis）后续用户访问需要授权的模块的操作时，都携带这个token，服务器接收到请求后，先对token验证，如果token存在，才表名时合法请求 这个其实用过jwt的同学应该都会清楚这个流程。 3.3.1 token授权认证方案 用户输入用户名和密码，发起登录请求 服务端校验密码，如果校验通过，生成全局唯一token 将token存在redis中，key是token, value为用户ID，设置一个过期时间 将token返回给客户端 用户发起其他业务请求时，需要携带这个token 后台服务统一拦截接口请求，进行token有效性验证，并从中获取用户信息，供后续业务逻辑使用，如果token不存在，请求无效。 3.3.2 如何保证token的安全？token被劫持呢？ 比如说，如果我拿到了token，是不是就可以调用服务端的任何接口？可以从下面几方面考虑 token设置合理的有效期 使用https协议 token可以再次加密 如果访问的时敏感信息，单纯的加token是不够的，通常还会设置白名单 3.4 时间戳timestamp超时机制 数据是很容易抓包，假设我们使用了https和加签，即使中间人抓到了数据报文，他也看不到真实数据，但是也要避免那种使用抓取的数据包进行恶意请求（如DOS攻击），以搞垮系统 这里我们可以引入时间戳超时机制，来保证接口安全。用户每次请求都带上当前时间的时间戳timestamp，服务器收到timestamp后，解密，验签通过后，与服务器当前时间进行比对，如果时间大于一定的时间（比如5分钟），则任务该请求无效。 3.5 timestamp+nonce方案防止重放攻击 时间戳超时机制也是有漏洞的，如果是在时间差内，黑客进行重放攻击，那么就可以使用timestamp + nonce方案了 nonce指唯一的随机字符串，用来标识每个被清明的请求，我们可以将每次请求的nonce参数存储到一个set集合中，或者使用json格式存储到数据库或缓存中，每次处理http请求是，首先判断请求的nonce参数是否在该集合中，如果存在则认为非法请求。 然而对于服务器而言， 永久保存nonce的代价非常大，可以通过timestamp来优化，因为timestamp参数对于超过5min的请求，都认为非法请求，所以我们只需要存储5min内的nonce参数集合即可。 3.6 限流机制 如果用户本来就是真实用户，他恶意频繁调用接口，那么这个时候就需要接入限流了。 常用的限流算法有： 令牌桶和漏桶算法 可以使用Guava的RateLimiter单机版限流，也可以使用Redis分布式限流，还可以使用阿里开源组件sentinel限流。比如：一分钟可以接受多少次请求 3.7 黑名单机制 如果发现了真实用户恶意请求,你可以搞个黑名单机制，把该用户拉黑。一般情况，会有些竞争对手，或者不坏好意的用户，想搞你的系统的。所以，为了保证安全，一般我们的业务系统，需要有个黑名单机制。对于黑名单发起的请求，直接返回错误码好了 3.8 白名单机制 有了黑名单机制，也可以搞个白名单机制啦。第三方需要接入我们的系统时，是需要提前申请网络白名单的。申请个IP网络白名单，只有白名单里面的请求，才可以访问我们的系统。 3.9 数据脱敏掩码 对于密码，或者手机号、身份证这些敏感信息，一般都需要脱敏掩码再展示的，如果是密码，还需要加密再保存到数据库。 对于手机号、身份证信息这些，日常开发中，在日志排查时，看到的都应该是掩码的。目的就是尽量不泄漏这些用户信息，虽然能看日志的只是开发和运维，但是还是需要防一下，做掩码处理。 对于密码保存到数据库，我们肯定不能直接明文保存。最简单的也需要MD5处理一下再保存，Spring Security中的 BCryptPasswordEncoder也可以，它的底层是采用SHA-256 +随机盐+密钥对密码进行加密，而SHA和MD系列是一样的，都是hash摘要类的算法。 3.10 数据参数合法性校验 接口数据的安全性保证，还需要我们的系统，有个数据合法性校验，简单来说就是参数校验，比如身份证长度，手机号长度，是否是数字等等。 4. 接口性能优化方案 4.1 本地缓存 本地缓存，最大的优点是应用和cache是在同一个进程内部，请求缓存非常快，没有过多的网络开销等，在单应用不要集群支持或者集群情况下各节点无需互相通知的场景使用本地缓存比较合适。 缺点；缓存和应用程序耦合，多个应用程序无法直接共享缓存，各应用或集群的各个节点都需要维护自己的缓存，对内存是一种浪费。 常用的本地缓存框架有Guava、Caffeine等，引入jar包即可直接使用 适用场景 对缓存内容实效性要求不高，能够接收一定的延迟，可以设置较短过期时间，被动失效更新保持数据的新鲜度 缓存的内容不会改变，比如：订单号与Uid的映射关系，一旦创建就不会发生改变 注意问题 内存cache数据条目上限，避免内存占用过多导致应用瘫痪 内存中的数据一处策略 实际开发中最好采用成熟的开源框架，避免踩坑 4.2 分布式缓存 分布式缓存借助分布式概念，集群化部署、独立运维、荣康无上限。虽然会有网络传输损耗，但1~2ms的延迟相较其他的可以忽略。 优秀的分布式缓存系统有大家所熟知的 Memcached 、Redis。对比关系型数据库和缓存存储，其在读和写性能上的差距可谓天壤之别，Redis单节点已经可以做到 8W+ QPS。设计方案时尽量把读写压力从数据库转移到缓存上，有效保护脆弱的关系型数据库。 注意问题 缓存的命中率，如果太低无法起到抗压的作用，压力还是压到了下游的存储层 缓存的空间大小，这个要根据具体业务场景来评估，防止空间不足，导致一些热点数据被置换出去 缓存数据的一致性 缓存的快速扩容问题 缓存的接口平均RT，最大RT，最小RT 缓存的QPS 网络出口流量 客户端连接数 4.3 并行化 梳理业务流程，画出时序图，分清楚哪些是串行？哪些是并行？充分利用多核 CPU 的并行化处理能力 如下图所示，存在上下文依赖的采用串行处理，否则采用并行处理 JDK 的 CompletableFuture 提供了非常丰富的API，大约有50种 处理串行、并行、组合以及处理错误的方法，可以满足我们的场景需求。 4.4 异步化 一个接口的 RT 响应时间是由内部业务逻辑的复杂度决定的，执行的流程约简单，那接口的耗费时间就越少。 所以，普遍做法就是将接口内部的非核心逻辑剥离出来，异步化来执行。 下图是一个电商的创建订单接口，创建订单记录并插入数据库是我们的核心诉求，至于后续的用户通知，如：给用户发个短信等，如果失败，并不影响主流程的完成。 我们会将这些操作从主流程中剥离出来。 4.5 池化 TCP 三次握手非常耗费性能，所以我们引入了 Keep-Alive 长连接，避免频繁的创建、销毁连接。 池化技术也是类似道理，将很多能重复使用的对象缓存起来，放到一个池子里，用的时候去申请一个实例对象 ，用完后再放回池子里。 池化技术的核心是资源的“预分配”和“循环使用”，常见的池化技术的使用有：线程池、内存池、数据库连接池、HttpClient 连接池等 连接池的几个重要参数：最小连接数、空闲连接数、最大连接数 比如创建一个线程池： 12345678new ThreadPoolExecutor(3, 15, 5, TimeUnit.MINUTES, new ArrayBlockingQueue&lt;&gt;(10), new ThreadFactoryBuilder().setNameFormat(&quot;data-thread-%d&quot;).build(), (r, executor) -&gt; &#123; if (r instanceof BaseRunnable) &#123; ((BaseRunnable) r).rejectedExecute(); &#125; &#125;); 4.6 分库分表 MySQL的底层 innodb 存储引擎采用 B+ 树结构，三层结构支持千万级的数据存储。 当然，现在互联网的用户基数非常大，这么大的用户量，单表通常很难支撑业务需求，将一个大表水平拆分成多张结构一样的物理表，可以极大缓解存储、访问压力。 分库分表主要有两个方向：垂直和水平。 说实话垂直方向（即业务方向）更简单。 在水平方向（即数据方向）上，分库和分表的作用，其实是有区别的，不能混为一谈。 分库：是为了解决数据库连接资源不足问题，和磁盘IO的性能瓶颈问题。 分表：是为了解决单表数据量太大，sql语句查询数据时，即使走了索引也非常耗时问题。此外还可以解决消耗cpu资源问题。 分库分表：可以解决 数据库连接资源不足、磁盘IO的性能瓶颈、检索数据耗时 和 消耗cpu资源等问题。 如果在有些业务场景中，用户并发量很大，但是需要保存的数据量很少，这时可以只分库，不分表。 如果在有些业务场景中，用户并发量不大，但是需要保存的数量很多，这时可以只分表，不分库。 如果在有些业务场景中，用户并发量大，并且需要保存的数量也很多时，可以分库分表。 4.7 SQL 优化 虽然有了分库分表，从存储维度可以减少很大压力，但「富不过三代」，我们还是要学会精打细算，就比如所有的数据库操作都是通过 SQL 来执行。 一个不好的SQL会对接口性能产生很大影响。 比如： 搞了个深度翻页，每次数据库引擎都要预查非常多的数据 索引缺失，走了全表扫描 一条 SQL 一次查询 几万条数据 4.8 预先计算 有很多业务的计算逻辑比较复杂，比如页面要展示一个网站的 PV、微信的拼手气红包等 如果在用户访问接口的瞬间触发计算逻辑，而这些逻辑计算的耗时通常比较长，很难满足用户的实时性要求。 一般我们都是提前计算，然后将算好的数据预热到缓存中，接口访问时，只需要读缓存即可 4.9 事务相关 很多业务逻辑有事务要求，针对多个表的写操作要保证事务特性。 但事务本身又特别耗费性能，为了能尽快结束，不长时间占用数据库连接资源，我们一般要减少事务的范围。 将很多查询逻辑放到事务外部处理。 另外在事务内部，一般不要进行远程的 RPC 接口访问，一般占用的时间比较长 @Transactional注解这种声明式事务的方式提供事务功能，容易造成大事务，引发其他的问题 从图中能够看出，大事务问题可能会造成接口超时，对接口的性能有直接的影响。 优化大事务: 少用@Transactional注解 将查询(select)方法放到事务外 事务中避免远程调用 事务中避免一次性处理太多数据 有些功能可以非事务执行 有些功能可以异步处理 4.10 海量数据处理 如果数据量过大，除了采用关系型数据库的分库分表外，我们还可以采用 NoSQL；如：MongoDB、Hbase、Elasticsearch、TiDB NoSQL 采用分区架构，对数据海量存储能较好的支持，但是事务方面可能没那么友好。 每一个 NoSQL 框架都有自己的特色，有支持 搜索的、有列式存储、有文档存储，大家可以根据自己的业务场景选择合适的框架。 4.11 批量读写 当下的计算机CPU处理速度还是很多的，而 IO 一般是个瓶颈，如：磁盘IO、网络IO。 有这么一个场景，查询 100 个人的账户余额？ 有两个设计方案： 方案一：开单次查询接口，调用方内部循环调用 100 次 方案二：服务提供方开一个批量查询接口，调用方只需查询 1 次 (更优) 数据库的写操作也是一样道理，为了提高性能，我们一般都是采用批量更新。 4.12 锁的粒度 并发业务，为了防止数据的并发更新对数据的正确性产生干扰，我们通常是采用 加锁 ，涉及独享资源每次只能是一个线程来处理。 问题点在于，锁是成对出现的，有加锁就是释放锁 对于非竞争资源，我们没有必要圈在锁内部，会严重影响系统的并发能力。 控制锁的范围是我们要考虑的重点。 4.12.1 synchronized 在java中提供了synchronized关键字给我们的代码加锁。 通常有两种写法：在方法上加锁 和在代码块上加锁。 123456//上传文件public synchronized doSave(String fileUrl) &#123; mkdir(); //创建文件夹 uploadFile(fileUrl); //上传 sendMessage(fileUrl); //发送信息&#125; 这里加锁的目的是为了防止并发的情况下，创建了相同的目录，第二次会创建失败，影响业务功能。 但这种直接在方法上加锁，锁的粒度有点粗。因为doSave方法中的上传文件和发消息方法，是不需要加锁的。只有创建目录方法，才需要加锁。 我们都知道文件上传操作是非常耗时的，如果将整个方法加锁，那么需要等到整个方法执行完之后才能释放锁。显然，这会导致该方法的性能很差，变得得不偿失。 我们可以改成在代码块上加锁了，具体代码如下: 123456789public void doSave(String path,String fileUrl) &#123; synchronized(this) &#123; if(!exists(path)) &#123; mkdir(path); &#125; &#125; uploadFile(fileUrl); sendMessage(fileUrl);&#125; 样改造之后，锁的粒度一下子变小了，只有并发创建目录功能才加了锁。而创建目录是一个非常快的操作，即使加锁对接口的性能影响也不大。 最重要的是，其他的上传文件和发送消息功能，任然可以并发执行。 当然，这种做在单机版的服务中，是没有问题的。但现在部署的生产环境，为了保证服务的稳定性，一般情况下，同一个服务会被部署在多个节点中。如果哪天挂了一个节点，其他的节点服务任然可用。 多节点部署避免了因为某个节点挂了，导致服务不可用的情况。同时也能分摊整个系统的流量，避免系统压力过大。 同时它也带来了新的问题：synchronized只能保证一个节点加锁是有效的，但如果有多个节点如何加锁呢? 这就需要使用：分布式锁了。目前主流的分布式锁包括：redis分布式锁、zookeeper分布式锁 和 数据库分布式锁 4.12.2 Redis分布式锁 在分布式系统中，由于redis分布式锁相对于更简单和高效，成为了分布式锁的首先，被我们用到了很多实际业务场景当中。 使用redis分布式锁的伪代码如下: 12345678910111213141516public void doSave(String path,String fileUrl) &#123; try &#123; String result = jedis.set(lockKey, requestId, &quot;NX&quot;, &quot;PX&quot;, expireTime); if (&quot;OK&quot;.equals(result)) &#123; if(!exists(path)) &#123; mkdir(path); uploadFile(fileUrl); sendMessage(fileUrl); &#125; return true; &#125; &#125; finally&#123; unlock(lockKey,requestId); &#125; return false;&#125; 跟之前使用synchronized关键字加锁时一样，这里锁的范围也太大了，换句话说就是锁的粒度太粗，这样会导致整个方法的执行效率很低。 其实只有创建目录的时候，才需要加分布式锁，其余代码根本不用加锁。 于是，我们需要优化一下代码： 12345678910111213141516171819public void doSave(String path,String fileUrl) &#123; if(this.tryLock()) &#123; mkdir(path); &#125; uploadFile(fileUrl); sendMessage(fileUrl);&#125;private boolean tryLock() &#123; try &#123; String result = jedis.set(lockKey, requestId, &quot;NX&quot;, &quot;PX&quot;, expireTime); if (&quot;OK&quot;.equals(result)) &#123; return true; &#125; &#125; finally&#123; unlock(lockKey,requestId); &#125; return false;&#125; 上面代码将加锁的范围缩小了，只有创建目录时才加了锁。这样看似简单的优化之后，接口性能能提升很多。说不定，会有意外的惊喜喔。哈哈哈。 redis分布式锁虽说好用，但它在使用时，有很多注意的细节，隐藏了很多坑。以后遇到了再记录下来。 4.12.3 数据库分布式锁 mysql数据库中主要有三种锁： 表锁：加锁快，不会出现死锁。但锁定粒度大，发生锁冲突的概率最高，并发度最低。 行锁：加锁慢，会出现死锁。但锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 间隙锁：开销和加锁时间界于表锁和行锁之间。它会出现死锁，锁定粒度界于表锁和行锁之间，并发度一般。 并发度越高，意味着接口性能越好。 所以数据库锁的优化方向是： 优先使用行锁，其次使用间隙锁，再其次使用表锁。 4.13 上下文传递 当需要一个数据时，如果没有调 RPC 接口去查，比如想用户信息这种通用型接口 因为前面要用，肯定已经查过。但是我们知道方法的调用都是以栈帧的形式来传递，随着一个方法执行完毕而出栈，方法内部的局部变量也就被回收了。 后面如果又要用到这个信息，只能重新去查。 如果能定义一个Context 上下文对象，将一些中间信息存储并传递下来，会大大减轻后面流程的再次查询压力。 4.14 空间大小 创建集合List&lt;String&gt; lists = Lists.newArrayList();如果说，要往里面插入 1000000 个元素，有没有更好的方式？ 方式一 结果：1000000 次插入 List，花费时间：154 方式二 结果：1000000 次插入 List，花费时间：134 如果我们预先知道集合要存储多少元素，初始化集合时尽量指定大小，尤其是容量较大的集合。 ArrayList 初始大小是 10，超过阈值会按 1.5 倍大小扩容，涉及老集合到新集合的数据拷贝，浪费性能。 4.15 查询优化 避免一次从 DB 中查询大量的数据到内存中，可能会导致内存不足，建议采用分批、分页查询 5. 接口设计的锦囊 别说话，先看图： 5.1 批量思想：批量操作数据库 打个比喻:假如你需要搬一万块砖到楼顶,你有一个电梯,电梯一次可以放适量的砖（最多放500）, 你可以选择一次运送一块砖,也可以一次运送500,你觉得哪种方式更方便，时间消耗更少? 123456789//优化前//for循环单笔入库for(TransDetail detail:transDetailList)&#123; insert(detail);&#125;//优化后batchInsert(transDetailList); 5.2 异步思想：耗时操作，考虑放到异步执行 耗时操作，考虑用异步处理，这样可以降低接口耗时。 假设一个转账接口，匹配联行号，是同步执行的，但是它的操作耗时有点长，优化前的流程： 为了降低接口耗时，更快返回，你可以把匹配联行号移到异步处理，优化后： 除了转账这个例子，日常工作中还有很多这种例子。比如：用户注册成功后，短信邮件通知，也是可以异步处理的~ 至于异步的实现方式，你可以用线程池，也可以用消息队列实现。 5.3 空间换时间思想：恰当使用缓存。 在适当的业务场景，恰当地使用缓存，是可以大大提高接口性能的。缓存其实就是一种空间换时间的思想，就是你把要查的数据，提前放好到缓存里面，需要时，直接查缓存，而避免去查数据库或者计算的过程。 这里的缓存包括：Redis缓存，JVM本地缓存，memcached，或者Map等等。我举个我工作中，一次使用缓存优化的设计吧，比较简单，但是思路很有借鉴的意义。 那是一次转账接口的优化，老代码，每次转账，都会根据客户账号，查询数据库，计算匹配联行号。 优化前：每次都查数据库，都计算匹配，比较耗时，所以使用缓存进行优化 5.4 预取思想：提前初始化到缓存 预取思想很容易理解，就是提前把要计算查询的数据，初始化到缓存。如果你在未来某个时间需要用到某个经过复杂计算的数据，才实时去计算的话，可能耗时比较大。这时候，我们可以采取预取思想，提前把将来可能需要的数据计算好，放到缓存中，等需要的时候，去缓存取就行。这将大幅度提高接口性能。 5.5 池化思想：预分配与循环使用 线程池可以帮我们管理线程，避免增加创建线程和销毁线程的资源损耗。 如果你每次需要用到线程，都去创建，就会有增加一定的耗时，而线程池可以重复利用线程，避免不必要的耗时。池化技术不仅仅指线程池，很多场景都有池化思想的体现，它的本质就是预分配与循环使用。 比如TCP三次握手，大家都很熟悉吧，它为了减少性能损耗，引入了Keep-Alive长连接，避免频繁的创建和销毁连接。当然，类似的例子还有很多，如数据库连接池、HttpClient连接池。 我们写代码的过程中，学会池化思想，最直接相关的就是使用线程池而不是去new一个线程。 5.6 事件回调思想：拒绝阻塞等待 如果你调用一个系统B的接口，但是它处理业务逻辑，耗时需要10s甚至更多。然后你是一直阻塞等待，直到系统B的下游接口返回，再继续你的下一步操作吗？这样显然不合理。 我们参考IO多路复用模型。即我们不用阻塞等待系统B的接口，而是先去做别的操作。等系统B的接口处理完，通过事件回调通知，我们接口收到通知再进行对应的业务操作即可。 5.7 远程调用由串行改为并行 假设我们设计一个APP首页的接口，它需要查用户信息、需要查banner信息、需要查弹窗信息等等。如果是串行一个一个查，比如查用户信息200ms，查banner信息100ms、查弹窗信息50ms，那一共就耗时350ms了，如果还查其他信息，那耗时就更大了。 其实我们可以改为并行调用，即查用户信息、查banner信息、查弹窗信息，可以同时并行发起。 5.8 锁粒度避免过粗 在高并发场景，为了防止超卖等情况，我们经常需要加锁来保护共享资源。但是，如果加锁的粒度过粗，是很影响接口性能的。 什么是加锁粒度呢？ 其实就是就是你要锁住的范围是多大。比如你在家上卫生间，你只要锁住卫生间就可以了吧，不需要将整个家都锁起来不让家人进门吧，卫生间就是你的加锁粒度。 不管你是synchronized加锁还是redis分布式锁，只需要在共享临界资源加锁即可，不涉及共享资源的，就不必要加锁。这就好像你上卫生间，不用把整个家都锁住，锁住卫生间门就可以了。 比如，在业务代码中，有一个ArrayList因为涉及到多线程操作，所以需要加锁操作，假设刚好又有一段比较耗时的操作（代码中的slowNotShare方法）不涉及线程安全问题。反例加锁，就是一锅端，全锁住: 123456789101112131415161718192021//不涉及共享资源的慢方法private void slowNotShare() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(100); &#125; catch (InterruptedException e) &#123; &#125;&#125;//错误的加锁方法public int wrong() &#123; long beginTime = System.currentTimeMillis(); IntStream.rangeClosed(1, 10000).parallel().forEach(i -&gt; &#123; //加锁粒度太粗了，slowNotShare其实不涉及共享资源 synchronized (this) &#123; slowNotShare(); data.add(i); &#125; &#125;); log.info(&quot;cosume time:&#123;&#125;&quot;, System.currentTimeMillis() - beginTime); return data.size();&#125; 正例： 123456789101112public int right() &#123; long beginTime = System.currentTimeMillis(); IntStream.rangeClosed(1, 10000).parallel().forEach(i -&gt; &#123; slowNotShare();//可以不加锁 //只对List这部分加锁 synchronized (data) &#123; data.add(i); &#125; &#125;); log.info(&quot;cosume time:&#123;&#125;&quot;, System.currentTimeMillis() - beginTime); return data.size();&#125; 5.9 切换存储方式：文件中转暂存数据 如果数据太大，落地数据库实在是慢的话，就可以考虑先用文件的方式暂存。先保存文件，再异步下载文件，慢慢保存到数据库。 之前开发了一个转账接口。如果是并发开启，10个并发度，每个批次1000笔转账明细数据，数据库插入会特别耗时，大概6秒左右；这个跟我们公司的数据库同步机制有关，并发情况下，因为优先保证同步，所以并行的插入变成串行啦，就很耗时。 优化前，1000笔明细转账数据，先落地DB数据库，返回处理中给用户，再异步转账。如图： 记得当时压测的时候，高并发情况，这1000笔明细入库，耗时都比较大。所以我转换了一下思路，把批量的明细转账记录保存的文件服务器，然后记录一笔转账总记录到数据库即可。接着异步再把明细下载下来，进行转账和明细入库。最后优化后，性能提升了十几倍。 优化后，流程图如下： 如果你的接口耗时瓶颈就在数据库插入操作这里，用来批量操作等，还是效果还不理想，就可以考虑用文件或者MQ等暂存。有时候批量数据放到文件，会比插入数据库效率更高。 5.10 索引 提到接口优化，很多小伙伴都会想到添加索引。没错，添加索引是成本最小的优化，而且一般优化效果都很不错。 索引优化这块的话，一般从这几个维度去思考： 你的SQL加索引了没？ 你的索引是否真的生效？ 你的索引建立是否合理？ 5.10.1 没加索引 explain先看执行计划 1explain select * from user_info where userId like &#x27;%123&#x27;; 你也可以通过命令show create table，整张表的索引情况。 1show create table user_info; 如果某个表忘记添加某个索引，可以通过alter table add index命令添加索引 1alter table user_info add index idx_name (name); 一般就是：SQL的where条件的字段，或者是order by 、group by后面的字段需需要添加索引。 5.10.2 索引不生效 有时候，即使你添加了索引，但是索引会失效的 5.10.3 索引设计不合理 我们的索引不是越多越好，需要合理设计。比如： 删除冗余和重复索引。 索引一般不能超过5个 索引不适合建在有大量重复数据的字段上、如性别字段 适当使用覆盖索引 如果需要使用force index强制走某个索引，那就需要思考你的索引设计是否真的合理了 5.11 优化SQL 除了索引优化，其实SQL还有很多其他有优化的空间。比如这些： 5.12 避免大事务问题 为了保证数据库数据的一致性，在涉及到多个数据库修改操作时，我们经常需要用到事务。而使用spring声明式事务，又非常简单，只需要用一个注解就行@Transactional，如下面的例子： 1234567@Transactionalpublic int createUser(User user)&#123; //保存用户信息 userDao.save(user); passCertDao.updateFlag(user.getPassId()); return user.getUserId();&#125; 这块代码主要逻辑就是创建个用户，然后更新一个通行证pass的标记。如果现在新增一个需求，创建完用户，调用远程接口发送一个email消息通知，很多小伙伴会这么写： 12345678@Transactionalpublic int createUser(User user)&#123; //保存用户信息 userDao.save(user); passCertDao.updateFlag(user.getPassId()); sendEmailRpc(user.getEmail()); return user.getUserId();&#125; 这样实现可能会有坑，事务中嵌套RPC远程调用，即事务嵌套了一些非DB操作。如果这些非DB操作耗时比较大的话，可能会出现大事务问题。 所谓大事务问题就是，就是运行时间长的事务。由于事务一致不提交，就会导致数据库连接被占用，即并发场景下，数据库连接池被占满，影响到别的请求访问数据库，影响别的接口性能。 大事务引发的问题主要有：接口超时、死锁、主从延迟等等。因此，为了优化接口，我们要规避大事务问题。我们可以通过这些方案来规避大事务： RPC远程调用不要放到事务里面 一些查询相关的操作，尽量放到事务之外 事务中避免处理太多数据 5.13 深分页问题 深分页问题，为什么会慢？我们看下这个SQL 1select id,name,balance from account where create_time&gt; &#x27;2020-09-19&#x27; limit 100000,10; limit 100000,10意味着会扫描100010行，丢弃掉前100000行，最后返回10行。即使create_time，也会回表很多次。 我们可以通过标签记录法和延迟关联法来优化深分页问题， 其他优化方案可以查看博客 破解LIMIT和OFFSET分页性能瓶颈。 5.14 优化程序结构 优化程序逻辑、程序代码，是可以节省耗时的。比如，你的程序创建多不必要的对象、或者程序逻辑混乱，多次重复查数据库、又或者你的实现逻辑算法不是最高效的，等等。 我举个简单的例子：复杂的逻辑条件，有时候调整一下顺序，就能让你的程序更加高效。 假设业务需求是这样：如果用户是会员，第一次登陆时，需要发一条感谢短信。如果没有经过思考，代码直接这样写了 123if(isUserVip &amp;&amp; isFirstLogin)&#123; sendSmsMsg();&#125; 假设有5个请求过来，isUserVip判断通过的有3个请求，isFirstLogin通过的只有1个请求。那么以上代码，isUserVip执行的次数为5次，isFirstLogin执行的次数也是3次，如下： 如果调整一下isUserVip和isFirstLogin的顺序： 123if(isFirstLogin &amp;&amp; isUserVip )&#123; sendMsg();&#125; 5.15 压缩传输内容 压缩传输内容，传输报文变得更小，因此传输会更快啦。10M带宽，传输10k的报文，一般比传输1M的会快呀。 比如视频网站：如果不对视频做任何压缩编码，因为带宽又是有限的。巨大的数据量在网络传输的耗时会比编码压缩后，慢好多倍。 5.16 海量数据处理，考虑NoSQL 之前看过几个慢SQL，都是跟深分页问题有关的。发现用来标签记录法和延迟关联法，效果不是很明显，原因是要统计和模糊搜索，并且统计的数据是真的大。最后跟组长对齐方案，就把数据同步到Elasticsearch，然后这些模糊搜索需求，都走Elasticsearch去查询了。 我想表达的就是，如果数据量过大，一定要用关系型数据库存储的话，就可以分库分表。但是有时候，我们也可以使用NoSQL，如Elasticsearch、Hbase等。 5.17 线程池设计要合理 我们使用线程池，就是让任务并行处理，更高效地完成任务。但是有时候，如果线程池设计不合理，接口执行效率则不太理想。 一般我们需要关注线程池的这几个参数：核心线程、最大线程数量、阻塞队列。 如果核心线程过小，则达不到很好的并行效果。 如果阻塞队列不合理，不仅仅是阻塞的问题，甚至可能会OOM 如果线程池不区分业务隔离，有可能核心业务被边缘业务拖垮 5.18 机器问题 （fullGC、线程打满、太多IO资源没关闭等等） 有时候，我们的接口慢，就是机器处理问题。主要有fullGC、线程打满、太多IO资源没关闭等等。 之前排查过一个fullGC问题：运营小姐姐导出60多万的excel的时候，说卡死了，接着我们就收到监控告警。后面排查得出，我们老代码是Apache POI生成的excel，导出excel数据量很大时，当时JVM内存吃紧会直接Full GC了。 如果线程打满了，也会导致接口都在等待了。所以。如果是高并发场景，我们需要接入限流，把多余的请求拒绝掉。 如果IO资源没关闭，也会导致耗时增加。这个大家可以看下，平时你的电脑一直打开很多很多文件，是不是会觉得很卡。 记录这篇博客花了好几天的时间，主要是筛选一些同类型的博客，然后整理一下，在这过程中也自己也是受益良多，也更加系统性的了解、熟悉接口方面的知识。这篇博客主要是借鉴于捡田螺的小男孩 设计好接口的36个锦囊 以及 18种接口优化总结；","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"接口设计","slug":"接口设计","permalink":"https://xiaoyuge5201.github.io/tags/%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/"}]},{"title":"破解LIMIT和OFFSET分页性能瓶颈","slug":"limit-offset","date":"2022-11-20T04:56:15.000Z","updated":"2022-11-20T04:56:15.000Z","comments":false,"path":"limit-offset/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/limit-offset/index.html","excerpt":"","text":"1. 分页方法分类 LIMIT X 12-- LIMIT X 表示: 读取 X 条数据select * from user limit 20 LIMIT Y OFFSET X 123-- LIMIT Y OFFSET X 表示: 跳过 X 条数据，读取 Y 条数据select * from user limit 20 OFFSET 10-- 从第10+1 行开始读取20条数据 LIMIT X, Y 12-- 跳过 X 条数据，读取 Y 条数据select * from user limit 20 , 10 对于简单的小型应用程序和数据量不是很大的场景，这种方式还是没有问题的，但是一旦数据量过大，这种分页方式存在瓶颈。 2. LIMIT和OFFSET 的问题 OFFSET 和 LIMIT 对于数据量少的项目来说是没有问题的，但是，当数据库里的数据量超过服务器内存能够存储的能力，并且需要对所有数据进行分页，问题就会出现，为了实现分页，每次收到分页请求时，数据库都需要进行低效的全表遍历 全表遍历就是一个全表扫描的过程，就是根据双向链表把磁盘上的数据页加载到磁盘的缓存页里去，然后在缓存页内部查找那条数据，这个过程是非常满的，所以说当数据量大的时候，全表遍历的性能非常低，时间特别长，应该尽量避免全表遍历 为了获取一页的数据：10万行中的第50000行到第50020行需要先获取 5 万行，这么做非常低效！ 3. 初探LIMIT查询效率 3.1 建表 测试数据库采用的是（存储引擎采用InnoDB） 表结构如下： 123456CREATE TABLE `user` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(100) DEFAULT NULL, `age` int DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 3.2 插入数据 12345678910111213-- 创建存储过程， 参数param1 为int 类型CREATE DEFINER=`root`@`localhost` PROCEDURE `insert_limit_test`(param1 int)BEGIN -- for循环遍历 插入 350万条数据 WHILE param1 &lt; 3500000 DO -- 插入表数据 INSERT INTO `user` ( `name`, `age` ) VALUES (CONCAT(&#x27;name_&#x27;,param1) , (param1 % 4)+10 ); SET param1 = param1 + 1; END WHILE;END;-- 调用存储过程CALL insert_limit_test(1); 1234567mysql&gt; select count(*) from user;+----------+| count(*) |+----------+| 3499999 |+----------+1 row in set (0.11 sec) 3.3 开始测试 首先偏移量设置为0，取20条数据(中间输出省略) 1234567891011mysql&gt; select * from user limit 0,20;+----+---------+------+| id | name | age |+----+---------+------+| 1 | name_1 | 11 |#...中间输出省略| 18 | name_18 | 12 || 19 | name_19 | 13 || 20 | name_20 | 10 |+----+---------+------+20 rows in set (0.00 sec) 可以看到查询时间基本忽略不计，于是我们要一步一步的加大这个偏移量然后进行测试，先将偏移量改为10000(中间输出省略)： 123456789101112mysql&gt; select * from user limit 10000,20;+-------+------------+------+| id | name | age |+-------+------------+------+| 10001 | name_10001 | 11 || 10002 | name_10002 | 12 | #...中间输出省略| 10018 | name_10018 | 12 || 10019 | name_10019 | 13 || 10020 | name_10020 | 10 |+-------+------------+------+20 rows in set (0.00 sec) 可以看到查询时间还是非常短的，几乎可以忽略不计，于是我们将偏移量直接上到340W(中间输出省略)： 1234567891011mysql&gt; select * from user limit 3400000,20;+---------+--------------+------+| id | name | age |+---------+--------------+------+| 3400001 | name_3400001 | 11 |#...中间输出省略| 3400018 | name_3400018 | 12 || 3400019 | name_3400019 | 13 || 3400020 | name_3400020 | 10 |+---------+--------------+------+20 rows in set (0.48 sec) 这个时候就可以看到非常明显的变化了，查询时间增到了0.48s。 3.4 分析原因 根据下面的结果可以看到三条查询语句都进行了全表扫描： 1234567891011121314151617181920212223mysql&gt; explain select * from user limit 0,20;+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 3493299 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+1 row in set, 1 warning (0.00 sec)mysql&gt; explain select * from user limit 10000, 20;+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 3493299 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+1 row in set, 1 warning (0.01 sec)mysql&gt; explain select * from user limit 3400000, 20;+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+| 1 | SIMPLE | user | NULL | ALL | NULL | NULL | NULL | NULL | 3493299 | 100.00 | NULL |+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+1 row in set, 1 warning (0.00 sec) 此时就可以知道的是，在偏移量非常大的时候，就像案例中的LIMIT 3400000,20这样的查询。 此时MySQL就需要查询3400020行数据，然后在返回最后20条数据。 前边查询的340W数据都将被抛弃，这样的执行结果可不是我们想要的。 接下来就是优化大偏移量的性能问题 4. 优化 1SELECT * FROM user WHERE id&gt;10 limit 20 这是一种基于指针的分页。你要在本地保存上一次接收到的主键 (通常是一个 ID) 和 LIMIT，而不是 OFFSET 和 LIMIT，那么每一次的查询可能都与此类似。 为什么？因为通过显式告知数据库最新行，数据库就确切地知道从哪里开始搜索（基于有效的索引），而不需要考虑目标范围之外的记录。 我们再来一次测试(中间输出省略)： 123456789101112131415161718mysql&gt; select * from user where id &gt; 3400000 limit 20;+---------+--------------+------+| id | name | age |+---------+--------------+------+| 3400001 | name_3400001 | 11 |#....中间输出省略| 3400019 | name_3400019 | 13 || 3400020 | name_3400020 | 10 |+---------+--------------+------+20 rows in set (0.00 sec)mysql&gt; EXPLAIN SELECT * FROM user WHERE id&gt;3400000 LIMIT 20;+----+-------------+-------+------------+-------+---------------+---------+---------+------+--------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+-------+------------+-------+---------------+---------+---------+------+--------+----------+-------------+| 1 | SIMPLE | user | NULL | range | PRIMARY | PRIMARY | 4 | NULL | 198326 | 100.00 | Using where |+----+-------------+-------+------------+-------+---------------+---------+---------+------+--------+----------+-------------+1 row in set, 1 warning (0.00 sec) 返回同样的结果，第一个查询使用了0.48 sec，而第二个仅用了0.00 sec。 注意：如果我们的表没有主键，比如是具有多对多关系的表，那么就使用传统的 OFFSET/LIMIT 方式，只是这样做存在潜在的慢查询问题。所以建议在需要分页的表中使用自动递增的主键，即使只是为了分页。 继续优化 类似于查询 SELECT * FROM table_name WHERE id &gt; 3400000 LIMIT 20; 这样的效率非常快,因为主键上是有索引的,但是这样有个缺点,就是ID必须是连续的,并且查询不能有WHERE语句,因为WHERE语句会造成过滤数据。那使用场景就非常的局限了，于是我们可以这样 使用覆盖索引优化 mysql的查询完全命中索引的时候，称为覆盖索引，是非常快的，因为查询只需要在索引上进行查找，之后就可以直接返回，而不用再回数据表那数据，因此我们可以先查处索引的ID，然后根据ID取数据 123456789101112-- user 为表名SELECT * FROM (SELECT id FROM user LIMIT 3400000,20) a LEFT JOIN user b ON a.id = b.id;mysql&gt; explain SELECT * FROM (SELECT id FROM user LIMIT 3400000,20) a LEFT JOIN user b ON a.id = b.id;+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | 3400020 | 100.00 | NULL || 1 | PRIMARY | b | NULL | eq_ref | PRIMARY | PRIMARY | 4 | a.id | 1 | 100.00 | NULL || 2 | DERIVED | user | NULL | index | NULL | PRIMARY | 4 | NULL | 3493299 | 100.00 | Using index |+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+3 rows in set, 1 warning (0.00 sec) 或者是 1234567891011SELECT * FROM user a INNER JOIN (SELECT id FROM user LIMIT 3400000,20) b USING (id);mysql&gt; explain SELECT * FROM user a INNER JOIN (SELECT id FROM user LIMIT 3400000,20) b USING (id);+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | NULL | ALL | NULL | NULL | NULL | NULL | 3400020 | 100.00 | NULL || 1 | PRIMARY | a | NULL | eq_ref | PRIMARY | PRIMARY | 4 | b.id | 1 | 100.00 | NULL || 2 | DERIVED | user | NULL | index | NULL | PRIMARY | 4 | NULL | 3493299 | 100.00 | Using index |+----+-------------+------------+------------+--------+---------------+---------+---------+------+---------+----------+-------------+3 rows in set, 1 warning (0.00 sec) 5. 总结 数据量大的时候不能使用OFFSET/LIMIT来进行分页，因为OFFSET越大，查询时间越久。 当然不能说所有的分页都不可以，如果你的数据就那么几千、几万条，那就很无所谓，随便使用。 如果我们的表没有主键，比如是具有多对多关系的表，那么就使用传统的 OFFSET/LIMIT 方式。 这种方法适用于要求ID为数值类型，并且查出的数据ID连续的场景且不能有其他字段的排序。","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"select ... for update表锁还是行锁","slug":"select-for-update","date":"2022-11-03T13:40:16.000Z","updated":"2022-11-03T13:40:16.000Z","comments":false,"path":"select-for-update/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/select-for-update/index.html","excerpt":"","text":"1. 概要 Select 查询语句是不会枷锁的，但是Select … for update 除了有查询语句的作用，还是加锁，而且是悲观锁。 使用索引： 行锁 未使用索引：表锁 2. 建表 123456789101112131415--建表语句CREATE TABLE t_user ( id INT ( 11 ) NOT NULL AUTO_INCREMENT, name VARCHAR ( 255 ) DEFAULT NULL, age INT ( 11 ) DEFAULT NULL, addr VARCHAR ( 255 ) DEFAULT NULL, PRIMARY KEY ( id ), -- 主键索引 KEY idx_age ( age ) USING BTREE -- 唯一索引) ENGINE = INNODB AUTO_INCREMENT = 1 DEFAULT CHARSET = utf8;-- 插入数据INSERT INTO t_user (name, age, addr) VALUES (&#x27;张三&#x27;, 12, &#x27;上海&#x27;);INSERT INTO t_user (name, age, addr) VALUES (&#x27;李四&#x27;, 31, &#x27;广东&#x27;);INSERT INTO t_user (name, age, addr) VALUES (&#x27;王五&#x27;, 32, &#x27;南昌&#x27;);INSERT INTO t_user (name, age, addr) VALUES (&#x27;赵六&#x27;, 24, &#x27;广东&#x27;); 需要关闭自动提交，通过set @@autocommit = 0;设置为手动提交，0代表手动提交，1代表自动提交 3. 验证 3.1 场景一 使用主键id为1条件去查询，然后开启另一个事务对主键id为1对数据进行更新； 第一个事务使用select … for update查询，没有提交事务； 第二个事务，去更新主键id为1的数据，被阻塞了，长时间拿不到锁导致报错 结论：使用主键字段进行select … for update操作会锁住当前记录。 3.2 场景二 使用主键id=1为条件查询，开启另一个事务对主键id=2的数据进行更新 第一个事务使用select … for update查询，没有提交事务； 第二个事务对另一条id为2的数据更新，可以看到更新成功。 结论：使用主键字段进行select … for update操作会锁住当前记录，其他行数据可以进行正常的更新操作。 3.3 场景三 使用 唯一索引age12 查询，开启另一个事务对 唯一索引age=12 的数据进行更新 第一个事务使用select … for update查询，没有提交事务； 第二个事务对age=12的数据更新，被阻塞了。 结论：使用唯一索引字段进行select … for update操作会锁住当前记录，其他数据可以进行正常的更新操作。 3.4 场景四 使用普通字段 addr 进行操作 第一个事务使用select … for update查询，没有提交事务； 第二个事务进行任何数据更新操作，被阻塞了。 结论：使用非索引字段进行select … for update操作都会锁表，没有commit之前任何更新操作无法获取锁。","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"过滤器Filter与拦截器区别","slug":"filter-interceptor","date":"2022-10-29T14:25:32.000Z","updated":"2022-10-29T14:25:32.000Z","comments":false,"path":"filter-interceptor/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/filter-interceptor/index.html","excerpt":"","text":"1. 过滤器（Filter） Servlet中的过滤器Filter实现了javax.servlet.Filter接口的服务器端程序，主要用途是设置字符集（CharacterEncodingFilter）、控制权限、控制转向、用户是否已经登陆、有没有权限访问该页面等。其工作原理是，只要你在web.xml文件配置好要拦截的客户端请求，它都会帮你拦截到请求，此时，其实你可以对请求或响应(Request、response)统一设置编码； 它web应用启动而启动，只初始化一次，以后就可以拦截相关请求，只有当你的web应用停止或重新部署的时候才销毁。 Filter可以认为是Servlet的一种&quot;加强版&quot;，它主要用于对用户请求进行预处理，也可以对HttpServletResponse进行后处理，是个典型的处理链。 Filter可以对用户请求生成响应，和Servlet相同 处理流程：用户请求-&gt;Filter预处理-&gt;Servlet处理请求生成响应-&gt;Filter对响应进行后处理 1.1 Filter用处 在HttpServletRequest到达Servlet之前，拦截客户的HttpServletRequest。 根据需要检查HttpServletRequest，也可以修改HttpServletRequest头和数据。 在HttpServletResponse到达客户端之前，拦截HttpServletResponse。 根据需要检查HttpServletResponse，也可以修改HttpServletResponse头和数据。 Filter有如下几个种类。 1.2 Filter种类 用户授权的Filter：Filter负责检查用户请求，根据请求过滤用户非法请求。 日志Filter：详细记录某些特殊的用户请求。 负责解码的Filter:包括对非标准编码的请求解码。 能改变XML内容的XSLT Filter等。 Filter可以负责拦截多个请求或响应；一个请求或响应也可以被多个Filter拦截。 1.3 创建Filter步骤 创建Filter处理类，并实现javax.servlet.Filter接口 web.xml文件中配置Filter（或者使用@WebFilter注解） javax.servlet.Filter接口中中定义的三个方法： void init(FilterConfig config):用于完成Filter的初始化。 void destory():用于Filter销毁前，完成某些资源的回收。 void doFilter(ServletRequest request,ServletResponse response,FilterChain chain):实现过滤功能，该方法就是对每个请求及响应增加的额外处理。该方法可以实现对用户请求进行预处理(ServletRequest request)，也可实现对服务器响应进行后处理(ServletResponse response)—它们的分界线为是否调用了chain.doFilter(),执行该方法之前，即对用户请求进行预处理；执行该方法之后，即对服务器响应进行后处理。 2. 拦截器（Interceptor） 拦截器是在面向切面变成中应用的，就是service或一个方法前/后调用一个方法。是基础java的放射机制。拦截是不是在web.xml 在AOP（Aspect-Oriented Programming)中用于某个方法或字段被访问之前，进行拦截，然后在之前或之后加入某些操作，甚至在抛出异常的时候做业务逻辑的操作。拦击器是AOP的一种实现策略。 2.1 拦截器的实现方式 SpringMVC中的Interceptor拦截请求是通过HandlerInterceptor来实现的，在SpringMVC中定义Interceptor主要有两种方式： 实现Spring的HandlerInterceptor接口或者继承了实现HandlerInterceptor接口的类（比如 HandlerInterceptorAdapter ） 实现Spring的WebRequestInterceptor接口，或者继承了实现WebRequestInterceptor接口的类 Interceptor中的方法： preHandle (HttpServletRequest request, HttpServletResponse response, Object handle) 方法，顾名思义，该方法将在请求处理之前进行调用。SpringMVC 中的Interceptor 是链式的调用的，在一个应用中或者说是在一个请求中可以同时存在多个Interceptor 。每个Interceptor 的调用会依据它的声明顺序依次执行，而且最先执行的都是Interceptor 中的preHandle 方法，所以可以在这个方法中进行一些前置初始化操作或者是对当前请求的一个预处理，也可以在这个方法中进行一些判断来决定请求是否要继续进行下去。该方法的返回值是布尔值Boolean类型的，当它返回为false 时，表示请求结束，后续的Interceptor 和Controller 都不会再执行；当返回值为true 时就会继续调用下一个Interceptor 的preHandle 方法，如果已经是最后一个Interceptor 的时候就会是调用当前请求的Controller 方法 postHandle (HttpServletRequest request, HttpServletResponse response, Object handle, ModelAndView modelAndView) 方法，由preHandle 方法的解释我们知道这个方法包括后面要说到的afterCompletion 方法都只能是在当前所属的Interceptor 的preHandle 方法的返回值为true 时才能被调用。postHandle 方法，顾名思义就是在当前请求进行处理之后，也就是Controller 方法调用之后执行，但是它会在DispatcherServlet 进行视图返回渲染之前被调用，所以我们可以在这个方法中对Controller 处理之后的ModelAndView 对象进行操作。postHandle 方法被调用的方向跟preHandle 是相反的，也就是说先声明的Interceptor 的postHandle 方法反而会后执行，这和Struts2 里面的Interceptor 的执行过程有点类型。Struts2 里面的Interceptor 的执行过程也是链式的，只是在Struts2 里面需要手动调用ActionInvocation 的invoke 方法来触发对下一个Interceptor 或者是Action 的调用，然后每一个Interceptor 中在invoke 方法调用之前的内容都是按照声明顺序执行的，而invoke 方法之后的内容就是反向的。 afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handle, Exception ex) 方法，该方法也是需要当前对应的Interceptor 的preHandle 方法的返回值为true 时才会执行。顾名思义，该方法将在整个请求结束之后，也就是在DispatcherServlet 渲染了对应的视图之后执行。这个方法的主要作用是用于进行资源清理工作的 123456789101112131415161718192021222324252627282930313233import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import org.apache.log4j.Logger;import org.springframework.web.servlet.ModelAndView;import org.springframework.web.servlet.handler.HandlerInterceptorAdapter;public class ExecuteTimeInterceptor extends HandlerInterceptorAdapter&#123; private static final Logger logger = Logger.getLogger(ExecuteTimeInterceptor.class); //before the actual handler will be executed public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; long startTime = System.currentTimeMillis(); request.setAttribute(&quot;startTime&quot;, startTime); return true; &#125; //after the handler is executed public void postHandle(HttpServletRequest request, HttpServletResponse response, Object handler, ModelAndView modelAndView) throws Exception &#123; long startTime = (Long)request.getAttribute(&quot;startTime&quot;); long endTime = System.currentTimeMillis(); //统计耗时 long executeTime = endTime - startTime; //modified the exisitng modelAndView modelAndView.addObject(&quot;executeTime&quot;,executeTime); //log it if(logger.isDebugEnabled())&#123; logger.debug(&quot;[&quot; + handler + &quot;] executeTime : &quot; + executeTime + &quot;ms&quot;); &#125; &#125;&#125; 非Springboot项目 使用mvc:interceptors标签来声明需要加入到SpringMVC拦截器链中的拦截器 12345678910111213&lt;mvc:interceptors&gt; &lt;!-- 使用bean定义一个Interceptor，直接定义在mvc:interceptors根下面的Interceptor将拦截所有的请求 --&gt; &lt;bean class=&quot;com.company.app.web.interceptor.AllInterceptor&quot;/&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/**&quot;/&gt; &lt;mvc:exclude-mapping path=&quot;/parent/**&quot;/&gt; &lt;bean class=&quot;com.company.authorization.interceptor.SecurityInterceptor&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;mvc:interceptor&gt; &lt;mvc:mapping path=&quot;/parent/**&quot;/&gt; &lt;bean class=&quot;com.company.authorization.interceptor.SecuritySystemInterceptor&quot; /&gt; &lt;/mvc:interceptor&gt; &lt;/mvc:interceptors&gt; 可以利用mvc:interceptors标签声明一系列的拦截器，然后它们就可以形成一个拦截器链，拦截器的执行顺序是按声明的先后顺序执行的，先声明的拦截器中的preHandle方法会先执行，然而它的postHandle方法和afterCompletion方法却会后执行。 在mvc:interceptors标签下声明interceptor主要有两种方式： 直接定义一个Interceptor实现类的bean对象。使用这种方式声明的Interceptor拦截器将会对所有的请求进行拦截。 使用mvc:interceptor标签进行声明。使用这种方式进行声明的Interceptor可以通过mvc:mapping子标签来定义需要进行拦截的请求路径。 经过上述两步之后，定义的拦截器就会发生作用对特定的请求进行拦截了 Springboot项目 配置拦截器 1234567891011121314151617@Configuration//@Configurationpublicpublic class WebAppConfigurer implements WebMvcConfigurer &#123; /** * 配置拦截器 */ @Override public void addInterceptors(InterceptorRegistry registry) &#123; // 多个拦截器组成一个拦截器链 registry.addInterceptor(new ExecuteTimeInterceptor()).addPathPatterns(&quot;/**&quot;); //API限流拦截 registry.addInterceptor(accessLimitAjaxInterceptor()).addPathPatterns(&quot;/**&quot;).excludePathPatterns(&quot;/static/**&quot;,&quot;/login.html&quot;); registry.addInterceptor(accessInterceptor()).addPathPatterns(&quot;/**&quot;).excludePathPatterns(&quot;/static/**&quot;,&quot;/login.html&quot;); &#125;&#125; 2.2 拦截器（interceptor）使用 请求到达DispatcherServlet DispatcherServlet发送至Interceptor，执行preHandler 请求到达Controller 请求结束后，执行postHandler 3. 过滤器（Filter）与 拦截器（Interceptor）的区别 Spring的Interceptor(拦截器)与Servlet的Filter有相似之处，比如二者都是AOP编程思想的体现，都能实现权限检查、日志记录等。不同的是： Filter Interceptor Summary Filter 接口定义在 javax.servlet 包中 接口 HandlerInterceptor 定义在org.springframework.web.servlet 包中 Filter 定义在 web.xml 中 Filter在只在 Servlet 前后起作用。Filters 通常将 请求和响应（request/response） 当做黑盒子，Filter 通常不考虑servlet 的实现。 拦截器能够深入到方法前后、异常抛出前后等，因此拦截器的使用具有更大的弹性。允许用户介入（hook into）请求的生命周期，在请求过程中获取信息，Interceptor 通常和请求更加耦合。 在Spring架构的程序中，优先使用拦截器，几乎所有的Filter能够做的事情，Interceptor都可以实现 Filter 是 Servlet 规范规定的。 而拦截器既可以用于Web程序，也可以用于Application、Swing程序中。 使用范围不同 Filter 是在 Servlet 规范中定义的，是 Servlet 容器支持的。 而拦截器是在 Spring容器内的，是Spring框架支持的。 规范不同 Filter 不能够使用 Spring 容器资源 拦截器是一个Spring的组件，归Spring管理，配置在Spring文件中，因此能使用Spring里的任何资源、对象，例如 Service对象、数据源、事务管理等，通过IoC注入到拦截器即可 Spring使用interceptor更容易 Filter 是被 Server(like Tomcat) 调用 Interceptor 是被 Spring 调用 Filter优先于interceptor执行 3.1 执行顺序 用户请求 -&gt; 过滤前 -&gt; 拦截前 -&gt; Action处理 -&gt; 拦截后 -&gt; 过滤后 -&gt; 响应 4. 过滤器（Filter）与 拦截器（Interceptor）常见用途 Authentication Filters Logging and Auditing Filtersx Image conversion Filters Data compression Filters Encryption Filters Tokenizing Filters Filters that trigger resource access events XSL/T filters Mime-type chain Filter Request Filters 可以: 执行安全检查 perform security checks 格式化请求头和主体 reformat request headers or bodies 审查或者记录日志 audit or log requests 根据请求内容授权或者限制用户访问 Authentication-Blocking requests based on user identity. 根据请求频率限制用户访问 Response Filters 可以: 压缩响应内容,比如让下载的内容更小 Compress the response stream 追加或者修改响应 append or alter the response stream 创建或者整体修改响应 create a different response altogether 根据地方不同修改响应内容 Localization-Targeting the request and response to a particular locale. 5. 总结 过滤器：所谓过滤器顾名思义是用来过滤的，在java web中，你传入的request,response提前过滤掉一些信息，或者提前设置一些参数，然后再传入servlet或者struts的action进行业务逻辑，比如过滤掉非法url（不是login.do的地址请求，如果用户没有登陆都过滤掉）,或者在传入servlet或者struts的action前统一设置字符集，或者去除掉一些非法字符（聊天室经常用到的，一些骂人的话）。filter 流程是线性的， url传来之后，检查之后，可保持原来的流程继续向下执行，被下一个filter, servlet接收等. java的拦截器 主要是用在插件上，扩展件上比如 hibernate spring struts2等 有点类似面向切片的技术，在用之前先要在配置文件即xml文件里声明一段的那个东西。 拦截器（Interceptor）是基于Java的反射机制，而过滤器（Filter）是基于函数回调。从灵活性上说拦截器功能更强大些，Filter能做的事情，都能做，而且可以在请求前，请求后执行，比较灵活。Filter主要是针对URL地址做一个编码的事情、过滤掉没用的参数、安全校验（比较泛的，比如登录不登录之类），太细的话，还是建议用interceptor。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Mysql重置密码","slug":"mysql-forget-pwd","date":"2022-10-29T13:43:23.000Z","updated":"2022-10-29T13:43:23.000Z","comments":false,"path":"mysql-forget-pwd/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql-forget-pwd/index.html","excerpt":"","text":"Mysql 安装的时候忘记保存默认密码，需要重置密码； 1. 跳过Mysql密码认证 1vim /etc/my.cnf 在文档内[mysqld]第一行添加skip-grant-tables用来跳过密码验证的过程 2. 重启Mysql 1234service mysqld restart#或者systemctl restart mysqld 然后再输入mysql -uroot -p 一直按enter 就可以顺利进入数据库 123show databases;use mysql;select * from user; 3. 创建用户 1234create user &#x27;root&#x27;@&#x27;localhost&#x27; identity by &#x27;123456&#x27;;-- lolcahost表示本地，mysql登陆的时候不用指定IP登陆-- 如果需要外网访问，则将localhost改成 % 此步骤可能会报以下错误，没报错的跳过第4步 1234567mysql&gt; ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement#输入mysql&gt; flush privileges;刷新配置# 再次创建用户mysql&gt; create user &#x27;root&#x27;@&#x27;localhost&#x27; identity by &#x27;123456&#x27;; 再次报错，这步没报错的也是直接跳到赋予权限那一步，报错的以下操作： 12mysql&gt; drop user &#x27;root&#x27;@&#x27;localhost&#x27;;mysql&gt; create user &#x27;root&#x27;@&#x27;localhost&#x27; identified by &#x27;123456&#x27;; 4. 赋予root权限 1234mysql&gt; GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;localhost&#x27; WITH GRANT OPTION; #赋予所有库所有表操作权限mysql&gt; flush privileges;mysql&gt; exit; 再次查询user表，会发现用户表新建了一个root用户 5. 修改配置文件 1234567vim /etc/my.cnf##删除配置#skip-grant-table=1##保存并重启mysqlservice myqld restart","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"Tidb查询优化","slug":"tidb-optimization","date":"2022-10-15T06:23:37.000Z","updated":"2022-10-15T06:23:37.000Z","comments":false,"path":"tidb-optimization/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/tidb-optimization/index.html","excerpt":"","text":"1. tidb查询优化 tidb数据库查询3亿条数据需要花费20s以上，然后按照下面的优化方式执行过后，查询只需要1.5s； 查看表的健康度 12345 show stats_healthy where table_name=&#x27;xxxx&#x27;; ``` 2. 查看自动统计状态 ```sql show variables like &#x27;%auto_analyze%&#x27;; tidb_auto_analyze_start_time和tidb_auto_analyze_end_time控制了自动收集统计信息的窗口； tidb_auto_analyze_ratio 控制了可以进行自动收集统计信息的阈值，默认为0.5，如果后续表的数据量增加，可以酌情调整这个参数到0.3或者0.2，意味着这张表的数据修改超过总表行数的30%或者20%就会自动收集，会更敏感一些 官方文档：https://docs.pingcap.com/zh/tidb/dev/statistics#自动更新 调整自动收集统计信息的阈值 1set global tidb_auto_analyze_ratio = 0.2; 查看健康度低于阈值的表信息 1show stats_healthy where healthy&lt;80; 低于80的重新收集统计信息 1analyze table xxx 加快索引添加速度 12345show global variables like &#x27;tidb_ddl_reorg_%&#x27;; # 记录原来的数值， 添加完索引后记得改回去set global tidb_ddl_reorg_batch_size = 1024;set global tidb_ddl_reorg_worker_cnt = 16; 如果某张表因为业务需求导入了大量的数据，而这些数据在达到tidb_auto_analyze_start_time之前就需要使用SQL进行查询，此时建议对导入的表进行手动的收集； 我们也可以分析业务的行为，如果某张表有定时任务会自动插入大量数据，自动任务完成后需要产生报表的需求，也建议在自动任务结束后手动对相关的表进行手动收集；","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"tidb","slug":"tidb","permalink":"https://xiaoyuge5201.github.io/tags/tidb/"}]},{"title":"CentOS Kafka 3.2.0 单机集群安装（伪集群）","slug":"kafka-colony-install","date":"2022-07-03T13:45:23.000Z","updated":"2022-07-03T13:45:23.000Z","comments":false,"path":"kafka-colony-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/kafka-colony-install/index.html","excerpt":"","text":"1. 准备工作 由于没有那么多台机器，所以在同一台机器上运行多个Kafka服务，只是端口不同 安装路径： /usr/local/tools ; 服务器IP： 192.168.44.161 基于Kafka单机版安装流程，请查看 CentOS安装kafka 3.2.0单机版 所有Kafka节点连接到相同的ZK（或ZK集群），需要先安装一个ZK，请参考 CentOS安装Zookeeper 3.7.1单节点 , 在本例中ZK也安装在这台机器上。 注意：单机的kafka和集群的Kafka不要混用一个ZK，否则会出现数据混乱的问题。 2. 下载解压kafka 1234cd /usr/local/toolswget https://dlcdn.apache.org/kafka/3.2.0/kafka_2.12-3.2.0.tgztar -xzvf kafka_2.12-3.2.0.tgzcd kafka_2.12-3.2.0 3. 修改配置文件 复制3个配置文件 1234cd configcp server.properties server1.properties cp server.properties server2.properties cp server.properties server3.properties 修改配置文件中的broker.id分别为1、2、3 listeners这一行取消注释，端口号分别为9093、9094、9095 log.dirs分别设置为kafka-logs1、kafka-logs2、kafka-logs3（先创建） 1mkdir -p /tmp/kafka-logs1 /tmp/kafka-logs2 /tmp/kafka-logs3 server1.properties 的配置： 123broker.id=1listeners=PLAINTEXT://192.168.44.161:9093log.dirs=/tmp/kafka-logs1 server2.properties 的配置: 123broker.id=2listeners=PLAINTEXT://192.168.44.161:9094log.dirs=/tmp/kafka-logs2 server3.properties 的配置： 123broker.id=3listeners=PLAINTEXT://192.168.44.161:9095log.dirs=/tmp/kafka-logs3 如果listeners取消注释导致topic创建失败，可以修改为 12listeners=PLAINTEXT://:9093advertised.listeners=PLAINTEXT://10.1.14.159:9093 4. 启动3个服务 启动ZK12cd /usr/local/tools/apache-zookeeper-3.7.1-bin/bin./zkServer.sh start 启动Kafka 1234cd ../bin./kafka-server-start.sh -daemon ../config/server1.properties./kafka-server-start.sh -daemon ../config/server2.properties./kafka-server-start.sh -daemon ../config/server3.properties PS：如果遇到zk node exists的问题，先把brokers节点删掉（临时解决方案）。 5. 集群下创建Topic 在bin目录下，创建一个名为ygbtest的topic，只有一个服务本一个分区： 1sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic ygbtest 查看一创建的topic： 1sh kafka-topics.sh -list -zookeeper localhost:2181 6. 集群下启动Consumer 在一个新的原车鞥窗口中： 1sh kafka-console-consumer.sh --bootstrap-server 192.168.44.161:9093,192.168.44.161:9094,192.168.44.161:9095 --topic ygbtest --from-beginning kafka相关命令可以查看这篇博客 kafka常用命令 7. 集群下启动Producer 打开一个新的窗口，在kafka解压目录下： 1sh kafka-console-producer.sh --broker-list 192.168.44.161:9093,192.168.44.161:9094,192.168.44.161:9095 --topic ygbtest 8. 集群下Producer窗口发送消息 在生产者Producer窗口输入hello world 回车","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://xiaoyuge5201.github.io/tags/kafka/"}]},{"title":"基于Canal和Kafka实现数据同步","slug":"canal-kafka-async","date":"2022-07-03T08:38:33.000Z","updated":"2022-07-03T08:38:33.000Z","comments":false,"path":"canal-kafka-async/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/canal-kafka-async/index.html","excerpt":"","text":"1. 前言 Canal的作用：把自己&quot;伪装&quot;成一个Mysql的slave，不停同步master的binlog数据，再把binlog数据以TCP或者MQ的方式（支持kafka、RabbitMQ、RocketMQ）发送给需要同步数据的项目 canal项目地址：https://github.com/alibaba/canal/releases , 2022-05-24发布的最新版1.1.6。 测试需要同步的目标数据库是192.168.44.121上部署的数据库 2. 在目标数据库上创建用户和数据库 注意 121 的数据库首先要开启binlog，binlog-format必须是ROW 12log-bin=/var/lib/mysql/mysql-binbinlog-format=ROW 用户和数据库创建 12345678910111213-- 创建canal专用的用户，用于访问master获取binlogCREATE USER canal IDENTIFIED BY &#x27;123456&#x27;;-- 给canal用户分配查询和复制的权限GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO canal@&#x27;%&#x27;;-- 刷新权限FLUSH PRIVILEGES;ALTER USER &#x27;canal&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;;-- 创建测试数据库CREATE DATABASE `canaltest` CHARSET `utf8mb4` COLLATE `utf8mb4_unicode_ci`; 3. 安装ZK和kafka 在192.168.44.161机器上安装ZK和kafka, 这里我们安装伪集群版本，具体步骤请参考 CentOS Kafka 3.2.0 单机集群安装（伪集群） 4. 安装canal 下载canal 以安装目录:/usr/local/tools/canal 为例。 123456cd /usr/local/tools/midir canalcd canalwget https://github.com/alibaba/canal/releases/download/canal-1.1.6/canal.deployer-1.1.6.tar.gztar -zxvf canal.deployer-1.1.6.tar.gz 如果下载慢的话，可以先下载到本地，然后上传到服务器 修改配置：conf/canal.properties 12cd /usr/local/tools/canal/confvim canal.propertis 修改配置如下： 12canal.serverMode=kafkacanal.mq.servers = 192.168.44.160:9092 修改配置：example/instance.properties 12cd /usr/local/tools/canal/examplevim instance.properties 1234567canal.instance.master.address=192.168.44.121:3306canal.instance.dbUsername=canalcanal.instance.dbPassword=123456# 新增canal.instance.defaultDatabaseName=canaltest# 这个topic会自动创建canal.mq.topic=canal-topic 启动canal 1234cd /usr/local/tools/canal/binsh startup.sh# 查看实例日志tail -100f /usr/local/tools/canal/logs/canal/canal.log 5. 建表测试 在canaltest数据随表建一张表，做增删改的操作。 在Kafka服务器上消费这个topic 1./kafka-console-consumer.sh --bootstrap-server 192.168.44.160:9092 --topic canal-topic kafka相关命令可以查看这篇博客 kafka常用命令 成功消费到canal发送的消息：","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"Canal","slug":"Canal","permalink":"https://xiaoyuge5201.github.io/tags/Canal/"}]},{"title":"RocketMQ 二主二从异步集群部署","slug":"rocketmq-colony-install","date":"2022-07-03T03:37:05.000Z","updated":"2022-07-03T03:37:05.000Z","comments":false,"path":"rocketmq-colony-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/rocketmq-colony-install/index.html","excerpt":"","text":"1. 节点规划 第一台机器 192.168.44.163 端口 名称 9876 NameServer1 10910 BrokerA-master 10921 BrokerB-slave 第二台机器 192.168.44.164 端口 名称 9876 NameServer2 10911 BrokerA-slave 10920 BrokerB-slave 2. 下载并解压 具体操作可以查看 CentOS安装RocketMQ以及常用命令 123456cd /usr/local/toolswget https://dlcdn.apache.org/rocketmq/4.9.4/rocketmq-all-4.9.4-bin-release.zip#解压unzip rocketmq-all-4.9.4-bin-release.zip #改名mv rocketmq-all-4.9.4-bin-release rocketmq 在两台机器上都下载、解压好。 在rocketmq/conf目录下，有三种建议配置模式： 2m-2s-async(2主2从异步) —— 本文采用这种 2m-2s-sync (2主2从同步) 2m-noslave (2主) 现在需要修改两台机器上2m-2s-async这个目录中的文件。 配置文件修改之前先备份。 3. 配置第一台机器163 192.168.44.163的两个配置文件 broker-a.properties 12cd /usr/local/tools/rocketmq/conf/2m-2s-asyncvim broker-a.properties 修改的内容（名字自定义，保持一直，否则不能组成集群） 1brokerClusterName=ygb-cluster 增加内容: 1234567891011121314151617181920#Broker 对外服务的监听端口listenPort=10910#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#nameServer地址，分号分割namesrvAddr=192.168.44.163:9876;192.168.44.164:9876#存储路径storePathRootDir=/usr/local/tools/rocketmq/store/broker-a#commitLog 存储路径storePathCommitLog=/usr/local/tools/rocketmq/store/broker-a/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/tools/rocketmq/store/broker-a/consumequeue#消息索引存储路径storePathIndex=/usr/local/tools/rocketmq/store/broker-a/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/tools/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/tools/rocketmq/store/abort broker-b-s.properties 1vim broker-b-s.properties 修改的内容（名字自定义，保持一直，否则不能组成集群） 1brokerClusterName=ygb-cluster 增加内容: 1234567891011121314151617181920#Broker 对外服务的监听端口listenPort=10921#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#nameServer地址，分号分割namesrvAddr=192.168.44.163:9876;192.168.44.164:9876#存储路径storePathRootDir=/usr/local/tools/rocketmq/store/broker-b-s#commitLog 存储路径storePathCommitLog=/usr/local/tools/rocketmq/store/broker-b-s/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/tools/rocketmq/store/broker-b-s/consumequeue#消息索引存储路径storePathIndex=/usr/local/tools/rocketmq/store/broker-b-s/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/tools/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/tools/rocketmq/store/abort 4. 配置第二台机器164 192.168.44.164的两个配置文件; 修改的内容基本一致，主要是注意一下端口号、路径名。 broker-b.properties 12cd /usr/local/tools/rocketmq/conf/2m-2s-asyncvim broker-b.properties 修改的内容（名字自定义，保持一直，否则不能组成集群） 1brokerClusterName=ygb-cluster 增加内容: 1234567891011121314151617181920#Broker 对外服务的监听端口listenPort=10920#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#nameServer地址，分号分割namesrvAddr=192.168.44.163:9876;192.168.44.164:9876#存储路径storePathRootDir=/usr/local/tools/rocketmq/store/broker-b#commitLog 存储路径storePathCommitLog=/usr/local/tools/rocketmq/store/broker-b/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/tools/rocketmq/store/broker-b/consumequeue#消息索引存储路径storePathIndex=/usr/local/tools/rocketmq/store/broker-b/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/tools/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/tools/rocketmq/store/abort broker-a-s.properties 12cd /usr/local/tools/rocketmq/conf/2m-2s-asyncvim broker-a-s.properties 修改的内容（名字自定义，保持一直，否则不能组成集群） 1brokerClusterName=ygb-cluster 增加内容: 1234567891011121314151617181920#Broker 对外服务的监听端口listenPort=10911#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#nameServer地址，分号分割namesrvAddr=192.168.44.163:9876;192.168.44.164:9876#存储路径storePathRootDir=/usr/local/tools/rocketmq/store/broker-a-s#commitLog 存储路径storePathCommitLog=/usr/local/tools/rocketmq/store/broker-a-s/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/tools/rocketmq/store/broker-a-s/consumequeue#消息索引存储路径storePathIndex=/usr/local/tools/rocketmq/store/broker-a-s/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/tools/rocketmq/store/checkpoint#abort 文件存储路径abortFile=/usr/local/tools/rocketmq/store/abort 5. 创建数据目录 在第一台机器163 执行（只需要执行一次） 1mkdir -p /usr/local/tools/rocketmq/store/broker-a /usr/local/tools/rocketmq/store/broker-a/consumequeue /usr/local/tools/rocketmq/store/broker-a/commitlog /usr/local/tools/rocketmq/store/broker-a/index /usr/local/tools/rocketmq/logs /usr/local/tools/rocketmq/store/broker-b-s /usr/local/tools/rocketmq/store/broker-b-s/consumequeue /usr/local/tools/rocketmq/store/broker-b-s/commitlog /usr/local/tools/rocketmq/store/broker-b-s/index 在第二台机器164 执行（只需要执行一次） 1mkdir -p /usr/local/tools/rocketmq/store/broker-a-s /usr/local/tools/rocketmq/store/broker-a-s/consumequeue /usr/local/tools/rocketmq/store/broker-a-s/commitlog /usr/local/tools/rocketmq/store/broker-a-s/index /usr/local/tools/rocketmq/logs /usr/local/tools/rocketmq/store/broker-b /usr/local/tools/rocketmq/store/broker-b/consumequeue /usr/local/tools/rocketmq/store/broker-b/commitlog /usr/local/tools/rocketmq/store/broker-b/index 6. 启动两个NameServer 启动第一台163的NameServer 1nohup sh /usr/local/tools/rocketmq/bin/mqnamesrv &gt;/usr/local/tools/rocketmq/logs/mqnamesrv.log 2&gt;&amp;1 &amp; 启动第二台164的NameServer 1nohup sh /usr/local/tools/rocketmq/bin/mqnamesrv &gt;/usr/local/tools/rocketmq/logs/mqnamesrv.log 2&gt;&amp;1 &amp; 7. 启动Broker 启动 163 机器上的broker-a-master（在163上执行） 1nohup sh /usr/local/tools/rocketmq/bin/mqbroker -c /usr/local/tools/rocketmq/conf/2m-2s-async/broker-a.properties &gt; /usr/local/tools/rocketmq/logs/broker-a.log 2&gt;&amp;1 &amp; 在虚拟机中可能由于内存不够导致无法启动，日志文件中出现如下错误： 12nohup: ignoring inputJava HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000005c0000000, 8589934592, 0) failed; error=&#x27;Cannot allocate memory&#x27; (errno=12) 1vim /usr/local/tools/rocketmq/bin/runbroker.sh 把8g和4g改成512m和256m 1JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms512m -Xmx512m -Xmn256m&quot; 再次启动。 启动 164 机器上的broker-a-s（在164上执行） 1nohup sh /usr/local/tools/rocketmq/bin/mqbroker -c /usr/local/tools/rocketmq/conf/2m-2s-async/broker-a-s.properties &gt; /usr/local/tools/rocketmq/logs/broker-a-s.log 2&gt;&amp;1 &amp; 启动 164 的broker-b-master（在164上执行） 1nohup sh /usr/local/tools/rocketmq/bin/mqbroker -c /usr/local/tools/rocketmq/conf/2m-2s-async/broker-b.properties &gt; /usr/local/tools/rocketmq/logs/broker-b.log 2&gt;&amp;1 &amp; 启动 163 机器上的broker-b-s（在163上执行） 1nohup sh /usr/local/tools/rocketmq/bin/mqbroker -c /usr/local/tools/rocketmq/conf/2m-2s-async/broker-b-s.properties &gt; /usr/local/tools/rocketmq/logs/broker-b-s.log 2&gt;&amp;1 &amp; 查看两台机器的端口启动情况 1netstat -an|grep 端口号","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"https://xiaoyuge5201.github.io/tags/rocketmq/"}]},{"title":"CentOS安装RocketMQ以及常用命令","slug":"rocketmq-install","date":"2022-07-02T07:53:24.000Z","updated":"2022-07-02T07:53:24.000Z","comments":false,"path":"rocketmq-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/rocketmq-install/index.html","excerpt":"","text":"1.前言 nameserver默认端口：9876 rocketmq默认端口：10911 服务器IP：172.21.16.253 安装路径：/usr/local/tools 2. 下载 RocketMQ官网链接：http://rocketmq.apache.org/ ,然后选择Lastest Release进入下载界面 右键复制链接地址，wget下载，或者下载后上传到Centos服务器上。 12cd /usr/local/toolswget https://dlcdn.apache.org/rocketmq/4.9.4/rocketmq-all-4.9.4-bin-release.zip 3. 解压 解压二进制包，修改文件夹名称 12unzip rocketmq-all-4.9.4-bin-release.zip mv rocketmq-all-4.9.4-bin-release rocketmq 如果提示-bash: unzip: command not found 123#安装zip 和 unzipyum install zipyum install unzip 创建数据存储目录 1mkdir -p /usr/local/tools/rocketmq/store/broker-a /usr/local/tools/rocketmq/store/broker-a/consumequeue /usr/local/tools/rocketmq/store/broker-a/commitlog /usr/local/tools/rocketmq/store/broker-a/index /usr/local/tools/rocketmq/broker-a/logs 4. 修改配置文件 12cd /usr/local/tools/rocketmq/confvim broker.conf 增加内容 1234567891011121314151617181920#Broker 对外服务的监听端口listenPort=10911#是否允许 Broker 自动创建Topic，建议线下开启，线上关闭autoCreateTopicEnable=true#是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭autoCreateSubscriptionGroup=true#nameServer地址，分号分割namesrvAddr=localhost:9876#存储路径storePathRootDir=/usr/local/tools/rocketmq/store/broker-a#commitLog 存储路径storePathCommitLog=/usr/local/tools/rocketmq/store/broker-a/commitlog#消费队列存储路径存储路径storePathConsumeQueue=/usr/local/tools/rocketmq/store/broker-a/consumequeue#消息索引存储路径storePathIndex=/usr/local/tools/rocketmq/store/broker-a/index#checkpoint 文件存储路径storeCheckpoint=/usr/local/tools/rocketmq/store/broker-a/checkpoint#abort 文件存储路径abortFile=/usr/local/tools/rocketmq/store/broker-a/abort 5. 启动 依次启动nameserver和broker ,这两个命令可以做成alias 1234cd /usr/local/tools/rocketmq/binnohup sh mqnamesrv &amp;nohup sh mqbroker -c /usr/local/tools/rocketmq/conf/broker.conf &amp; 在虚拟机中有可能因为内存不够而启动失败 1）设置bin目录下的 runserver.sh 1JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn512m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m&quot; 2）设置bin目录下的 runbroker.sh 1JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn128m&quot; 3）如果改了上面两个还不行，那在修改bin目录下面的tools.sh 1JAVA_OPT=&quot;$&#123;JAVA_OPT&#125; -server -Xms256m -Xmx256m -Xmn256m -XX:PermSize=128m -XX:MaxPermSize=128m&quot; 6. 查看日志 启动成功后查看mq动态日志 12tail -f ~/logs/rocketmqlogs/namesrv.logtail -f ~/logs/rocketmqlogs/broker.log 7.关闭服务 1234cd /usr/local/tools/rocketmq/binsh mqshutdown namesrvsh mqshutdown broker 8. 常用命令 RocketMQ 提供有控制台及一系列控制台目录，用户管理员对主题、集群、broker等信息的管理。 进入rocketmq下的bin目录，可以看到该目录下有个mqadmin脚本 查看帮助 1sh mqadmin help 命令名称 例如，查看updateTopic的使用 1sh mqadmin help updateTopic 下面列举一些常用的命令。 8.1 创建（修改）Topic 指令： updateTopic 类路径：com.alibaba.rocketmq.tools.command.topic.UpdateTopicSubCommand 参数 参数 是否必填 说明 -b 如果 -c为空，则必填 broker地址，表示topic建在该broker -c 如果 -b为空，则必填 cluster名称，表示topic建在该集群（集群可通过clusterList查询） -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… -p 否 指定新topic的权限限制( W -r 否 可读队列数（默认为8） -w 否 可写队列数（默认为8） -t 是 topic名称（名称只能使用字符 [1]+$ ） 举例 12# 在集群DefaultCluster上创建主题ZTEExample，nameserve地址为172.21.16.253:9876sh mqadmin updateTopic –n 172.21.16.253:9876 –c DefaultCluster –t ZTEExample 8.2 删除Topic 指令： deleteTopic 类路径：com.alibaba.rocketmq.tools.command.topic.DeleteTopicSubCommand 参数 参数 是否必填 说明 -c 是 cluster名称，表示删除某集群下的某个topic （集群可通过clusterList查询） -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… -t 是 topic名称（名称只能使用字符 [2]+$ ） 举例 12# 在集群DefaultCluster上删除主题ZTEExample，nameserve地址为172.21.16.253:9876sh mqadmin deleteTopic –n 172.21.16.253:9876 –c DefaultCluster –t ZTEExample 8.3 创建（修改）订阅组 指令： updateSubGroup 类路径：com.alibaba.rocketmq.tools.command.consumer.UpdateSubGroupSubCommand 参数 参数 是否必填 说明 -b 如果 -c为空，则必填 broker地址，表示topic建在该broker -c 如果 -b为空，则必填 cluster名称，表示topic建在该集群（集群可通过clusterList查询） -d 否 是否容许广播方式消费 -g 是 订阅组名 -i 否 从哪个broker开始消费 -m 否 是否容许从队列的最小位置开始消费，默认会设置为false -q 否 消费失败的消息放到一个重试队列，每个订阅组配置几个重试队列 -r 否 重试消费最大次数，超过则投递到死信队列，不再投递，并报警 -s 否 消费功能是否开启 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… -w 否 发现消息堆积后，将Consumer的消费请求重定向到另外一台Slave机器 8.4 删除订阅组配置 指令： deleteSubGroup 类路径：com.alibaba.rocketmq.tools.command.consumer.DeleteSubscriptionGroupCommand 参数 参数 是否必填 说明 -b 如果 -c为空，则必填 broker地址，表示订阅组建在该broker -c 如果 -b为空，则必填 cluster名称，表示topic建在该集群（集群可通过clusterList查询） -g 是 订阅组名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.5 更新broker配置文件 指令： updateBrokerConfig 类路径：com.alibaba.rocketmq.tools.command.broker.UpdateBrokerConfigSubCommand 参数 参数 是否必填 说明 -b 如果 -c为空，则必填 broker地址，表示订阅组建在该broker -c 如果 -b为空，则必填 cluster名称，表示topic建在该集群（集群可通过clusterList查询） -k 否 是否容许广播方式消费 -v 是 value值 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.6 查看topic 列表信息 指令： topicList 类路径：com.alibaba.rocketmq.tools.command.broker.UpdateBrokerConfigSubCommand 参数 参数 是否必填 说明 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# nameserve地址为172.21.16.253:9876;sh mqadmin topicList -n 172.21.16.253:9876 8.7 查看路由信息 指令： topicRoute 类路径： com.alibaba.rocketmq.tools.command.topic.TopicRouteSubCommand 参数 参数 是否必填 说明 -t 是 topic名称 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查看主题%DLQ%consumer1的路由，nameserve地址为172.21.16.253:9876sh mqadmin topicRoute -n 172.21.16.253:9876 -t %DLQ%consumer1 8.8 查看topic统计信息 指令： topicStatus 类路径：com.alibaba.rocketmq.tools.command.topic.TopicStatsSubCommand 参数 参数 是否必填 说明 -t 是 topic名称 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查看主题%DLQ%consumer1的统计信息，nameserve地址为172.21.16.253:9876sh mqadmin topicStatus -n 172.21.16.253:9876 -t %DLQ%consumer1 8.9 查看broker统计信息 指令： brokerStatus 类路径：com.alibaba.rocketmq.tools.command.broker.BrokerStatsSubCommand 参数 参数 是否必填 说明 -b 是 broker地址 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查看broker(crmdb)的统计信息，broker地址为172.21.16.253:10911,nameserve地址为172.21.16.253:9876sh mqadmin brokerStatus –n 172.21.16.253:9876 -b 172.21.16.253:10911 8.10 根据消息ID查询消息 指令： queryMsgById 类路径：com.alibaba.rocketmq.tools.command.message.QueryMsgByIdSubCommand 参数 参数 是否必填 说明 -i 是 msgId -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12#查询msgId= C0A89F8000002A9F000000000000449A的消息，nameserve地址为172.21.16.253:9876sh mqadmin queryMsgById -n 172.21.16.253:9876 -i C0A89F8000002A9F000000000000449A 8.11 根据消息key查询消息 指令： queryMsgByKey 类路径：com.alibaba.rocketmq.tools.command.message.QueryMsgByKeySubCommand 参数 参数 是否必填 说明 -f 否 被查询消息的截止时间 -k 是 msgKey -t 是 Topic名称 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查询Topic= TopicTest下key=i0的消息，nameserve地址为172.21.16.253:9876sh mqadmin queryMsgByKey -n 172.21.16.253:9876 -t TopicTest -k i0 8.12 根据Offset查询消息 指令： queryMsgByOffset 类路径：com.alibaba.rocketmq.tools.command.message.QueryMsgByOffsetSubCommand 参数 参数 是否必填 说明 -b 是 Broker名称，表示订阅组建在该broker（这里需要注意填写的是broker的名称，不是broker的地址，broker名称可以在clusterList查到） -i 是 query队列id -o 是 offset值 -t 是 Topic名称 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12#查询brokerName=broker-a，Topic= TopicTest的第1个队列下offset=0的消息，nameserve地址为172.21.16.253:9876sh mqadmin queryMsgByOffset -n 172.21.16.253:9876 -b broker-a -i 0 -t TopicTest -o 0 8.13 查询Producer的网络连接 该命令只打印当前与cluster连接的producer网络连接信息 指令： producerConnection 类路径：com.alibaba.rocketmq.tools.command.connection.ProducerConnectionSubCommand 参数 参数 是否必填 说明 -g 是 生产者所属组名 -t 是 topic名称 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查询当前属于group（生产者组）=simple-producer-test的生产者到topic=ZTEExample的网络连接，nameserve地址为172.21.16.253:9876sh mqadmin producerConnection -n 172.21.16.253:9876 -g simple-producer-test -t ZTEExample 8.14 查询Consumer的网络连接 该命令只打印当前与cluster连接的Consumer网络连接信息 指令： consumerConnection 类路径：com.alibaba.rocketmq.tools.command.connection.ConsumerConnectionSubCommand 参数 参数 是否必填 说明 -g 是 生产者所属组名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查询当前属于group（消费者组）=simple-consumer-test的消费者的网络连接，nameserve地址为172.21.16.253:9876sh mqadmin consumerConnection -n 172.21.16.253:9876 -g simple-consumer-test 8.15 查看订阅组消费状态 指令： consumerProgress 类路径：com.alibaba.rocketmq.tools.command.consumer.ConsumerProgressSubCommand 参数 参数 是否必填 说明 -g 是 消费者所属组名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12# 查询当前属于group（消费者组）=simple-consumer-test的订阅状态，nameserve地址为172.21.16.253:9876sh mqadmin consumerProgress -n 172.21.16.253:9876 -g simple-consumer-test 8.16 查看集群消息 指令： clusterList 类路径：com.alibaba.rocketmq.tools.command.cluster.ClusterListSubCommand 参数 参数 是否必填 说明 -m 否 打印更多信息 (增加打印出如下信息 #InTotalYest, #OutTotalYest, #InTotalToday ,#OutTotalToday) -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 123# 查询当前集群状态，nameserve地址为172.21.16.253:9876sh mqadmin clusterList -n 172.21.16.253:9876 sh mqadmin clusterList -n 172.21.16.253:9876 –m 8.17 添加（更新）KV配置信息 指令： updateKvConfig 类路径：com.alibaba.rocketmq.tools.command.namesrv.UpdateKvConfigCommand 参数 参数 是否必填 说明 -k 是 key值 -v 是 value值 -s 是 Namespace值 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.18 删除KV配置信息 指令： deleteKvConfig 类路径：com.alibaba.rocketmq.tools.command.namesrv.DeleteKvConfigCommand 参数 参数 是否必填 说明 -k 是 key值 -s 是 Namespace值 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.19 添加（更新）Project group配置信息 指令： updateProjectGroup 类路径：com.alibaba.rocketmq.tools.command.namesrv.UpdateProjectGroupCommand 参数 参数 是否必填 说明 -i 是 服务器ip -p 是 project group名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.20 删除Project group配置信息 指令： deleteProjectGroup 类路径：com.alibaba.rocketmq.tools.command.namesrv.DeleteProjectGroupCommand 参数 参数 是否必填 说明 -i 是 服务器ip -p 是 project group名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.21 取得Project group配置信息 指令： getProjectGroup 类路径：com.alibaba.rocketmq.tools.command.namesrv.GetProjectGroupCommand 参数 参数 是否必填 说明 -i 是 服务器ip -p 是 project group名 -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 8.22 获取Consumer消费进度 该命令只打印当前与cluster连接的consumer的消费进度 指令： getConsumerStatus 类路径：com.alibaba.rocketmq.tools.command.offset.GetConsumerStatusCommand 参数 参数 是否必填 说明 -g 是 消费者所属组名 -t 是 查询主题 -i 否 Consumer客户端ip -h 否 打印帮助 -n 是 nameserve服务地址列表，格式ip:port;ip:port;… 举例 12#查询属于group（消费者组）=simple-consumer-test的消费者在Topic=ZTEExample上的消费状态，nameserve地址为172.21.16.253:9876sh mqadmin getConsumerStatus -n 172.21.16.253:9876 -g simple-consumer-test -t ZTEExample a-zA-Z0-9_- ↩︎ a-zA-Z0-9_- ↩︎","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"rocketmq","slug":"rocketmq","permalink":"https://xiaoyuge5201.github.io/tags/rocketmq/"}]},{"title":"Apache archiva Maven私有仓库搭建","slug":"Apache-archiva","date":"2022-06-28T03:49:42.000Z","updated":"2022-06-28T03:49:42.000Z","comments":false,"path":"Apache-archiva/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/Apache-archiva/index.html","excerpt":"","text":"1. 搭建 环境准备 JDK 1.8 Apache Archiva 2.2.8 Apache-maven 3.6.3 （ https://maven.apache.org/download.cgi ） Apache Archiva安装文件下载 123456archiva官网地址：https://archiva.apache.org/index.cgi#a下载地址：https://archiva.apache.org/download.cgi （这个太慢了，几K/s，坑）镜像仓库：https://mirrors.tuna.tsinghua.edu.cn/apache/archiva建议从镜像仓库拉取！！！ 将下载的tar.gz包上传到服务器相应位置 解压tar.gz包 12tar -zxvf ./apache-archiva-2.2.8-bin.tar.gz chmod 775 ./apache-archiva-2.2.8 修改端口（8080默认），如不需要，请跳过 启动 123cd /User/xiaoyuge/maven/apache-archiva-2.2.8/bin./archiva start #执行启动命令，建议第一次启动使用：./archiva console 可以打印启动信息./archiva stop #停止命令 各版本操作系统下详细安装及服务注册参照：http://archiva.apache.org/docs/2.2.8/adminguide/standalone.html 将 archiva在Centos中安装成服务(root执行) 1ln -sf /Users/xiaoyuge/maven/apache-archiva-2.2.8/bin/archiva /etc/init.d/archiva 这样就可以通过service启动 12service archiva startservice archiva stop 启动成功后，访问maven服务器地址: http://ip:8080 点击右上角的Create Admin User创建管理员账号 2. 上传私有jar包 访问：http://localhost:8080/#upload，上传私有jar包到仓库 Repository Id 选择 Archiva Managed Internal Repository则是把依赖作为正式版. 查看地址：http://host:port/repository/internal Repository Id 选择Archiva Managed Snapshot则是把依赖作为快照版. 查看地址：http://host:port/repository/snapshots 按照以下步骤依次操作： 保存后，提示以下信息表示上传成功！ 在左侧菜单栏Browse中查看上传的jar ，如下所示： 也可以通过命令的方式上传（需要配置maven 的setting.xml） 在/Users/xiaoyuge/Desktop有一个junit-4.13.2.jar，使用mvn deploy命令上传如下 1mvn deploy:deploy-file -Dfile=/Users/xiaoyuge/Desktop/junit-4.13.2.jar -DrepositoryId=archiva-releases -DgroupId=junit -DartifactId=junit -Dversion=4.13.2 -Durl=http://localhost:8080/repository/internal 命令解释： -Dfile ：要上传到私服的jar包， jar包全路径 -DrepositoryId： 仓库ID，要与maven 的setting.xml配置文件中的server一致，否则401； -DgroupId： groupId主包名 -DartifactId： 项目名 -Dversion：版本号 -Durl：远程仓库地址 上传结果如下如所示： 同时在私服仓库中可以查看到刚上传的jar 3. 项目使用 配置maven中的setting.xml文件，配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;settings xmlns=&quot;http://maven.apache.org/SETTINGS/1.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&quot;&gt; &lt;localRepository&gt;\\Common\\my_repository&lt;/localRepository&gt; &lt;servers&gt; &lt;server&gt; &lt;id&gt;archiva-releases&lt;/id&gt;&lt;!--要和mvn命令中的 -DrepositoryId 一致--&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;xiaoyuge0318&lt;/password&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;archiva-snapshots&lt;/id&gt;&lt;!--要和mvn命令中的 -DrepositoryId 一致--&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;xiaoyuge0318&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt; &lt;mirrors&gt; &lt;!-- 私服地址 start --&gt; &lt;mirror&gt; &lt;!-- 正式版 --&gt; &lt;id&gt;archiva-releases&lt;/id&gt; &lt;mirrorOf&gt;internal&lt;/mirrorOf&gt; &lt;url&gt;http://localhost:8080/repository/internal&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;!-- 快照版 --&gt; &lt;id&gt;archiva-snapshots&lt;/id&gt; &lt;mirrorOf&gt;snapshots&lt;/mirrorOf&gt; &lt;url&gt;http://localhost:8080/repository/snapshots&lt;/url&gt; &lt;/mirror&gt;true &lt;/mirrors&gt; &lt;profiles&gt; &lt;profile&gt; &lt;activation&gt; &lt;activeByDefault&gt;true&lt;/activeByDefault&gt; &lt;/activation&gt; &lt;repositories&gt; &lt;!-- 正式版 --&gt; &lt;repository&gt; &lt;id&gt;internal&lt;/id&gt; &lt;name&gt;Archiva Managed Internal Repository&lt;/name&gt; &lt;url&gt;http://localhost:8080/repository/internal&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;!-- 快照版 --&gt; &lt;repository&gt; &lt;id&gt;snapshots&lt;/id&gt; &lt;name&gt;Archiva Managed Snapshots Repository&lt;/name&gt; &lt;url&gt;http://localhost:8080/repository/snapshots&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;/profile&gt; &lt;/profiles&gt; &lt;!-- 私服地址END --&gt;&lt;/settings&gt;","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://xiaoyuge5201.github.io/tags/linux/"}]},{"title":"深入Tomcat源码学习","slug":"tomcat","date":"2022-06-26T01:48:09.000Z","updated":"2022-06-26T01:48:09.000Z","comments":false,"path":"tomcat/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/tomcat/index.html","excerpt":"","text":"1. Tomcat简介 Apache是web服务器，Tomcat是应用服务器，apache tomcat只是一个servlet容器，是Apache的扩展；Apache和Tomcat都可以做为独立的web服务器来运行，但是Apache不能解释java程序（jsp,servlet）。 两者都是一种容器，只不过发布的东西不同：Apache是html容器，功能像IIS一样；Tomcat是jsp/servlet容器，用于发布jsp及java的，类似的有IBM的websphere、BEA的Weblogic，sun的JRun等等。 打个比方：Apache是一辆卡车，上面可以装一些东西如html等。但是不能装水，要装水必须要有容器（桶），Tomcat就是一个桶（装像Java这样的水），而这个桶也可以不放在卡车上。 官网地址： https://tomcat.apache.org/ 1.1 网络架构图 1.2 web监听端口 DefaultServletSocketFactory.java 12345@Overridepublic ServerSocket createSocket (int port, int backlog, InetAddress ifAddress) throws IOException &#123; return new ServerSocket (port, backlog, ifAddress);&#125; 1.3 Servlet容器 找到Tomcat源码中对应一个web项目的类 Context.class 找到Tomcat源码 —&gt;web.xml文件对应的类 12&lt;Context path=&quot;/app&quot; doBase=&quot;E:\\\\app&quot;/&gt;&lt;Context path=&quot;/app1&quot; doBase=&quot;E:\\\\app1&quot;/&gt; 既然这段配置能够代表一个web项目在磁盘的访问路径，Context标签就是代表一个web项目 在tomcat官网中（https://tomcat.apache.org/tomcat-8.0-doc/architecture/overview.html）可以看到相应的文档说明 123A Context represents a web application. A Host may contain multiple contexts, each with a unique path. The Context interface may be implemented to create custom Contexts, but this is rarely the case because the StandardContext provides significant additional functionality.//大致意思：一个Context文表示web应用程序。一个主机可以包含多个Context，每个Context都有一个唯一的路径。上下文接口可以用来创建自定义Context，但这种情况很少发生，因为StandardContext提供了重要的附加功能。 那么在StandardContext中是如何加载这些项目的？ 123456789101112131415161718192021222324252627282930313233343536public boolean loadOnStartup(Container children[]) &#123; // Collect &quot;load on startup&quot; servlets that need to be initialized TreeMap&lt;Integer, ArrayList&lt;Wrapper&gt;&gt; map = new TreeMap&lt;&gt;(); for (int i = 0; i &lt; children.length; i++) &#123; Wrapper wrapper = (Wrapper) children[i]; int loadOnStartup = wrapper.getLoadOnStartup(); if (loadOnStartup &lt; 0) continue; Integer key = Integer.valueOf(loadOnStartup); ArrayList&lt;Wrapper&gt; list = map.get(key); if (list == null) &#123; list = new ArrayList&lt;&gt;(); map.put(key, list); &#125; list.add(wrapper); &#125; // Load the collected &quot;load on startup&quot; servlets for (ArrayList&lt;Wrapper&gt; list : map.values()) &#123; for (Wrapper wrapper : list) &#123; try &#123; wrapper.load(); &#125; catch (ServletException e) &#123; getLogger().error(sm.getString(&quot;standardWrapper.loadException&quot;, getName()), StandardWrapper.getRootCause(e)); // NOTE: load errors (including a servlet that throws // UnavailableException from tht init() method) are NOT // fatal to application startup, excepted if failDeploymentIfServletLoadedOnStartupFails is specified if(getComputedFailCtxIfServletStartFails()) &#123; return false; &#125; &#125; &#125; &#125; return true; &#125; 那这些Wrapper是否就是Servlet ? WebXml.java 1private static final StringManager sm = StringManager.getManager(Constants.PACKAGE_NAME); Contants 1public static final String WEB_XML_LOCATION = &quot;/WEB-INF/web.xml&quot;; ContextConfig 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212private void configureContext(WebXml webxml) &#123; // As far as possible, process in alphabetical order so it is easy to // check everything is present // Some validation depends on correct public ID context.setPublicId(webxml.getPublicId()); // Everything else in order context.setEffectiveMajorVersion(webxml.getMajorVersion()); context.setEffectiveMinorVersion(webxml.getMinorVersion()); for (Entry&lt;String, String&gt; entry : webxml.getContextParams().entrySet()) &#123; context.addParameter(entry.getKey(), entry.getValue()); &#125; context.setDenyUncoveredHttpMethods( webxml.getDenyUncoveredHttpMethods()); context.setDisplayName(webxml.getDisplayName()); context.setDistributable(webxml.isDistributable()); for (ContextLocalEjb ejbLocalRef : webxml.getEjbLocalRefs().values()) &#123; context.getNamingResources().addLocalEjb(ejbLocalRef); &#125; for (ContextEjb ejbRef : webxml.getEjbRefs().values()) &#123; context.getNamingResources().addEjb(ejbRef); &#125; for (ContextEnvironment environment : webxml.getEnvEntries().values()) &#123; context.getNamingResources().addEnvironment(environment); &#125; for (ErrorPage errorPage : webxml.getErrorPages().values()) &#123; context.addErrorPage(errorPage); &#125; for (FilterDef filter : webxml.getFilters().values()) &#123; if (filter.getAsyncSupported() == null) &#123; filter.setAsyncSupported(&quot;false&quot;); &#125; context.addFilterDef(filter); &#125; for (FilterMap filterMap : webxml.getFilterMappings()) &#123; context.addFilterMap(filterMap); &#125; context.setJspConfigDescriptor(webxml.getJspConfigDescriptor()); for (String listener : webxml.getListeners()) &#123; context.addApplicationListener(listener); &#125; for (Entry&lt;String, String&gt; entry : webxml.getLocaleEncodingMappings().entrySet()) &#123; context.addLocaleEncodingMappingParameter(entry.getKey(), entry.getValue()); &#125; // Prevents IAE if (webxml.getLoginConfig() != null) &#123; context.setLoginConfig(webxml.getLoginConfig()); &#125; for (MessageDestinationRef mdr : webxml.getMessageDestinationRefs().values()) &#123; context.getNamingResources().addMessageDestinationRef(mdr); &#125; // messageDestinations were ignored in Tomcat 6, so ignore here context.setIgnoreAnnotations(webxml.isMetadataComplete()); for (Entry&lt;String, String&gt; entry : webxml.getMimeMappings().entrySet()) &#123; context.addMimeMapping(entry.getKey(), entry.getValue()); &#125; // Name is just used for ordering for (ContextResourceEnvRef resource : webxml.getResourceEnvRefs().values()) &#123; context.getNamingResources().addResourceEnvRef(resource); &#125; for (ContextResource resource : webxml.getResourceRefs().values()) &#123; context.getNamingResources().addResource(resource); &#125; boolean allAuthenticatedUsersIsAppRole = webxml.getSecurityRoles().contains( SecurityConstraint.ROLE_ALL_AUTHENTICATED_USERS); for (SecurityConstraint constraint : webxml.getSecurityConstraints()) &#123; if (allAuthenticatedUsersIsAppRole) &#123; constraint.treatAllAuthenticatedUsersAsApplicationRole(); &#125; context.addConstraint(constraint); &#125; for (String role : webxml.getSecurityRoles()) &#123; context.addSecurityRole(role); &#125; for (ContextService service : webxml.getServiceRefs().values()) &#123; context.getNamingResources().addService(service); &#125; for (ServletDef servlet : webxml.getServlets().values()) &#123; Wrapper wrapper = context.createWrapper(); // Description is ignored // Display name is ignored // Icons are ignored // jsp-file gets passed to the JSP Servlet as an init-param if (servlet.getLoadOnStartup() != null) &#123; wrapper.setLoadOnStartup(servlet.getLoadOnStartup().intValue()); &#125; if (servlet.getEnabled() != null) &#123; wrapper.setEnabled(servlet.getEnabled().booleanValue()); &#125; wrapper.setName(servlet.getServletName()); Map&lt;String,String&gt; params = servlet.getParameterMap(); for (Entry&lt;String, String&gt; entry : params.entrySet()) &#123; wrapper.addInitParameter(entry.getKey(), entry.getValue()); &#125; wrapper.setRunAs(servlet.getRunAs()); Set&lt;SecurityRoleRef&gt; roleRefs = servlet.getSecurityRoleRefs(); for (SecurityRoleRef roleRef : roleRefs) &#123; wrapper.addSecurityReference( roleRef.getName(), roleRef.getLink()); &#125; wrapper.setServletClass(servlet.getServletClass()); MultipartDef multipartdef = servlet.getMultipartDef(); if (multipartdef != null) &#123; if (multipartdef.getMaxFileSize() != null &amp;&amp; multipartdef.getMaxRequestSize()!= null &amp;&amp; multipartdef.getFileSizeThreshold() != null) &#123; wrapper.setMultipartConfigElement(new MultipartConfigElement( multipartdef.getLocation(), Long.parseLong(multipartdef.getMaxFileSize()), Long.parseLong(multipartdef.getMaxRequestSize()), Integer.parseInt( multipartdef.getFileSizeThreshold()))); &#125; else &#123; wrapper.setMultipartConfigElement(new MultipartConfigElement( multipartdef.getLocation())); &#125; &#125; if (servlet.getAsyncSupported() != null) &#123; wrapper.setAsyncSupported( servlet.getAsyncSupported().booleanValue()); &#125; wrapper.setOverridable(servlet.isOverridable()); context.addChild(wrapper); &#125; for (Entry&lt;String, String&gt; entry : webxml.getServletMappings().entrySet()) &#123; context.addServletMapping(entry.getKey(), entry.getValue()); &#125; SessionConfig sessionConfig = webxml.getSessionConfig(); if (sessionConfig != null) &#123; if (sessionConfig.getSessionTimeout() != null) &#123; context.setSessionTimeout( sessionConfig.getSessionTimeout().intValue()); &#125; SessionCookieConfig scc = context.getServletContext().getSessionCookieConfig(); scc.setName(sessionConfig.getCookieName()); scc.setDomain(sessionConfig.getCookieDomain()); scc.setPath(sessionConfig.getCookiePath()); scc.setComment(sessionConfig.getCookieComment()); if (sessionConfig.getCookieHttpOnly() != null) &#123; scc.setHttpOnly(sessionConfig.getCookieHttpOnly().booleanValue()); &#125; if (sessionConfig.getCookieSecure() != null) &#123; scc.setSecure(sessionConfig.getCookieSecure().booleanValue()); &#125; if (sessionConfig.getCookieMaxAge() != null) &#123; scc.setMaxAge(sessionConfig.getCookieMaxAge().intValue()); &#125; if (sessionConfig.getSessionTrackingModes().size() &gt; 0) &#123; context.getServletContext().setSessionTrackingModes( sessionConfig.getSessionTrackingModes()); &#125; &#125; // Context doesn&#x27;t use version directly for (String welcomeFile : webxml.getWelcomeFiles()) &#123; /* * The following will result in a welcome file of &quot;&quot; so don&#x27;t add * that to the context * &lt;welcome-file-list&gt; * &lt;welcome-file/&gt; * &lt;/welcome-file-list&gt; */ if (welcomeFile != null &amp;&amp; welcomeFile.length() &gt; 0) &#123; context.addWelcomeFile(welcomeFile); &#125; &#125; // Do this last as it depends on servlets for (JspPropertyGroup jspPropertyGroup : webxml.getJspPropertyGroups()) &#123; String jspServletName = context.findServletMapping(&quot;*.jsp&quot;); if (jspServletName == null) &#123; jspServletName = &quot;jsp&quot;; &#125; if (context.findChild(jspServletName) != null) &#123; for (String urlPattern : jspPropertyGroup.getUrlPatterns()) &#123; context.addServletMapping(urlPattern, jspServletName, true); &#125; &#125; else &#123; if(log.isDebugEnabled()) &#123; for (String urlPattern : jspPropertyGroup.getUrlPatterns()) &#123; log.debug(&quot;Skiping &quot; + urlPattern + &quot; , no servlet &quot; + jspServletName); &#125; &#125; &#125; &#125; for (Entry&lt;String, String&gt; entry : webxml.getPostConstructMethods().entrySet()) &#123; context.addPostConstructMethod(entry.getKey(), entry.getValue()); &#125; for (Entry&lt;String, String&gt; entry : webxml.getPreDestroyMethods().entrySet()) &#123; context.addPreDestroyMethod(entry.getKey(), entry.getValue()); &#125; &#125; 2. Tomcat核心架构 每一层级对应的都是xml文件中的标签，以及源码中的实体类，其中有多层的图形表示可以存在多个 Tips: 亿图不充钱限制了组件个数，Context只画了一个！ 2.2 Tomcat组件 XML配置文件结构如下： 1234567891011&lt;Server&gt; &lt;!--顶层类元素：一个配置文件中只能有一个&lt;Server&gt;元素，可包含多个Service。--&gt; &lt;Service&gt; &lt;!--顶层类元素：本身不是容器，可包含一个Engine，多个Connector。--&gt; &lt;Connector/&gt; &lt;!--连接器类元素：代表通信接口。--&gt; &lt;Engine&gt; &lt;!--容器类元素：为特定的Service组件处理所有客户请求，可包含多个Host。--&gt; &lt;Host&gt; &lt;!--容器类元素：为特定的虚拟主机处理所有客户请求，可包含多个Context。--&gt; &lt;Context&gt; &lt;!--容器类元素：为特定的Web应用处理所有客户请求。--&gt; &lt;/Context&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 2.2.1 Server 代表整个Tomcat实例，在JVM中式单例的，它还负责管理包含Service组件的声明周期，下图式对Server组件的一个简单描述： 可以在Server.xml文件对server组件进行配置 配置属性有：name, shutdown, port, command, class name等 shutdown port默认为8005 shutdown command 默认为SHUTDOWN; 处于安全，只能从同一台服务器发出SHUTDOWN命令 提供JNDI的实现，可以放任意对象（如DataSource, 环境变量等）； 2.2.2 Service Service组件代表的式一组请求处理主键，一个Server实例可以包含多个Service实例，每个Service实例与一组Connector实例和单个Engine实例相关联( Service 是 Server 内部的中间组件，它将一个或多个 Connector 绑定到一个Engine 上。) 单Service实例一般够用了，如果需要针对不同的IP或者port使用不同的Service组件来处理，则可以使用多Service实例 2.2.3 Connector Connector组件把Engine从不同的通信协议中隔离出来，如HTTP, HTTPS, AJP等； 可以配置Tomcat的工作模式: Standalone &amp; Conjunction Standalone模式：tomcat可以配置HTTP/HTTPS的connector，它既要处理静态内容，也要委托Engine处理动态内容 Conjunction模式：客户端是Apache或者是IIS之类的WEB Server； 当Web Server决定将请求转交给Tomcat处理时，它通过AJP协议与Tomcat交互；AJP是基于二进制流的比HTTP更高效一些； 关于Connector的几个重要点： 监听的IP和port 处理请求的最大线程数，如果所有的线程都忙，则会丢弃新的请求 所有的Connector接收到请求后，转化成统一的模式，再交给唯一的Engine处理；Engine负责处理请i去并产生响应； Connector将Engine产生的响应按照合适的协议发送到客户端 常见Connector： http/1.1 http/2 ajp(apache jserv protocol) 专用于tomcat前端是apache反向代理的情况下 Tomcat既作为web服务器（解析http协议，响应客户端，静态；非处理动态（委托）），也作为应用程序服务器：请求来自于浏览器。 Tomcat应该考虑工作情形并为相应情形下的请求分别定义好需要的连接器才能正确接收来自于客户端的请求。 此处暂先介绍HTTP/1.1连接器的属性设置。ajp后文再做介绍。 HTTP连接器表示支持HTTP/1.1协议的组件。设置了该连接器就表示catalina启用它的独立web服务功能，当然，肯定也提供它必须的servlets和jsp执行功能。在一个service中可以配置一个或多个连接器，每个连接器都可以将请求转发给它们相关联的engine以处理请求、创建响应。 每个流入的请求都需要一个独立的线程来接收。当并发请求数量超出maxThreads指定的值时，多出的请求将被堆叠在套接字（socket）中，直到超出acceptCount指定的值。超出accpetCount的请求将以&quot;connection refused&quot;错误进行拒绝。 12345678910111213141516171819202122232425262728&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt;&lt;!--HTTP连接器的属性实在太多，详细配置方法见官方手册。通常定义HTTP连接器时必须定义的属性只有&quot;port&quot;。 --&gt;&lt;!-- address：指定连接器监听的地址，默认为所有地址，即0.0.0.0。 maxThreads：支持的最大并发连接数，默认为200；如果引用了executor创建的共享线程池，则该属性被忽略。 acceptCount：设置等待队列的最大长度；通常在tomcat所有处理线程均处于繁忙状态时，新发来的请求将被放置于等待队列中； maxConnections：允许建立的最大连接数。acceptCount和maxThreads是接受连接的最大线程数。存在一种情况，maxConnections小于acceptCount时，超出maxConnections的连接请求将被接收，但不会与之建立连接。 port：监听的端口，默认为0，此时表示随机选一个端口，通常都应该显式指定监听端口。 protocol：连接器使用的协议，用于处理对应的请求。默认为HTTP/1.1，此时它会自动在基于Java NIO或APR/native连接器之间进行切换。定义AJP协议时通常为AJP/1.3。 redirectPort：如果某连接器支持的协议是HTTP，当接收客户端发来的HTTPS请求时，则转发至此属性定义的端口。 connectionTimeout：等待客户端发送请求的超时时间，单位为毫秒，默认为60000，即1分钟；注意，这时候连接已经建立。 keepAliveTimeout：长连接状态的超时时间。超出该值时，长连接将关闭。 enableLookups：是否通过request.getRemoteHost()进行DNS查询以获取客户端的主机名；默认为true，应设置为false防止反解客户端主机；compression：是否压缩数据。默认为off。设置为on时表示只压缩text文本，设置为force时表示压缩所有内容。应该在压缩和sendfile之间做个权衡。 useSendfile：该属性为NIO（非阻塞IO）的属性，表示是否启用sendfile的功能。默认为true，启用该属性将会禁止compression属性。当协议指定为HTTP/1.1时，默认会自动在NIO/APR协议处理方式上进行按需切换。如要显式指定协议，方式如下： --&gt;&lt;connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;&gt;&lt;connector port=&quot;8080&quot; protocol=&quot;org.apache.coyote.http11.Http11NioProtocol&quot;&gt;&lt;connector port=&quot;8080&quot; protocol=&quot;org.apache.coyote.http11.Http11Nio2Protocol&quot;&gt;&lt;connector port=&quot;8080&quot; protocol=&quot;org.apache.coyote.http11.Http11AprProtocol&quot;&gt;&lt;!-- 其中NIO是C/C++的非阻塞IO复用模型在JAVA中的IO实现，NIO2即AIO是异步NIO，即异步非阻塞IO： NioProtocol ：non blocking Java NIO connector Nio2Protocol：non blocking Java NIO2 connector AprProtocol ：the APR/native connector --&gt; 多个属性的SSL连接服务器 1234&lt;Connector port=&quot;8443&quot; maxThreads=&quot;150&quot; minSpareThreads=&quot;25&quot; maxSpareThreads=&quot;75&quot; enableLookups=&quot;false&quot; acceptCount=&quot;100&quot; debug=&quot;0&quot; scheme=&quot;https&quot; secure=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; /&gt; 2.2.4 Engine ​ 其实就是Servlet Engine；一个service组件只能包含一个Engine组件；但是一个Engine可以包含多个Host组件；它接受代表请求和相应的对象，然后将工作委托给相应的host组件进行处理；如果没有找到对应的host组件，则委托给default host来处理； ​ Engine代表服务请求处理管道；由于Server可能有多个 Connector 连接器， Engine 负责接收并处理来自这些 Connector 的所有请求，并将响应返回给对应的 Connector，最终返回给客户端。 ​ Engine是service组件中用来分析协议的引擎机器，它从一个或多个connector上接收请求，并将请求交给对应的虚拟主机进行处理，最后返回完整的响应数据给connector，通过connector将响应数据返回给客户端。 只有一个engine元素必须嵌套在每个service中，且engine必须在其所需要关联的connector之后，这样在engine前面的connector都可以被此engine关联，而在engine后面的connector则被忽略，因为一个service中只允许有一个engine。 123456789101112&lt;!--定义--&gt;&lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt;&lt;/Engine&gt;&lt;Engine name=&quot;Standalone&quot; defaultHost=&quot;localhost&quot; jvmRoute=&quot;TomcatA&quot;&gt;&lt;/Engine&gt;&lt;!--常用的engine属性有： className：实现engine的类，该类必须实现org.apache.catalina.Engine接口。不给定该属性时将采用默认的标准类org.apache.catalina.core.StandardEngine。 defaultHost：指定处理请求的默认虚拟主机。在Engine中定义的多个虚拟主机的主机名称中至少有一个跟defaultHost定义的主机名称同名。 name：Engine组件的名称，用于记录日志和错误信息，无关紧要的属性，可随意给定。 jvmRoute(session+标识符，记录在服务端)：在启用session粘性时指定使用哪种负载均衡的标识符。所有的tomcat server实例中该标识符必须唯一，它会追加在session标识符的尾部，因此能让前端代理总是将特定的session转发至同一个tomcat实例上。(Session与cookie功能效果相同。Session与Cookie的区别在于Session是记录在服务端的,而Cookie是记录在客户端的。 )注意: jvmRoute同样可以使用jvmRoute的系统属性来设置。如果此处设置了jvmRoute，则覆盖jvmRoute系统属性。关于jvmRoute的使用，在后面tomcat ajp负载均衡的文章中介绍。engine是容器中的顶级子容器，其内可以嵌套一个或多个Host作为虚拟主机，且至少一个host要和engine中的默认虚拟主机名称对应。除了host，还可以嵌套releam和valve组件。--&gt; 2.2.5 Host ​ Host容器用来定义虚拟主机。engine从connector接收到请求进行分析后，会将相关的属性参数传递给对应的(筛选方式是从请求首部的host字段和虚拟主机名称进行匹配)虚拟host进行处理。如果没有合适的虚拟主机，则传递给默认虚拟主机。因此每个容器中必须至少定义一个虚拟主机，且必须有一个虚拟主机和engine容器中定义的默认虚拟主机名称相同; 123456789101112131415&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;Alias&gt;www.a.com&lt;/Alias&gt; &lt;!--Alias为Host指定的主机名定义主机别名--&gt;&lt;/Host&gt;&lt;!-- 常用属性说明： className：实现host容器的类，该类必须实现org.apache.catalina.Host接口。不给定该属性时将采用默认的标准类org.apache.catalina.core.StandardHost。 name：虚拟主机的主机名，忽略大小写(初始化时会自动转换为小写)。可以使用前缀星号通配符，如&quot;*.a.com&quot;。使用了星号前缀的虚拟主机的匹配优先级低于精确名称的虚拟主机。 appBase：此Host的webapps目录，即webapp部署在此虚拟主机上时的存放目录。包括非归档的web应用程序目录和归档后的WAR文件的目录。使用相对路径时基于$CATALINA_BASE。 xmlBase：部署在此虚拟主机上的context xml目录。 startStopThreads：启动context容器时的并行线程数。如果使用了自动部署功能，则再次部署或更新时使用相同的线程池。 autoDeploy：在Tomcat处于运行状态时放置于appBase目录中的应用程序文件是否自动进行deploy或自动更新部署状态。这等于同时开启了deployOnStartup属性和reload/redeploy webapp的功能。触发自动更新时将默认重载该webapp。默认为true。 unpackWars：在执行此webapps时是否先对归档格式的WAR文件解压再运行，设置为false时则直接执行WAR文件；默认为true。设置为false时会损耗性能。 workDir：该虚拟主机的工作目录。每个webapp都有自己的临时IO目录，默认该工作目录为$CATALINA_BASE/work。 大多数时候都只需设置虚拟主机名称name和appBase属性即可，其余采用默认，默认时会自动部署webapp--&gt; 两个重要点： domain name: 每个host必须要有一个唯一的domain name; 浏览器发过来的请求里包含有该domain name; domain name在Engine里必须是唯一的 app base folder: 发布到该host里的应用的目录名；可以是相对CATALINE_BASE的相对路径，也可以是文件系统的绝对路径 当host获得一个针对特定host请求时，将会在该Host环境下把请求匹配到对应的Context上；然后把请求交给这个Context来处理 2.2.6 Context ​ 一个Context对应一个web application； 它由多个servlet组成；在创建context时，将根据conf/web.xml和webapps/${context path}/WEB-INFO/web.xml加载servlet并创建映射表 Document Base: 存放war或解压后的context的地方 Context Path：唯一标志一个context;当没有匹配任何一个context时，默认的context将会处理该请求；默认的context的context path为空 Automatic reload: 一旦监测到context有修改，则会自动重启context，只用于开发模式； 2.2.7 Wrapper ​ Wrapper是context的子元素，代表了一个Servlet（或一个jsp被编译后的servlet）；它负责加载servlet、实例化servlet、以及触发生命周期方法的调用，如init()、service()、destory()；另外wrapper也负责调用与servlet相关的Filter。 2.2.8 嵌套组件 Excutor: 执行器，供 Connector 使用的线程池，可配置多个 cnnector自建，executer共享 执行器定义tomcat各组件之间共享的线程池。在以前，每个connector都会独自创建自己的线程池，但现在，可以定义一个线程池，各组件都可以共享该线程池，不过主要是为各connector之间提供共享。注意，executor创建的是共享线程池，如果某个connector不引用executor创建的线程池，那么该connector仍会根据自己指定的属性创建它们自己的线程池。 连接器必须要实现org.apache.catalina.Executor接口（server的classname，必须实现的接口）。它是一个嵌套在service组件中的元素，为了挑选所使用的connector，该元素还必须定义在connector元素之前。 1234567891011121314151617&lt;Executor name=&quot;tomcatThreadPool&quot; namePrefix=&quot;catalina-exec-&quot; maxThreads=&quot;150&quot; minSpareThreads=&quot;4&quot;/&gt;&lt;!-- className（默认）：用于实现此组件的java类的名称，这个类必须实现接口org.apache.catalina.Executor。不 给定该属性时将采用默认的标准类org.apache.catalina.core.StandardThreadExecutor； name：该线程池的名称，其他组件需要使用该名称引用该线程池。 ---------------------------------------------------------------标准类的属性包括：threadPriority：线程优先级，默认值为5。daemon：线程是否以daemon的方式运行，默认值为true。namePrefix：执行器创建每个线程时的名称前缀，最终线程的名称为:namePrefix+threadNumber。maxThreads：线程池激活的最大线程数量。默认值为200。minSpareThreads：线程池中最少空闲的线程数量。默认值为25。maxIdleTime：在空闲线程关闭前的毫秒数。除非激活的线程数量小于或等于minSpareThreads的值，否则会有空闲线程的出现。默认值为60000ms，即空闲线程需要保留1分钟的空闲时间才被杀掉。maxQueueSize：可执行任务的最大队列数，达到队列上限时的连接请求将被拒绝。prestartminSpareThreads：在启动executor时是否立即创建minSpareThreads个线程数，默认为false，即在需要时才创建线程。--&gt; connector中指定所使用的线程 1234&lt;Connector executor=&quot;tomcatThreadPool&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; Manager： 会话管理器：用于实现http会话管理的功能。 Loader： 类加载器 Valve： 阀门，Tomcat组件层面的过滤器 Resource：资源路径：配置 web 程序的资源信息，如数据库连接信息。 Realm：领域：用于用户的认证和授权。 Listener：监听器：监听已注册组件的生命周期。 Cluster： 集群：专用于配置 Tomcat 集群的元素。 2.2.8 container ​ container不是tomcat的组件，它是一个概念，统称；包含Engine、host、context、wrapper 12345Container(容器:包括以下所有组件)----Engine（分发用户请求）--------Host（主机）----------------Context（应用）--------------------Wrapper（Servlet） 2.2.9 Server.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt; &lt;Listener className=&quot;org.apache.catalina.startup.VersionLoggerListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.AprLifecycleListener&quot; SSLEngine=&quot;on&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.JreMemoryLeakPreventionListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.mbeans.GlobalResourcesLifecycleListener&quot; /&gt; &lt;Listener className=&quot;org.apache.catalina.core.ThreadLocalLeakPreventionListener&quot; /&gt; &lt;GlobalNamingResources&gt; &lt;Resource name=&quot;UserDatabase&quot; auth=&quot;Container&quot; type=&quot;org.apache.catalina.UserDatabase&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; pathname=&quot;conf/tomcat-users.xml&quot; /&gt; &lt;/GlobalNamingResources&gt; &lt;Service name=&quot;Catalina&quot;&gt; &lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; &lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; &lt;Realm className=&quot;org.apache.catalina.realm.LockOutRealm&quot;&gt; &lt;Realm className=&quot;org.apache.catalina.realm.UserDatabaseRealm&quot; resourceName=&quot;UserDatabase&quot;/&gt; &lt;/Realm&gt; &lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt; &lt;/Host&gt; &lt;/Engine&gt; &lt;/Service&gt;&lt;/Server&gt; 2.2 Tomcat请求处理过程 接收到用户HTTP请求： http://localhost:8080/app/login/auth1 请求被发送到本机端后8080，被在那里侦听的coyote HTTP/1.1 Connector获得 Connector把该请求交给它所在得service得Engine来处理，并等待engine得回应 Engine获得请求localhost/app/login/auth1，匹配它所拥有得所有虚拟主机host Engine获得请求到名为localhost得host（即使匹配不到也把请求交给该host处理，因为该host被定义为该Engine得默认主机） 名字为localhost的host主机获的请求/app/login/auth1，匹配它所拥有的所有context host匹配到路径为/app的context(如果匹配不到就把该请求交给路径名为“” 的context去处理) path=“/app”的context获得请求/login/auth1，在它的mapping table中寻找对应的servlet Context匹配到URL PATTERN为/auth1的servlet，对应于servlet类 构造HttpServletRequest对象和HttpServletResponse对象，作为参数调用Servlet的doGet或者doPost方法 Context把执行完了的HttpServletResponse对象发回给host Host把HttpServletResponse对象返回给Engine Engine把HttpServletResponse对象返回给Connnector Connector把HttpServletResponse对象返回给客户brower 3. Servlet规范 Java想要进行Web服务功能提供 当Http服务器接收请求后，如何知道调用哪些java类来处理请求呢？ 有些类可能就是用来封装变量的，有些类才是用来处理请求的。为了识别出那些具有处理请求的类，定义了一个接口，这个接口就叫Servlet接口，如果想要让业务类具备处理请求的能力，都必须实现这个接口，实现了接口的业务类叫做Servlet。 对于特定的请求，Http服务器如何知道由哪个Servlet来处理？Servlet又是由谁来实例化呢 于是又有了Servlet容器。Http服务器把请求交给Servlet容器去处理，Servlet容器会将请求转发到具体的Servlet,如果这个Servlet还没创建，就加载并实例化这个Servlet，然后调用这个Servlet的接口方法。 Http服务器不直接调用业务类，而是把请求交给容器来处理，容器通过Servlet接口调用业务类。因此Servlet接口和Servlet容器的出现，使Http服务器和业务类解耦。 Servlet规范：Servlet接口 + Servlet容器。 Tomcat按照Servlet规范的要求实现了Servlet容器，同时它也具有Http服务器的功能。（如果我们要实现新的业务功能，只需要实现一个Servlet，然后把它注册到Tomcat(Servlet容器)中，剩下的事情由Tomcat帮我们来处理）。 3.1 Servlet接口定义了五个方法 1234567891011public interface Servlet&#123; void init(ServletConfig config) throws ServletException; ServletConfig getServletConfig(); void service(ServletRequest req, ServletResponse res）throws ServletException, IOException; String getServletInfo(); void destroy();&#125; init(ServletConfig config)： 和生命周期有关的方法，Servlet容器在加载Servlet类的时候会调用init方法。可能会在init方法里初始化一些资源。比如Springmvc中的DispatcherServlet,在init方法中创建了自己的spring容器。 ServletConfig getServletConfig()： ServletConfig就是封装Servlet的初始化参数。可以在web.xml给Servlet配置参数，然后在程序中通过getServletConfig方法拿到这些参数。 service(ServletRequest req, ServletResponse res) 业务类在这个方法里实现处理逻辑。ServletRequest用来封装请求信息，ServletResponse用来封装响应信息。本质上这两个类是对通信协议的封装。Http协议中的请求和响应就是对应了HttpServletRequest和HttpServletResponse这两个类。我们可以通过HttpServletRequest来获取所有请求相关的信息，包括请求路径，Cookie，Http头，请求参数等。 String getServletInfo() destroy()： 和生命周期有关的方法，Servlet容器在卸载Servlet类的时候会调用destory方法。在destory方法里释放这些资源。 4. Servlet容器 4.1 Servlet容器工作流程 ​ 当客户请求某一个资源时，Http服务器会用一个ServletRequest对象把客户的请求信息封装起来，然后调用Servlet容器的service方法，Servlet容器拿到请求后，根据请求的URL和Servlet的映射关系，找到相应的Servlet，如果Servlet还没有被加载，就用反射机制创建这个Servlet，并调用Servlet的init方法来完成初始化，接着调用Servlet的service方法来处理请求，把ServletResponse对象返回给Http服务器，Http服务器会把响应发送给客户端。 4.2 Web应用 4.2.1 Servlet注册 Servlet容器负责实例化和调用Servlet，那么Servlet是怎么注册到Servlet容器的呢？ 我们一般以Web应用程序的方式来部署Servlet的。根据Servlet规范，Web应用程序有一定的目录结构： 12345| - MyWebApp | - WEB-INF/web.xml -- 配置文件，用来配置 Servlet 等 | - WEB-INF/lib/ -- 存放 Web 应用所需各种 JAR 包 | - WEB-INF/classes/ -- 存放你的应用类，比如 Servlet 类 | - META-INF/ -- 目录存放工程的一些信息 在这个目录下分别放置了Servlet的类文件，配置文件，静态资源文件，Servlet容器通过读取配置文件，就可以找到并加载Servlet。 4.2.2 ServletContext Servlet规范中定义了ServletContext这个接口来对应一个Web应用 Web应用部署好以后，Servlet容器在启动时会加载Web应用，并为每个Web应用创建唯一的ServletContext对象。你可以把ServletContext看成是一个全局对象，一个Web应用可能有多个Servlet，这些Servlet可以通过全局的ServletContext来共享数据，这些数据包括Web应用的初始化参数，Web应用目录下的文件资源等。因为ServletContext持有所有Servlet实例，还可以通过它来实现Servlet请求的转发。 4.2.3 扩展机制：Filter和Listener Filter：过滤器，这个接口允许对请求和响应做一些统一的定制化处理，比如可以根据请求的频率来限制访问，根据国家地区的不同来修改响应的内容。 过滤器原理：Web应用部署完以后，Servlet容器需要实例化Filter并把Filter链接成一个FilterChain。当请求进来时，获取第一个Filter并调用doFilter方法， doFilter方法负责调用 FilterChain的下一个Filter。 Listener：监听器，当Web应用在Servlet容器中运行时，Servlet容器内部会不断发生各种事件，比如Web应用的启动和停止，用户请求到达等。Servlet容器提供了一些默认的监听器来监听这些事件，当事件发生时，Servlet容器会负责调用监听器的方法。自定义监听器需要把监听器配置在web.xml中。比如：Spring就实现了自己的监听器，用来监听ServletContext的启动事件，目的是当Servlet容器启动时，创建并初始化全局的Spring容器。 5. 各种容器 Tomcat在启动时给每个Web应用创建一个全局的上下文环境，这个上下文就是ServletContext，为后面的Spring容器提供宿主环境。 Tomcat在启动过程中触发容器初始化事件，Spring的ContextLoaderListener会监听到这个事件，它的contextInitialized方法会被调用，然后Spring会初始化全局的Spring根容器，这个就是Spring的Ioc容器，Ioc容器初始化完毕后，Spring将其存储到ServletContext中，便于以后获取。 Tomcat启动时还会扫描Servlet，一个Web应用中的Servlet可以有多个，以SpringMvc中的DispatcherServet为例，这个Servlet实际上是一个标准的前端控制器，用来转发，匹配，处理每个Servlet请求。 Servlet一般会延迟加载，当第一个请求到达时，Tomcat发现DispatcherServet还没有被实例化，就调用DispatcherServet的init方法，DispatcherServet在初始化的时候会建立自己的容器，叫做SpringMvc容器，用来持有SpringMvc相关的Bean。同时，SpringMvc还会通过ServletContext拿到Spring根容器，并把Spring根容器设置为SpringMvc容器的父容器，Spring容器可以访问父容器中的Bean，但是父容器不能访问子容器中的Bean（Spring容器不能访问SpringMvc容器里的Bean —&gt;Controller里可以访问Service对象，但是在Service里不可以访问Controller对象）。 web容器中有servlet容器，spring项目部署后存在spring容器和springmvc容器。其中spring控制service层和dao层的bean对象。springmvc容器控制controller层bean对象。servlet容器控制servlet对象。项目启动是，首先 servlet初始化，初始化过程中通过web.xml中spring的配置加载spring配置，初始化spring容器和springmvc容器。待容器加载完成。servlet初始化完成，则完成启动。 HTTP请求到达web容器后，会到达Servlet容器，容器通过分发器分发到具体的spring的Controller层。执行业务操作后返回结果。","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"Apache","slug":"Apache","permalink":"https://xiaoyuge5201.github.io/tags/Apache/"}]},{"title":"Docker安装RabbitMQ集群","slug":"rabbitmq","date":"2022-06-25T08:53:22.000Z","updated":"2022-06-25T08:53:22.000Z","comments":false,"path":"rabbitmq/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/rabbitmq/index.html","excerpt":"","text":"1. 安装 Docker 更新yum源（如果你的网速慢这一步就别做了） 1sudo yum update 添加仓库 1sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 查看最新版本 12#如果之前安装了docker，需要卸载旧版本yum list docker-ce --showduplicates | sort -r 安装Docker CE版本 1yum install docker-ce -y 2. 安装 RabbitMQ 1个磁盘节点+2个内存节点 拉取RabbitMQ镜像（带managment） 1docker pull rabbitmq:3.7.17-management 创建docker网络（让容器可以和主机通信） 1docker network create rabbitmqnet 创建三个容器，端口分别是 5673 5674 5675 ，管理端口是 15673 15674 15675 12345678910#英文引号docker run -d \\ --name=rabbitmq1 \\ -p 5673:5672 \\ -p 15673:15672 \\ -e RABBITMQ_NODENAME=rabbitmq1 \\ -e RABBITMQ_ERLANG_COOKIE=&#x27;GUPAOEDUFORBETTERYOU&#x27; \\ -h rabbitmq1 \\ --net=rabbitmqnet \\ rabbitmq:management 123456789docker run -d \\ --name=rabbitmq2 \\ -p 5674:5672 \\ -p 15674:15672 \\ -e RABBITMQ_NODENAME=rabbitmq1 \\ -e RABBITMQ_ERLANG_COOKIE=&#x27;GUPAOEDUFORBETTERYOU&#x27; \\ -h rabbitmq2 \\ --net=rabbitmqnet \\ rabbitmq:management 123456789docker run -d \\ --name=rabbitmq3 \\ -p 5675:5672 \\ -p 15675:15672 \\ -e RABBITMQ_NODENAME=rabbitmq1 \\ -e RABBITMQ_ERLANG_COOKIE=&#x27;GUPAOEDUFORBETTERYOU&#x27; \\ -h rabbitmq3 \\ --net=rabbitmqnet \\ rabbitmq:management 后两个节点作为内存节点加入集群 12345docker exec -it rabbitmq2 /bin/bash #进入容器内部rabbitmqctl stop_app #停止rabbitmq 服务 rabbitmqctl reset #重置rabbitmq rabbitmqctl join_cluster --ram rabbitmq1@rabbitmq1 #加入集群rabbitmqctl start_app #启动服务 12345docker exec -it rabbitmq3 /bin/bashrabbitmqctl stop_apprabbitmqctl resetrabbitmqctl join_cluster --ram rabbitmq1@rabbitmq1rabbitmqctl start_app 访问： http://ip:15673/; guest/guest登录 3. 常见问题 [error] Bad characters in cookie 12到创建容器的时候，docker run -d --name=rabbitmq1 -p 5673:5672 -p 15673:15672 -e RABBITMQ_NODENAME=rabbitmq1 -e RABBITMQ_ERLANG_COOKIE=‘GoodGoodStudyDayDayUp’ -h rabbitmq1 --net=rabbitmqnet rabbitmq:management由于Cookie的引号是中文引号，所以docker ps -a 时看到Status为Exited 解决方法：移除容器，把cookie的引号改为英文引号再执行就可以了","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"https://xiaoyuge5201.github.io/tags/rabbitMQ/"}]},{"title":"分库分表之 ShardingSphere-JDBC","slug":"sharding-jdbc","date":"2022-06-25T06:52:23.000Z","updated":"2022-06-25T06:52:23.000Z","comments":false,"path":"sharding-jdbc/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/sharding-jdbc/index.html","excerpt":"","text":"1. 动态数据源解决方案 随着业务数据量逐渐增大，带来存储的瓶颈以及查询瓶颈，我们可以将数据存放到多个数据服务中，以达到减轻数据库压力，缩短数据库操作时间； 目前关于动态数据源的解决方案大致包含以下5种，今天主要是学习一下Sharding-jdbc； 关于Mycat请查看 mycat学习 这篇博客。 2. ShardingSphere简介 2018年5月，sharding-jdbc更名为ShardingSphere 2018年11月，Sharding-sphere正式进入Apache基金会孵化器 2020年4月，从Apache孵化器毕业，成为Apache顶级项目Apache ShardingSphere ShardingSphere目前的定位已远超过人们熟知的分库分表的功能了，其拥有自己的⼀套开源的分布式数据库中间件解决⽅案组成的⽣态圈（ShardingSphere-JDBC、ShardingSphere-Proxy，ShardingSphere-sidecar（计划中）） 他们均提供标准化的数据分片、分布式事务和数据库治理功能，可适用于如Java同构、异构语言、容器、云原生等各种多样化的应用场景 官网地址：https://shardingsphere.apache.org/document/current/cn/overview/ 2.1 Apache ShardingSphere 官方定义：Apache ShardingSphere 是一套开源的分布式数据库增强计算引擎，其通过可插拔架构构建基于数据库之上的生态系统，实现包括数据分片、弹性伸缩、加密脱敏等功能为代表的增强能力 定位Apache ShardingSphere 产品定位为Database Plus，旨在构建异构数据库上层的标准和生态。 它关注如何充分合理地利用数据库的计算和存储能力，而并非实现一个全新的数据库 2.2 ShardingSphere-JDBC 定位于轻量级Java框架，在Java的jdbc层提供的额外服务， ShardingSphere-jdbc的主要功能在客户端尽心数据分片和读写分离，通过sharding-jdbc,应用可以使用jdbc访问已经读写分离的多个数据源，而不用关心数据库数量和数据的分布； 可以理解为增强版JDBC驱动，完全兼容JDBC和各种ORM框架 使用于任何给予Java的ORM框架，如：JPA、hibernate、mybatis、Spring JDBC Template或直接使用JDBC 基于任何第三方的数据库连接池，如：DBCP、C3P0、BoneCP、Druid、HikariCP等 支持任意实现JDBC规范的数据库，如Mysql、Oracle、SQLServer、PostgreSQL 源码：https://github.com/apache/shardingsphere/tree/master/shardingsphere-jdbc 2.3 ShardingSphere-Proxy 定位为透明化的数据库代理端，提供封装了数据库二进制协议的服务端版本，用于完成对异构语言的支持。 目前提供 MySQL 和 PostgreSQL（兼容 openGauss 等基于 PostgreSQL 的数据库）版本，它可以使用任何兼容 MySQL/PostgreSQL 协议的访问客户端（如：MySQL Command Client, MySQL Workbench, Navicat 等）操作数据，对 DBA 更加友好 向应用程序完全透明，可直接当做 MySQL/PostgreSQL 使用； 适用于任何兼容 MySQL/PostgreSQL 协议的的客户端。 源码：https://github.com/apache/shardingsphere/tree/master/shardingsphere-proxy 2.4 ShardingSphere-sidecar（TODO） 定位为 Kubernetes 的云原生数据库代理，以 Sidecar 的形式代理所有对数据库的访问。 通过无中心、零侵入的方案提供与数据库交互的啮合层，即 Database Mesh，又可称数据库网格。 3. 分片的核心概念 主要概念 由原来的一个数据库(表)拆分为真实存在的三个数据库(表) 逻辑表会在 SQL 解析和路由时被替换成真实的表名，分片键就是拆分的逻辑；sharding-jdbc可以选择多个分片键； 动态表 表名会变化，比如订单表，按照月份进行分表 绑定表 与Mycat的ER表对应，存在关联关系的两张表，他们的分片规则必须相同 广播表 与Mycat的全局表对应，所有节点的数据内容一致 4. Sharding-JDBC Demo演示 Apache ShardingSphere-JDBC 可以通过 Java，YAML，Spring 命名空间 和 Spring Boot Starter 这 4 种方式进行配置，开发者可根据场景选择适合的配置方式。 目前仅支持Java语言且java JRE 8或更高版本 下面使用SSM框架来集成shardingsphere-jdbc，操作数据库，由于资源有限，在一个数据库instance里面创建两个数据库db0、db1来模拟分库分表； 创建数据库db0,db1，以及创建下面的数据表 12345678910111213141516171819202122232425262728293031323334353637-- 用户表 CREATE TABLE `user_info` ( `user_id` bigint(128) NOT NULL, `user_name` varchar(45) DEFAULT NULL, `account` varchar(45) NOT NULL, `password` varchar(45) DEFAULT NULL, PRIMARY KEY (`user_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; -- 订单表 CREATE TABLE `t_order` ( `order_id` int(11) NOT NULL, `user_id` int(11) NOT NULL, PRIMARY KEY (`order_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; -- 订单明细表 CREATE TABLE `t_order_item` ( `item_id` int(11) NOT NULL, `order_id` int(11) NOT NULL, `user_id` int(11) NOT NULL, PRIMARY KEY (`item_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; -- 参数配置表 CREATE TABLE `t_config` ( `config_id` int(16) NOT NULL AUTO_INCREMENT, `para_name` varchar(255) DEFAULT NULL, `para_value` varchar(255) DEFAULT NULL, `para_desc` varchar(255) DEFAULT NULL, PRIMARY KEY (`config_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; truncate table user_info; truncate table t_order; truncate table t_order_item; truncate table t_config; 创建项目并引入依赖 1234567891011 &lt;!-- https://mvnrepository.com/artifact/org.apache.shardingsphere/shardingsphere-jdbc-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;shardingsphere-jdbc-core&lt;/artifactId&gt; &lt;version&gt;5.1.2&lt;/version&gt; &lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shardingsphere&lt;/groupId&gt; &lt;artifactId&gt;sharding-jdbc-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;4.1.1&lt;/version&gt; &lt;/dependency&gt; 使用mybatisPlus逆向工程生成entity、mapper、service等相关代码，这里就不贴业务代码了。 编辑application.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mybatis: mapper-locations: classpath:mapper/*.xml config-location: classpath:mybatis-config.xmlspring: shardingsphere: props: sql: show: true #打印sql语句 datasource: #数据源配置 names: db0,db1 db0: #数据源1 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/db1?serverTimezone=UTC&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8 username: root password: xiaoyuge db1: #数据源2 type: com.alibaba.druid.pool.DruidDataSource driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/db1?serverTimezone=UTC&amp;useSSL=false&amp;useUnicode=true&amp;characterEncoding=UTF-8 username: root password: xiaoyuge sharding: #分片 default-database-strategy: #数据库分片策略 inline: sharding-column: user_id #分片键 algorithm-expression: db$&#123;user_id % 2&#125; #分片算法 tables: #表分片策略 user_info: actual-data-nodes: db$-&gt;&#123;0..1&#125;.user_info #真实存储数据的节点，可以使用行内表达式,$-&gt;&#123;&#125; 是标准语法 databaseStrategy: inline: shardingColumn: user_id algorithm-expression: db$&#123;user_id % 2&#125; #分片算法 key-generator: column: user_id type: SNOWFLAKE #主键策略：SNOWFLAKE 、 UUID，如果设置了主键策略，那么插入的时候就不用指定主键的值 t_order: databaseStrategy: inline: shardingColumn: order_id algorithm-expression: db$&#123;order_id % 2&#125; #分片算法 actual-data-nodes: db$-&gt;&#123;0..1&#125;.t_order t_order_item: databaseStrategy: inline: shardingColumn: order_id algorithm-expression: db$&#123;order_id % 2&#125; #分片算法 actual-data-nodes: db$-&gt;&#123;0..1&#125;.t_order_item binding-tables[0]: t_order,t_order_item #绑定表配置 broadcast-tables: t_config #广播表配置 测试简单分库user_info 1234567891011121314151617181920212223242526272829303132333435363738394041import com.ygb.entity.UserInfo;import com.ygb.service.UserService;import org.junit.Test;import org.junit.runner.RunWith;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;import javax.annotation.Resource;import java.util.ArrayList;import java.util.List;@RunWith(SpringRunner.class)@SpringBootTest@MapperScan(basePackages = &quot;com.ygb.mapper&quot;)public class UserShardingTest &#123; @Resource UserService userService; @Test public void insert() &#123; //随机生成100条数据,插入到数据库中 for (int i = 1; i &lt;= 100; i++) &#123; UserInfo userInfo = new UserInfo(); userInfo.setAccount(&quot;account&quot; + i); userInfo.setPassword(&quot;password&quot; + i); userInfo.setUserName(&quot;name&quot; + i); userInfo.setUserId(i); userService.insert(userInfo); &#125; &#125; @Test public void select() &#123; UserInfo userInfo1 = userService.getUserInfoByUserId(1L); System.out.println(&quot;------userInfo1:&quot; + userInfo1); UserInfo userInfo2 = userService.getUserInfoByUserId(2L); System.out.println(&quot;------userInfo2:&quot; + userInfo2); &#125;&#125; 执行 insert方法后，结果如下图所示: 根据上面yaml配置user_info表按照分片键(sharding-column)user_id取模分片到不同数据库;user_id % 2 为偶数落下db0, 奇数落在db1 执行 select 方法，查询user_id为1 的数据，可以看到逻辑sql语句以及真实的sql语句路由到哪个节点上。 同理，查询user_id 为 2 的数据，根据表的分库规则，user_id % 2 == 0数据落在db0节点上； 测试绑定表t_order,t_order_item 12345678910111213141516@Testpublic void insert()&#123; for (int i = 1; i &lt;= 100; i++) &#123; //订单表 Order order = new Order(); order.setOrderId(i); order.setUserId(i); orderService.insert(order); //订单明细表 OrderItem orderItem = new OrderItem(); orderItem.setItemId(i); orderItem.setOrderId(i); orderItem.setUserId(i); orderItemService.insert(orderItem); &#125;&#125; 执行insert方法后，数据库表结果如下图所示： t_order表和t_order_item 使用相同的分片规则db$&#123;order_id % 2&#125;,相同的order_id分布在同一个节点上； 测试广播表t_config 1234567891011@Testpublic void insert()&#123; for (int i = 1; i &lt;= 10; i++) &#123; Config config = new Config(); config.setConfigId(i); config.setParaName(&quot;name&quot;+i); config.setParaValue(&quot;value&quot;+i); config.setParaDesc(&quot;desc&quot;+i); configService.insert(config); &#125;&#125; 执行insert方法后，查看执行日志，可以看到它向db0、db1两个节点分别发送了插入语句； 5. ShardingSphere分库分表原理剖析 基于上面的测试代码来分析ShardingSphere分库分表原理； 5.1 ShardingSphere-JDBC工作流程 1SQL 解析(解析引擎) =&gt; 执行器优化 =&gt; SQL 路由(路由引擎) =&gt; SQL 改写(改写引擎) =&gt; SQL 执行(执行引擎) =&gt; 结果归并(归并引擎) SQL解析 SQL 解析主要是词法和语法的解析。目前常见的 SQL 解析器主要有fdb，jsqlparser 和 Druid。Sharding-JDBC1.4.x之前的版本使用Druid作为SQL解析器。从 1.5.x 版本 开始，Sharding-JDBC采用完全自研的SQL解析引擎 SQL 路由 SQL 路由是根据分片规则配置以及解析上下文中的分片条件，将 SQL 定位至真正的 数据源。它又分为直接路由、标准路由和笛卡尔积路由 直接路由: 使用 Hint 方式。 标准路由：Binding 表是指使用同样的分片键和分片规则的一组表，也就是说任何情况下， Binding 表的分片结果应与主表一致。例如：order 表和 order_item 表，都根据 order_id 分片，结果应是 order_1 与 order_item_1 成对出现。这样的关联查询和单表查询复杂度 和性能相当。如果分片条件不是等于，而是 BETWEEN 或 IN，则路由结果不一定落入单 库（表），因此一条逻辑 SQL 最终可能拆分为多条 SQL 语句。 笛卡尔积路由：笛卡尔积查询最为复杂，因为无法根据 Binding 关系定位分片规则的一致性，所以 非 Binding 表的关联查询需要拆解为笛卡尔积组合执行。查询性能较低，而且数据库连 接数较高，需谨慎使用。 SQL改写 将逻辑表名称改成真实表名称，优化分页查询等 SQL执行 因为可能链接到多个真实数据源， Sharding -JDBC 将采用多线程并发执行 SQL SQL归并 如数据的组装、分页、排序等等。 下面我跟着源码学习一下完整的执行过程。 5.2 配置加载过程 首先由于我们引入sharding-jdbc-spring-boot-starter依赖，在依赖包中可以看到shardingSphere支持springboot,那么它肯定会有一个类似于SpringBoot自动装配类 SpringBootConfiguration这样的配置类； 查看一下源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/** * Spring boot starter configuration. */@Configuration@ComponentScan(&quot;org.apache.shardingsphere.spring.boot.converter&quot;)@EnableConfigurationProperties(&#123; SpringBootShardingRuleConfigurationProperties.class, SpringBootMasterSlaveRuleConfigurationProperties.class, SpringBootEncryptRuleConfigurationProperties.class, SpringBootPropertiesConfigurationProperties.class, SpringBootShadowRuleConfigurationProperties.class&#125;)@ConditionalOnProperty(prefix = &quot;spring.shardingsphere&quot;, name = &quot;enabled&quot;, havingValue = &quot;true&quot;, matchIfMissing = true)@AutoConfigureBefore(DataSourceAutoConfiguration.class) //@AutoConfigureBefore自动装配在DataSourceAutoConfiguration装配类之前完成，也就是说Shardingsphere创建得数据源就是全局得数据源，项目只要涉及到对数据库得任何操作都会经过ShardingDataSource得这一层处理@RequiredArgsConstructorpublic class SpringBootConfiguration implements EnvironmentAware &#123; //将之前配置得规则映射到此配置文件中，为创建数据源得过程提供配置信息 private final SpringBootShardingRuleConfigurationProperties shardingRule; private final SpringBootMasterSlaveRuleConfigurationProperties masterSlaveRule; private final SpringBootEncryptRuleConfigurationProperties encryptRule; private final SpringBootShadowRuleConfigurationProperties shadowRule; private final SpringBootPropertiesConfigurationProperties props; //对象存放得是配置得所有数据源映射信息，为后面获取数据库连接以及数据分片提供基础能力 private final Map&lt;String, DataSource&gt; dataSourceMap = new LinkedHashMap&lt;&gt;(); private final String jndiName = &quot;jndi-name&quot;; /** * 上面的测试代码基于分片的 策略配置，所以只有 ShardingRuleCondition 才满足装配条件。 * ShardingDataSourceFactory工厂类来创建 ShardingDataSource数据源， * Get sharding data source bean. * 条件注入不同的数据源 * @return data source bean * @throws SQLException SQL exception */ @Bean @Conditional(ShardingRuleCondition.class) public DataSource shardingDataSource() throws SQLException &#123; //配置转换过程， return ShardingDataSourceFactory.createDataSource(dataSourceMap, new ShardingRuleConfigurationYamlSwapper().swap(shardingRule), props.getProps()); &#125; /** * Get master-slave data source bean. * 条件注入不同的数据源 * @return data source bean * @throws SQLException SQL exception */ @Bean @Conditional(MasterSlaveRuleCondition.class) public DataSource masterSlaveDataSource() throws SQLException &#123; return MasterSlaveDataSourceFactory.createDataSource(dataSourceMap, new MasterSlaveRuleConfigurationYamlSwapper().swap(masterSlaveRule), props.getProps()); &#125; /** * Get encrypt data source bean. *条件注入不同的数据源 * @return data source bean * @throws SQLException SQL exception */ @Bean @Conditional(EncryptRuleCondition.class) public DataSource encryptDataSource() throws SQLException &#123; return EncryptDataSourceFactory.createDataSource(dataSourceMap.values().iterator().next(), new EncryptRuleConfigurationYamlSwapper().swap(encryptRule), props.getProps()); &#125; /** * Get shadow data source bean. * 条件注入不同的数据源 * @return data source bean * @throws SQLException SQL exception */ @Bean @Conditional(ShadowRuleCondition.class) public DataSource shadowDataSource() throws SQLException &#123; return ShadowDataSourceFactory.createDataSource(dataSourceMap, new ShadowRuleConfigurationYamlSwapper().swap(shadowRule), props.getProps()); &#125; /** * Create sharding transaction type scanner * * @return sharding transaction type scanner */ @Bean public ShardingTransactionTypeScanner shardingTransactionTypeScanner() &#123; return new ShardingTransactionTypeScanner(); &#125; @Override public final void setEnvironment(final Environment environment) &#123; String prefix = &quot;spring.shardingsphere.datasource.&quot;; for (String each : getDataSourceNames(environment, prefix)) &#123; try &#123; //遍历环境变量，将数据源保存到 dataSourceMap dataSourceMap.put(each, getDataSource(environment, prefix, each)); &#125; catch (final ReflectiveOperationException ex) &#123; throw new ShardingSphereException(&quot;Can&#x27;t find datasource type!&quot;, ex); &#125; catch (final NamingException namingEx) &#123; throw new ShardingSphereException(&quot;Can&#x27;t find JNDI datasource!&quot;, namingEx); &#125; &#125; &#125; //----------------------------省略------------------------------&#125; ShardingDataSourceFactory工厂类创建ShardingDataSource数据源 1234567891011121314151617181920public class ShardingDataSource extends AbstractDataSourceAdapter &#123; private final ShardingRuntimeContext runtimeContext; static &#123; //初始化路由装饰器（路由引擎，SPI方式） NewInstanceServiceLoader.register(RouteDecorator.class); //创建SQL改写上下文装饰器（改写引擎，SPI方式） NewInstanceServiceLoader.register(SQLRewriteContextDecorator.class); // 创建结果处理引擎（归并引擎，用于对查询结果合并处理，同上） NewInstanceServiceLoader.register(ResultProcessEngine.class); &#125; public ShardingDataSource(Map&lt;String, DataSource&gt; dataSourceMap, ShardingRule shardingRule, Properties props) throws SQLException &#123; super(dataSourceMap); this.checkDataSourceType(dataSourceMap); //创建运行时上下文（全局分片运行时上下文，用于保存分片所需得相关配置） this.runtimeContext = new ShardingRuntimeContext(dataSourceMap, shardingRule, props, this.getDatabaseType()); &#125; //----------------------------省略------------------------------&#125; ShardingRule配置规则解析类 配置转换过程。会将分表规则、分库规则、分表算法、分库算法等都解析到对应得 ShardingRuleConfiguration 通用分片配置类中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public ShardingRuleConfiguration swap(YamlShardingRuleConfiguration yamlConfiguration) &#123; ShardingRuleConfiguration result = new ShardingRuleConfiguration(); Iterator var3 = yamlConfiguration.getTables().entrySet().iterator(); while(var3.hasNext()) &#123; //将我们配置得分库、分表策略、分库算法、分表算法解析到TableRuleConfiguration中，每一张表都会对应一个配置类 Entry&lt;String, YamlTableRuleConfiguration&gt; entry = (Entry)var3.next(); YamlTableRuleConfiguration tableRuleConfig = (YamlTableRuleConfiguration)entry.getValue(); tableRuleConfig.setLogicTable((String)entry.getKey()); result.getTableRuleConfigs().add(this.tableRuleConfigurationYamlSwapper.swap(tableRuleConfig)); &#125; result.setDefaultDataSourceName(yamlConfiguration.getDefaultDataSourceName()); //绑定表 result.getBindingTableGroups().addAll(yamlConfiguration.getBindingTables()); //广播表 result.getBroadcastTables().addAll(yamlConfiguration.getBroadcastTables()); if (null != yamlConfiguration.getDefaultDatabaseStrategy()) &#123; result.setDefaultDatabaseShardingStrategyConfig(this.shardingStrategyConfigurationYamlSwapper.swap(yamlConfiguration.getDefaultDatabaseStrategy())); &#125; if (null != yamlConfiguration.getDefaultTableStrategy()) &#123; result.setDefaultTableShardingStrategyConfig(this.shardingStrategyConfigurationYamlSwapper.swap(yamlConfiguration.getDefaultTableStrategy())); &#125; if (null != yamlConfiguration.getDefaultKeyGenerator()) &#123; result.setDefaultKeyGeneratorConfig(this.keyGeneratorConfigurationYamlSwapper.swap(yamlConfiguration.getDefaultKeyGenerator())); &#125; Collection&lt;MasterSlaveRuleConfiguration&gt; masterSlaveRuleConfigs = new LinkedList(); Iterator var9 = yamlConfiguration.getMasterSlaveRules().entrySet().iterator(); while(var9.hasNext()) &#123; Entry&lt;String, YamlMasterSlaveRuleConfiguration&gt; entry = (Entry)var9.next(); YamlMasterSlaveRuleConfiguration each = (YamlMasterSlaveRuleConfiguration)entry.getValue(); each.setName((String)entry.getKey()); masterSlaveRuleConfigs.add(this.masterSlaveRuleConfigurationYamlSwapper.swap((YamlMasterSlaveRuleConfiguration)entry.getValue())); &#125; result.setMasterSlaveRuleConfigs(masterSlaveRuleConfigs); if (null != yamlConfiguration.getEncryptRule()) &#123; result.setEncryptRuleConfig(this.encryptRuleConfigurationYamlSwapper.swap(yamlConfiguration.getEncryptRule())); &#125; return result;&#125; 1SpringBootConfiguration -&gt; ShardingDataSourceFactory -&gt; ShardingRule -&gt; ShardingDataSource -&gt; ShardingRuntimeContext 5.3 分片运行时上下文创建过程 创建数据源的时候会在构造器中将运行时上下文ShardingRuntimeContext一同创建出来，ShardingRuntimeContext得构造器如下图 123456789101112public final class ShardingRuntimeContext extends MultipleDataSourcesRuntimeContext&lt;ShardingRule&gt; &#123; private final CachedDatabaseMetaData cachedDatabaseMetaData; private final ShardingTransactionManagerEngine shardingTransactionManagerEngine; public ShardingRuntimeContext(Map&lt;String, DataSource&gt; dataSourceMap, ShardingRule shardingRule, Properties props, DatabaseType databaseType) throws SQLException &#123; super(dataSourceMap, shardingRule, props, databaseType); this.cachedDatabaseMetaData = this.createCachedDatabaseMetaData(dataSourceMap); this.shardingTransactionManagerEngine = new ShardingTransactionManagerEngine(); this.shardingTransactionManagerEngine.init(databaseType, dataSourceMap); &#125; //----------------- 省略------------&#125; 类关系图如下： 发现运行时上下文进行了抽象，分片运行时上下文继承了MultipleDataSourcesRuntimeContext 多数据源运行时上下文，而多数据源运行时上下文又继承了 AbstractRuntimeContext 抽象上下文。而创建 ShardingRuntimeContext 分片运行时上下文得时候会同时将分片规则保存在抽象类中 123456789101112131415161718192021public abstract class AbstractRuntimeContext&lt;T extends BaseRule&gt; implements RuntimeContext&lt;T&gt; &#123; private final T rule; private final ConfigurationProperties properties; private final DatabaseType databaseType; private final ExecutorEngine executorEngine; private final SQLParserEngine sqlParserEngine; protected AbstractRuntimeContext(T rule, Properties props, DatabaseType databaseType) &#123; this.rule = rule; //1. 缓存整个分片规则，为后续的分片操作提供依据 this.properties = new ConfigurationProperties(null == props ? new Properties() : props); this.databaseType = databaseType; //2. 缓存数据库类型，用于后续执行的时候加载对应的数据库元数据 //3.创建执行引擎，根据当前执行连接是否持有事物来决定是否异步执行，根据配置的executor.size 参数决定创建多少个线程的线程池，默认不配置得话，使用 cachepool，配置了就使用固定线程数得线程池 this.executorEngine = new ExecutorEngine((Integer)this.properties.getValue(ConfigurationPropertyKey.EXECUTOR_SIZE)); //解析引擎，用于解析SQL为抽象语法树，解析过程分为词法解析和语法解析。从3.0之后解析会全面替换为 ANTLR this.sqlParserEngine = SQLParserEngineFactory.getSQLParserEngine(DatabaseTypes.getTrunkDatabaseTypeName(databaseType)); ConfigurationLogger.log(rule.getRuleConfiguration()); ConfigurationLogger.log(props); &#125; //----------------- 省略------------&#125; 1ShardingRuntimeContext-&gt; MultipleDataSourcesRuntimeContext -&gt; AbstractRuntimeContext-&gt; ExecutorEngine-&gt; SQLParserEngine 5.4 分片处理过程 由于测试代码使用的是Mybatis层，这里只是对Mybatis处理流程进行分析。 当一个查询sql执行时，首先经过Mybatis层 调用org.apache.ibatis.executor.BaseExecutor # queryFromDatabase方法 123456789101112131415private &lt;E&gt; List&lt;E&gt; queryFromDatabase(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException &#123; this.localCache.putObject(key, ExecutionPlaceholder.EXECUTION_PLACEHOLDER); List list; try &#123; list = this.doQuery(ms, parameter, rowBounds, resultHandler, boundSql); &#125; finally &#123; this.localCache.removeObject(key); &#125; this.localCache.putObject(key, list); if (ms.getStatementType() == StatementType.CALLABLE) &#123; this.localOutputParameterCache.putObject(key, parameter); &#125; return list;&#125; 通过模板抽象方法org.apache.ibatis.executor.BaseExecutor#doQuery查找具体实现（如果没有特殊配置，此处是SimpleExecutor）,并且将查询结果放入一级缓存中。 1234567891011121314 public &lt;E&gt; List&lt;E&gt; doQuery(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, BoundSql boundSql) throws SQLException &#123; Statement stmt = null; List var9; try &#123; Configuration configuration = ms.getConfiguration(); StatementHandler handler = configuration.newStatementHandler(this.wrapper, ms, parameter, rowBounds, resultHandler, boundSql); stmt = this.prepareStatement(handler, ms.getStatementLog()); var9 = handler.query(stmt, resultHandler); &#125; finally &#123; this.closeStatement(stmt); &#125; return var9;&#125; 在org.apache.ibatis.executor.SimpleExecutor#doQuery方法中会创建一个Statement，而此实例就是ShardingPreparedStatement 经过Mybatis预编译SQL处理器，然后在org.apache.ibatis.executor.statement.PreparedStatementHandler#query方法中执行了PreparedStatement的execute方法 12345public &lt;E&gt; List&lt;E&gt; query(Statement statement, ResultHandler resultHandler) throws SQLException &#123; PreparedStatement ps = (PreparedStatement)statement; ps.execute(); return this.resultSetHandler.handleResultSets(ps);&#125; 在前面知道此处的PreparedStatement是ShardingPreparedStatement，所以这里调用的是ShardingPreparedStatement的execute方法 123456789101112131415public boolean execute() throws SQLException &#123; boolean var1; try &#123; //1. 首先清理本地 PreparedStatementExecutor 中缓存的sql相关信息（创建执行单元的时候会将sql相关信息缓存到本地） this.clearPrevious(); //2. ② 然后执行prepare方法，此方法中有两个很关键的操作： this.prepare(); this.initPreparedStatementExecutor(); var1 = this.preparedStatementExecutor.execute(); &#125; finally &#123; this.clearBatch(); &#125; return var1;&#125; this.prepare()执行路由策略和SQL改写策略（这两步是分片的核心，另外也都是可供使用者扩展的） 1234567891011public ExecutionContext prepare(String sql, List&lt;Object&gt; parameters) &#123; List&lt;Object&gt; clonedParameters = this.cloneParameters(parameters); RouteContext routeContext = this.executeRoute(sql, clonedParameters); ExecutionContext result = new ExecutionContext(routeContext.getSqlStatementContext()); result.getExecutionUnits().addAll(this.executeRewrite(sql, clonedParameters, routeContext)); if ((Boolean)this.properties.getValue(ConfigurationPropertyKey.SQL_SHOW)) &#123; SQLLogger.logSQL(sql, (Boolean)this.properties.getValue(ConfigurationPropertyKey.SQL_SIMPLE), result.getSqlStatementContext(), result.getExecutionUnits()); &#125; return result;&#125; org.apache.shardingsphere.underlying.pluggble.prepare.BasePrepareEngine#executeRoute方法，注册路由装饰器。 1234private RouteContext executeRoute(String sql, List&lt;Object&gt; clonedParameters) &#123; this.registerRouteDecorator(); return this.route(this.router, sql, clonedParameters);&#125; 获取已经注册的RouteDecorator类实例，过滤掉泛型是BaseRule类型的（ShardingRule是其子类，所以重新的时候覆写 getType方法时，一定要是BaseRule类型的） 实例化路由装饰器 调用模板方法 route，最终会调用到DataNodeRouter的 executeRoute 方法 123456789101112 private RouteContext executeRoute(String sql, List&lt;Object&gt; parameters, boolean useCache) &#123; //1. 解析引擎： 通过 SQLParserEngine 解析SQL（并且此处默认是会将解析后的语句缓存起来，也就证实了前面会什么会先清理缓存），然后通过调用parse0方法解析SQL并缓存 RouteContext result = this.createRouteContext(sql, parameters, useCache); Entry entry; //2. 循环执行注册了的路由装饰器 for(Iterator var5 = this.decorators.entrySet().iterator(); var5.hasNext(); result = ((RouteDecorator)entry.getValue()).decorate(result, this.metaData, (BaseRule)entry.getKey(), this.properties)) &#123; entry = (Entry)var5.next(); &#125; return result;&#125; 然后开始分片路由装饰器org.apache.shardingsphere.sharding.route.engine.ShardingRouteDecorator#decorate ① 获取分片条件：根据不同的语句创建不同的 条件解析引擎来构造分片条件（获取的分片条件用于在执行路由判断时决定使用哪种分片策略） ② 通过工厂创建出 ShardingRouteEngine 实例，一般情况下 会创建出来 ShardingStandardRoutingEngine（没有配置什么骚操作的情况下），然后调用 标准路由执行引擎的 路由方法 12345678910111213141516171819202122 public RouteResult route(ShardingRule shardingRule) &#123; if (this.isDMLForModify(this.sqlStatementContext) &amp;&amp; 1 != ((TableAvailable)this.sqlStatementContext).getAllTables().size()) &#123; throw new ShardingSphereException(&quot;Cannot support Multiple-Table for &#x27;%s&#x27;.&quot;, new Object[]&#123;this.sqlStatementContext.getSqlStatement()&#125;); &#125; else &#123; //根据路由节点生成路由结果 RouteResult return this.generateRouteResult(this.getDataNodes(shardingRule, shardingRule.getTableRule(this.logicTableName))); &#125; &#125;/** * 获取数据节点：此处获取的就是真实的SQL路由情况（比如：ds0.table_0）， * 首先判断是否使用直接路由(强制路由)，若使用则走强制路由的分片算法去计算分片；然后再判断是否根据分片条件去路由， * 若有的话，则根据配置的分片算法（内联）根据分片值计算出来具体分到哪个库哪张表；若都没有的话，则直接走混合路由的处理逻辑 * */ private Collection&lt;DataNode&gt; getDataNodes(ShardingRule shardingRule, TableRule tableRule) &#123; if (this.isRoutingByHint(shardingRule, tableRule)) &#123; return this.routeByHint(shardingRule, tableRule); &#125; else &#123; return this.isRoutingByShardingConditions(shardingRule, tableRule) ? this.routeByShardingConditions(shardingRule, tableRule) : this.routeByMixedConditions(shardingRule, tableRule); &#125; &#125; 找到了路由节点 前面一直在讲prepare方法 回到ShardingPreparedStatement#execute方法中，调用initPreparedStatementExecutor() 初始化PreparedStatementExecutor实例 并将解析出来的执行上下文中的相关SQL语句组设置到缓存中,然后调用执行器的执行方法 12345678//此处会获取到需要执行的SQL集合，主要是通过maxConnectionsSizePerQuery每次执行时最大连接数来判断sql执行单元应该分成几组，maxConnectionsSizePerQuery的值默认是1。则表示，// 如果真实的sql有10条，那么每组拆分10条，总共拆分成1组，// 此时会判断 maxConnectionsSizePerQuery 是否大于10，小于的话则会选择当前批次执行的是连接限制模式（只允许占用一个库的一个连接），相反则是内存限制模式，不会限制创建的连接数 private void initPreparedStatementExecutor() throws SQLException &#123; this.preparedStatementExecutor.init(this.executionContext); this.setParametersForStatements(); this.replayMethodForStatements(); &#125; ShardingPreparedStatement#executeQuery方法中最后调用执行器的执行方法this.preparedStatementExecutor.execute() 12345678public boolean execute() throws SQLException &#123; //1. 获取sql执行回调类（真正操作数据库） boolean isExceptionThrown = ExecutorExceptionHandler.isExceptionThrown(); //2. 调用 executeCallback方法，此方法继承自父类AbstractStatementExecutor SQLExecuteCallback&lt;Boolean&gt; executeCallback = SQLExecuteCallbackFactory.getPreparedSQLExecuteCallback(this.getDatabaseType(), isExceptionThrown); List&lt;Boolean&gt; result = this.executeCallback(executeCallback); return null != result &amp;&amp; !result.isEmpty() &amp;&amp; null != result.get(0) ? (Boolean)result.get(0) : false;&#125; 进入父类org.apache.shardingsphere.shardingjdbc.executor.AbstractStatementExecutor#executeCallback，SQL执行模板SQLExecuteTemplate类通过委派其成员ExecutorEngine执行引擎来执行真正的操作 12345protected final &lt;T&gt; List&lt;T&gt; executeCallback(SQLExecuteCallback&lt;T&gt; executeCallback) throws SQLException &#123; List&lt;T&gt; result = this.sqlExecuteTemplate.execute(this.inputGroups, executeCallback); this.refreshMetaDataIfNeeded(this.connection.getRuntimeContext(), this.sqlStatementContext); return result;&#125; 执行引擎对拆分的SQL执行单元执行处理，如图： ① 并发执行（是否是并发执行通过 是否持有事务来判断的，例如 本地事务但是你修改为非自动提交事务，那么此时就是持有事务状态，则此时就是同步执行语句） ② 迭代出SQL执行组的第一个，其余的SQL异步执行 ③ 同步执行第一个SQL执行组（方便与后面的执行组进行合并起来） ④ 通过其内置的线程池来异步执行SQL 此时一条查询语句到这里就执行完了，接下来我们接着分析对查询结果进行处理的操作 回到Mybatis中，最后对查询的结果集进行处理( resultSetHandler. handleResultSets(ps)，此处是org.apache.ibatis.executor.resultset.DefaultResultSetHandler结果集处理器 ),如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public List&lt;Object&gt; handleResultSets(Statement stmt) throws SQLException &#123; ErrorContext.instance().activity(&quot;handling results&quot;).object(this.mappedStatement.getId()); List&lt;Object&gt; multipleResults = new ArrayList(); int resultSetCount = 0; //1. 首先调用getFirstResultSet去获取第一个结果集，此处的 Statement 实例是 ShardingPreparedStatement ResultSetWrapper rsw = this.getFirstResultSet(stmt); List&lt;ResultMap&gt; resultMaps = this.mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); this.validateResultMapsCount(rsw, resultMapCount); while(rsw != null &amp;&amp; resultMapCount &gt; resultSetCount) &#123; ResultMap resultMap = (ResultMap)resultMaps.get(resultSetCount); this.handleResultSet(rsw, resultMap, multipleResults, (ResultMapping)null); rsw = this.getNextResultSet(stmt); this.cleanUpAfterHandlingResultSet(); ++resultSetCount; &#125; String[] resultSets = this.mappedStatement.getResultSets(); if (resultSets != null) &#123; while(rsw != null &amp;&amp; resultSetCount &lt; resultSets.length) &#123; ResultMapping parentMapping = (ResultMapping)this.nextResultMaps.get(resultSets[resultSetCount]); if (parentMapping != null) &#123; String nestedResultMapId = parentMapping.getNestedResultMapId(); ResultMap resultMap = this.configuration.getResultMap(nestedResultMapId); this.handleResultSet(rsw, resultMap, (List)null, parentMapping); &#125; rsw = this.getNextResultSet(stmt); this.cleanUpAfterHandlingResultSet(); ++resultSetCount; &#125; &#125; return this.collapseSingleResultList(multipleResults);&#125;//这里的Statement ShardingPreparedStatementprivate ResultSetWrapper getFirstResultSet(Statement stmt) throws SQLException &#123; ResultSet rs = stmt.getResultSet(); while(rs == null) &#123; if (stmt.getMoreResults()) &#123; rs = stmt.getResultSet(); &#125; else if (stmt.getUpdateCount() == -1) &#123; break; &#125; &#125; return rs != null ? new ResultSetWrapper(rs, this.configuration) : null;&#125; 调用了getResultSet()方法； 进入到org.apache.shardingsphere.shardingjdbc.jdbc.core.statement.ShardingPreparedStatement# getResultSet 将查询返回的结果集进行合并处理，Shardingsphere 的归并引擎功能上划分：遍历归并、排序归并（SQL中存在ORDER BY语句）、分组归并（SQL中有GroupBy子句）、聚合归并（含有聚合函数）、分页归并（含有Limit关键字） 12345678910111213public ResultSet getResultSet() throws SQLException &#123; if (null != this.currentResultSet) &#123; return this.currentResultSet; &#125; else &#123; if (this.executionContext.getSqlStatementContext() instanceof SelectStatementContext || this.executionContext.getSqlStatementContext().getSqlStatement() instanceof DALStatement) &#123; List&lt;ResultSet&gt; resultSets = this.getResultSets(); // ① MergedResult mergedResult = this.mergeQuery(this.getQueryResults(resultSets)); // ② this.currentResultSet = new ShardingResultSet(resultSets, mergedResult, this, this.executionContext); &#125; return this.currentResultSet; &#125;&#125; ① 获取所有Statement对应的结果集，此处是拿到真正数据源所对应的Statement实例，比如：我现在的数据源是 HikariDateSource，那么拿到的就是 HikariProxyPreparedStatement ② 执行合并逻辑：首先将结果集封装成流式查询结果对象StreamQueryResult，接着创建合并引擎org.apache.shardingsphere.underlying.pluggble.merge.MergeEngine#merge，然后调用合并引擎的合并方法 1234public MergedResult merge(List&lt;QueryResult&gt; queryResults, SQLStatementContext sqlStatementContext) throws SQLException &#123; this.registerMergeDecorator(); // ③ return this.merger.process(queryResults, sqlStatementContext); // ④ &#125; ③ 实例化合并引擎处理器ResultProcessEngine ④ 调用MergeEntry的 process 方法，委派来进行合并逻辑。 ⑤ ⑥ 中，判断若是ResultMergerEngine类型的合并引擎，则调用其merge方法执行真正的合并逻辑 显然满足类型判断，则此处会调用ShardingResultMergerEngine#newInstance 方法来实例化真正用于合并数据流的引擎 1234567public ResultMerger newInstance(DatabaseType databaseType, ShardingRule shardingRule, ConfigurationProperties properties, SQLStatementContext sqlStatementContext) &#123; if (sqlStatementContext instanceof SelectStatementContext) &#123; return new ShardingDQLResultMerger(databaseType); &#125; else &#123; return (ResultMerger)(sqlStatementContext.getSqlStatement() instanceof DALStatement ? new ShardingDALResultMerger(shardingRule) : new TransparentResultMerger()); &#125;&#125; 显然此处是查询语句，那么最终用于合并的引擎就是 ShardingDQLResultMerger，然后执行其merge方法 ⑦ 中判断sql中包含哪些关键字，然后创建对应的合并结果，如果条件都不满足，那么默认会使用 遍历流式归并方式合并数据。假设 我们此处SQL中带有 order by关键字，那么创建得合并结果对象就是OrderByStreamMergedResult ⑧ 对创建出来的排序合并结果进行装饰操作（就是判断有没有别的关键字，例如：Limit，如果有就会创建LimitDecoratorMergedResult 装饰器对象，在之前的排序合并基础上又多一个 Limit功能），再回到 ShardingPreparedStatement中，会创建一个 ShardingResultSet对象设置到当前的成员变量currentResultSet中，并返回。 此时如果是批量的场景，返回的结果集中实际上已经包含了所有的结果集（前面存放在OrderByStreamMergedResult的 orderByValuesQueue 队列中） 排序归并流程： 调用合并结果的 next方法时会执行如图 最后流程又回到Mybatis 结果集处理上了，将结果返回给请求调用方 6. 分片策略 策略包括了算法，算法是策略的一个属性。 Sharding-JDBC 中的分片策略有两个维度：分库（数据源分片）策略和分表策略；（mycat只支持要么分库或者要么分表） 跟 Mycat 不一样，Sharding-JDBC 没有提供内置的分片算法，而是通过实现接口ShardingStrategy， 让开发者自行实现，这样可以根据业务实际情况灵活地实现分片。 6.1 行表达式分片策略 InlineShardingStrategy 算法：行内表达式 $-&gt;{} 文档路径：https://shardingsphere.apache.org/document/current/cn/features/sharding/concept/inline-expression/ 只支持单分片键，提供对=和IN 操作的支持。行内表达式的配置比较简单。 例如： begin..end表示范围区间，如：db{begin..end} 表示范围区间，如：dbbegin..end表示范围区间，如：db-&gt;{0…1}表示db0, db1 [unit1,unit2,unitx]表示枚举值，如：{[unit1, unit2, unit_x]} 表示枚举值，如 ：[unit1,unit2,unitx​]表示枚举值，如：{[‘db0’, ‘db1’]} t_user_$-&gt;{u_id % 8} 表示 t_user 表根据 u_id 模 8，而分成 8 张表，表名称为 t_user_0 到 t_user_7。 行表达式中如果出现连续多个expression或{ expression }或expression或-&gt;{ expression }表达式，整个表达式最终的结果将会根据每个子表达式的结果进行笛卡尔组合。 例如，以下行表达式： [′db1′,′db2′]table{[&#x27;db1&#x27;, &#x27;db2&#x27;]}_table[′db1′,′db2′]t​able{1…3} 最终会解析为： db1_table1, db1_table2, db1_table3, db2_table1, db2_table2, db2_table3 6.2 标准分片策略 StandardShardingStrategy 标准分片策略只支持但分片键，提供了两个分片算法，分别对应了IN、BETWEEN 和 =；如果要是用标准分片策略，必须要实现PreciseShardingAlgorithm,用来处理=和IN的分片 RangeShardingAlgorithm是可选的，如果没有实现，SQL语句会发送到所有节点上执行。 算法：范围分片RangeShardingAlgorithm 和 精确分片PreciseShardingAlgorithm两种算法 123456789101112131415161718192021/** * 自定义分库策略 * 数据库分库的策略，根据分片键，返回数据库名称 */public class DBShardAlgo implements PreciseShardingAlgorithm&lt;Long&gt; &#123; @Override public String doSharding(Collection&lt;String&gt; collection, PreciseShardingValue&lt;Long&gt; preciseShardingValue) &#123; String db_name=&quot;ds&quot;; Long num= preciseShardingValue.getValue()%2; db_name=db_name + num; System.out.println(&quot;----------------db_name:&quot; + db_name); for (String each : collection) &#123; System.out.println(&quot;ds:&quot; + each); if (each.equals(db_name)) &#123; return each; &#125; &#125; throw new IllegalArgumentException(); &#125;&#125; 123456789101112131415161718/** * 自定义分表策略 * 等值查询使用的分片算法，包括in */public class TblPreShardAlgo implements PreciseShardingAlgorithm&lt;Long&gt; &#123; @Override public String doSharding(Collection&lt;String&gt; availableTargetNames, PreciseShardingValue&lt;Long&gt; shardingColumn) &#123; // 不分表 System.out.println(&quot;-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-availableTargetNames:&quot; + availableTargetNames); for (String tbname : availableTargetNames) &#123; //如果这里要分表可以根据 shardingColumn.getValue() % 2 //也可以根据月份分表，user_info_202201这种，只需要在这里拼接表名即可 System.out.println(shardingColumn.getValue() % 2+&quot;-------&quot;+ tbname); return tbname; &#125; throw new IllegalArgumentException(); &#125;&#125; 那么在配置文件中只需要指定分配规则 123##为了缩减篇幅，这里改成properties的格式spring.shardingsphere.sharding.tables.t_order.databaseStrategy.standard.shardingColumn=order_idspring.shardingsphere.sharding.tables.t_order.databaseStrategy.standard.precise-algorithm-class-name=com.ygb.config.TblPreShardAlgo 范围分片： 123456789101112131415161718192021/** * 范围查询所使用的分片算法 */public class TblRangeShardAlgo implements RangeShardingAlgorithm&lt;Long&gt; &#123; @Override public Collection&lt;String&gt; doSharding(Collection&lt;String&gt; availableTargetNames, RangeShardingValue&lt;Long&gt; rangeShardingValue) &#123; System.out.println(&quot;范围-*-*-*-*-*-*-*-*-*-*-*---------------&quot;+availableTargetNames); System.out.println(&quot;范围-*-*-*-*-*-*-*-*-*-*-*---------------&quot;+rangeShardingValue); Collection&lt;String&gt; collect = new LinkedHashSet&lt;&gt;(); Range&lt;Long&gt; valueRange = rangeShardingValue.getValueRange(); for (Long i = valueRange.lowerEndpoint(); i &lt;= valueRange.upperEndpoint(); i++) &#123; for (String each : availableTargetNames) &#123; if (each.endsWith(i % availableTargetNames.size() + &quot;&quot;)) &#123; collect.add(each); &#125; &#125; &#125; return collect; &#125;&#125; 查看数据库db0、db1，可以看到只有db0有数据； 6.3 复合分片策略 ComplexShardingStrategy 复合分片策略支持多分片键 算法：ComplexKeysShardingAlgorithm 场景：根据日期和ID两个字段分片，每个月3张表，先根据日期，然后在根据ID取模分片 6.4 Hint分片策略 HintShardingStrategy 通过 Hint 而非 SQL 解析的方式分片的策略 算法：HintShardingAlgorithm 6.5 不分片策略 NoneShardingStrategy 只在一个节点存储 算法：无 与Mycat对比 ShardingSphere-JDBC Mycat 工作层面 JDBC协议 Mysql协议/JDBC协议 运行方式 Jar包，客户端 独立服务，服务端 开发方式 代码/配置改动 连接地址修改（数据源） 运维方式 无 管理独立服务，运维成本高 性能 多线程并发操作，性能高 独立服务+网络开销，存在性能损失风险 功能范围 协议层面 包括分布式事务、数据迁移等 适用操作 OLTP OLTP+OLAP 支持数据库 基于JDBC协议的数据库 MySQL 和其他支持 JDBC 协议的数据库 支持语言 Java 支持 JDBC 协议的语言 从易用性和功能完善的角度来说，Mycat 似乎比 Sharding-JDBC 要好，因为有现成 的分片规则，也提供了4种ID生成方式，通过注解可以支持高级功能，比如跨库关联查询。 建议：小型项目可以用 Sharding-JDBC。大型项目，可以用 Mycat。","categories":[{"name":"05 分布式","slug":"05-分布式","permalink":"https://xiaoyuge5201.github.io/categories/05-%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"分布式","slug":"分布式","permalink":"https://xiaoyuge5201.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}]},{"title":"CentOS安装Zookeeper 3.7.1单节点","slug":"zookeeper-install","date":"2022-06-24T07:12:42.000Z","updated":"2022-06-24T07:12:42.000Z","comments":false,"path":"zookeeper-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/zookeeper-install/index.html","excerpt":"","text":"1. JDK依赖 请参考这篇博客：Linux安装JDK以及配置 2. 下载Zookeeper 下载连接：https://zookeeper.apache.org/releases.html, 找到版本3.7.1下载二进制版本，不需要编译 以安装路径 /usr/local/tools 为例 12cd /usr/local/toolswget https://dlcdn.apache.org/zookeeper/zookeeper-3.7.1/apache-zookeeper-3.7.1-bin.tar.gz 如果上面的地址下载比较慢的话，可以试下在apache仓库里面找（https://archive.apache.org/dist/zookeeper/） 3. 解压 1tar -xzvf apache-zookeeper-3.7.1-bin.tar.gz 4. 修改配置文件 1234cd apache-zookeeper-3.7.1-bin/confcp zoo_sample.cfg zoo.cfgvim zoo.cfg 端口号默认2181。 配置文件zoo.cfg里面的dataDir要修改，如果不改，需要创建这个目录 1mkdir -p /tmp/zookeeper 5. 启动ZK zookeeper默认端口 2181 12cd ../bin./zkServer.sh start 输出日志： 123ZooKeeper JMX enabled by defaultUsing config: /usr/local/tools/apache-zookeeper-3.7.1-bin/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 其他命令： 1234567891011121314ps -ef | grep zookeeper #查看是否启动成功./zkServer.sh status #查看zk的运行状态./zkCli.sh #客户端链接zkls #查看节点get /name #get 获取节点数据和更新信息， name为具体的节点名称create [-s] [-e] path data acl #create 创建节点 e 临时节点 s 顺序节点delete path [version] #删除节点","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://xiaoyuge5201.github.io/tags/Zookeeper/"}]},{"title":"kafka常用命令","slug":"kafka-command","date":"2022-06-23T06:57:03.000Z","updated":"2022-06-23T06:57:03.000Z","comments":false,"path":"kafka-command/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/kafka-command/index.html","excerpt":"","text":"1. 脚本概览 bin目录下的脚本作用 脚本 作用 connect-distributed.sh 用于启动多节点的Distributed模式的Kafka Connect组件 connect-standalone.sh 用于启动单节点的Standalone模式的Kafka Connect组件 kafka-acls.sh 用于设置Kafka权限，比如设置哪些用户可以访问Kafka的哪些TOPIC的权限 kafka-broker-api-versions.sh 主要用于验证不同Kafka版本之间服务器和客户端的适配性 kafka-configs.sh 配置管理脚本 kafka-console-consumer.sh kafka消费者控制台 kafka-console-producer.sh kafka生产者控制台 kafka-consumer-groups.sh kafka消费者组相关信息 kafka-consumer-perf-test.sh kafka消费者性能测试脚本 kafka-delegation-tokens.sh 用于管理Delegation Token。基于Delegation Token的认证是一种轻量级的认证机制，是对SASL认证机制的补充。 kafka-delete-records.sh 用于删除Kafka的分区消息 kafka-dump-log.sh 用于查看Kafka消息文件的内容 kafka-log-dirs.sh 用于查询各个Broker上的各个日志路径的磁盘占用情况 kafka-mirror-maker.sh 用于在Kafka集群间实现数据镜像 kafka-preferred-replica-election.sh 用于执行Preferred Leader选举，可以为指定的主题执行更换Leader的操作 kafka-producer-perf-test.sh kafka生产者性能测试脚本 kafka-reassign-partitions.sh 用于执行分区副本迁移以及副本文件路径迁移。 kafka-replica-verification.sh 复制进度验证脚本 kafka-run-class.sh 用于执行任何带main方法的Kafka类 kafka-server-start.sh 启动kafka服务 kafka-server-stop.sh 停止kafka服务 kafka-streams-application-reset.sh 用于给Kafka Streams应用程序重设位移，以便重新消费数据 kafka-topics.sh topic管理脚本 kafka-verifiable-consumer.sh 可检验的kafka消费者 kafka-verifiable-producer.sh 可检验的kafka生产者 trogdor.sh Kafka的测试框架，用于执行各种基准测试和负载测试 zookeeper-server-start.sh 启动zk服务 zookeeper-server-stop.sh 停止zk服务 zookeeper-shell.sh zk客户端 2. Broker服务 1、启动服务 1./kafka-server-start.sh ../config/server.properties 2、后台启动 1./kafka-server-start.sh -daemon ../config/server.properties 3、停止服务 1./kafka-server-stop.sh ../config/server.properties 3. 元数据 1、创建topic 1./kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic my-test-topic 2、查看所有topic 1./kafka-topics.sh --bootstrap-server localhost:9092 --list 从Kafka 2.2版本开始，Kafka社区推荐用–bootstrap-server参数替换–zookeeper参数用于指定Kafka Broker。集群的多个IP端口用逗号,隔开 3、查看topic详细信息 1kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic topic_name 4、给topic增加分区（只能增加不能减少） 1./kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-test-topic --partitions 10 5、删除topic（标记删除） 1./kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic my-test-topic 6、永久删除需要修改配置文件： 1delete.topic.enable=true 7、强制删除TOPIC的方法： 手动删除ZooKeeper节点/admin/delete_topics下以待删除TOPIC为名的znode。 手动删除TOPIC在磁盘上的分区目录。 在ZooKeeper中执行rmr /controller，触发Controller重选举，刷新Controller缓存。可能会造成大面积的分区Leader重选举。可以不执行，只是Controller缓存中没有清空待删除TOPIC，不影响使用。 8、修改partition副本数： 先配置一个reassign.json文件，内容： 例如my-test-topic有3个分区，原来只有一个副本，增加到2个副本 12345&#123;&quot;version&quot;:1, &quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;my-test-topic&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]&#125;,&#123;&quot;topic&quot;:&quot;my-test-topic&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[1,2]&#125;,&#123;&quot;topic&quot;:&quot;my-test-topic&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[2,0]&#125;]&#125; 执行kafka-reassign-partitions脚本： 1./kafka-reassign-partitions.sh --zookeeper localhost:2181 --reassignment-json-file reassign.json --execute 4. 生产者 1、发送消息 1./kafka-console-producer.sh --broker-list localhost:9092 --topic my-test-topic 5. 消费者 1、消费消息 1./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic my-test-topic 2、查看消费者组提交的位移数据: 1./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic __consumer_offsets --formatter &quot;kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter&quot; --from-beginning","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://xiaoyuge5201.github.io/tags/kafka/"}]},{"title":"CentOS安装kafka 3.2.0单机版","slug":"kafka-install","date":"2022-06-23T06:19:35.000Z","updated":"2022-06-23T06:19:35.000Z","comments":false,"path":"kafka-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/kafka-install/index.html","excerpt":"","text":"1. JDK依赖 请参考这篇博客：Linux安装JDK以及配置 2. 下载解压Kafka 下载地址：https://kafka.apache.org/downloads， 点击相应的版本，下载Binary 二进制版本而不是源码 我这里下载的是3.2.0版本（https://www.apache.org/dyn/closer.cgi?path=/kafka/3.2.0/kafka_2.12-3.2.0.tgz） 1234cd /usr/local/toolswget https://dlcdn.apache.org/kafka/3.2.0/kafka_2.12-3.2.0.tgztar -xzvf kafka_2.12-3.2.0.tgzcd kafka_2.12-3.2.0 3. 启动zookeeper(默认端口2181) kafka需要依赖ZK，安装包中已经自带了一个ZK，也可以改成指定已运行的ZK。 如果改成指定的ZK需要修改修改 kafka 安装目录下的 config/server.properties 文件中的 zookeeper.connect 。这里使用自带的ZK。 后台启动zk 1nohup ./bin/zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; zookeeper.nohup &amp; 检查zookeeper是否启动成功： 1ps -ef|grep zookeeper 4. 启动kafka（默认端口9092） 修改相关配置 1vim config/server.properties 123456broker.id=1 #Broker ID启动以后就不能改了listeners=PLAINTEXT://192.168.44.160:9092 #取消注释，改成本机IP：num.partitions=1 #num.partitions后面增加2行。auto.create.topics.enable=true #发送到不存在topic自动创建delete.topic.enable=true #允许永久删除topic 后台启动kafka 12345 nohup ./bin/kafka-server-start.sh ./config/server.properties &amp;#或者./bin/kafka-server-start.sh -daemon ./config/server.properties 5. 创建Topic 创建一个名为gptest的topic，只有一个副本，一个分区： 1sh bin/kafka-topics.sh --create --bootstrap-server localhost:2181 --replication-factor 1 --partitions 1 --topic gptest 查看已经创建的 topic： 1sh bin/kafka-topics.sh -list -bootstrap-server localhost:2181 从Kafka 2.2版本开始，Kafka社区推荐用–bootstrap-server参数替换–zookeeper参数用于指定Kafka Broker。集群的多个IP端口用逗号,隔开 5. 启动Producer 打开一个窗口，在kafka解压目录下： 1sh bin/kafka-console-producer.sh --broker-list localhost:9092 --topic gptest 6. 启动Consumer 在一个新的远程窗口中： 1sh bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic gptest --from-beginning 7. Producer窗口发送消息 输入hello world 回车 消费者收到了消息： 8. 删除kafka全部数据 1、停止每台机器上的kafka； 2、删除kafka存储目录（server.properties文件log.dirs配置，默认为“/tmp/kafka-logs”）全部topic的数据目录； 3、删除zookeeper上与kafka相关的znode节点；除了/zookeeper 4、重启kafka。","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"kafka","slug":"kafka","permalink":"https://xiaoyuge5201.github.io/tags/kafka/"}]},{"title":"VMWare创建虚拟机并设置静态IP","slug":"static-ip-setting","date":"2022-06-12T01:06:54.000Z","updated":"2022-06-12T01:06:54.000Z","comments":false,"path":"static-ip-setting/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/static-ip-setting/index.html","excerpt":"","text":"1. VMware+Centos7 静态IP设置 1.1 VMware设置 省略windows安装VMWare的过程。 查看虚拟网卡信息 虚拟机安装好以后，当前的系统会多出来两个虚拟网卡，一般情况下这两个网卡的命名是 12VMware Virtual Ethernet Adapter for VMnet1VMware Virtual Ethernet Adapter for VMnet8 我们可以把虚拟机中的系统的静态IP绑定到VMnet8上，所以下一步我们看一下VMnet8的ip地址 查看虚拟网卡IP 在当前操作系统中，输入ipconfig -all显示网卡的IP信息如下 记住当前VMnet8网卡的网段，如图所示，网段为:192.168.8 虚拟机设置 打开VMware，进入编辑 -&gt; 虚拟机网络编辑器 选中VMnet8这个网卡，点击NAT设置 在NAT设置中，可以看到子网IP、网关IP等信息。这里我们需要把网关IP记录下来： 192.168.8.2 设置虚拟机的网络连接方式 选中创建好的虚拟机，右键设置进入虚拟机设置面板。 将网络社配置设置为自定义，选中VMnet8这个网卡保存 1.2 Centos7中的静态ip设置 前置工作完成之后，就开始进入虚拟机的设置环节了 找到网卡信息配置 1ifconfig 找到网卡名称ens33，输入命令 1vi /etc/sysconfig/network-scripts/ifcfg-ens33 修改配置 这个配置需要修改两个地方 设置BOOTPROTO=“static” 添加IPADDR/NETMASK/DNS1/GATEWAY这几个配置 配置说明 IPADDR 就是当前虚拟机要设置的固定ip地址，网段要一致，我这边的案例是在8网段(这个网段是VMnet8对应的子网网段，不是真实机上的网段）。 NETMASK 子网掩码 用VMnet8对应的子网掩码值就行 DNS1 在真实机器上通过ipconfig，获得真实机器的网卡对应的DNS地址，填在这个位置 GATEWAY 网关地址，用前面第二个步骤中找到的网关地址： 192.168.8.2 重启网络服务 1service network restart 1.3 可能会遇到的问题 VMnet8的网段和真实机器上的网段不一样 真实机器的网段是8， 而VMnet8的网段是136. 由于网段不一致，就会存在网络不通的问题。所以第一步，应该是把VMnet8这个网卡的网段重新设置， 进入VMware， 找到 编辑 -&gt;虚拟网络编辑器 点击更改设置 修改子网IP，原本的网段是136， 改成8网段。 保存以后，改网卡会自动重启 真实机器无法Ping通虚拟机 原因1： 虚拟机的网段设置不正确，这个网段不是真实机器的网段，而是VMnet8 NAT模式对应的网段，本案例中的网段是8. 原因2：虚拟机迁移过，原本设置的网段在新的网络中无效，可以在VMware这个工具的如下菜单处 编辑 -&gt; 虚拟网络编辑器 还原默认设置，这个还原操作会重建虚拟网卡， 重建之后，VMware NAT模式的子网地址的网段会发生变化。后续的配置采用这个网段就行 2. Mac用VMWare创建虚拟机并设置静态IP 参考博客：https://blog.51cto.com/u_15298624/3033418","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"ip","slug":"ip","permalink":"https://xiaoyuge5201.github.io/tags/ip/"}]},{"title":"Vue学习","slug":"vue-1","date":"2022-06-11T14:12:58.000Z","updated":"2022-06-11T14:12:58.000Z","comments":false,"path":"vue-1/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/vue-1/index.html","excerpt":"","text":"1. 什么是vue vue 官网：https://cn.vuejs.org/ 渐进式框架， 2.安装 安装 npm npm 全称为 Node Package Manager，是一个基于Node.js的包管理器，也是整个Node.js社区最流行、支持的第三方模块最多的包管理器。 1npm -v 由于网络原因 安装 cnpm 12345#旧版，cnpm官方公告将在2022年6月30日停止老域名解析npm install -g cnpm --registry=https://registry.npm.taobao.org #新版npm install -g cnpm --registry=https://registry.npmmirror.com 安装 vue-cli 1cnpm install -g @vue/cli 安装 webpack webpack 是 JavaScript 打包器(module bundler) 1cnpm install -g webpack 3 Vue练习 3.1 Vue实例 1234567891011121314151617181920&lt;div id=&quot;app&quot;&gt; &#123;&#123; message &#125;&#125; &#123;&#123; name &#125;&#125;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; var data = &#123; message: &#x27;hello world&#x27;, name: &#x27;xiaoyuge&#x27; &#125;; //view model 数据模型 vm var vm = new Vue(&#123; el: &#x27;#app&#x27;, //如果要某一个属性声明式响应，必须在new vue的时候声明 data: data &#125;); //vm.$data.message = data.message == vm.message; //实例创建之后，可以通过 vm.$data 访问原始数据对象。Vue 实例也代理了 data 对象上所有的 property，因此访问 vm.message 等价于访问 vm.$data.message vm.$watch(&#x27;message&#x27;, function(newVal, oldVal) &#123; console.log(newVal, oldVal); &#125;) //修改值必须放到后面， vm.$data.message = &quot;test....&quot;&lt;/script&gt; 3.2 数据和方法 当一个 Vue 实例被创建时，它将 data 对象中的所有的 property 加入到 Vue 的响应式系统中。当这些 property 的值发生改变时，视图将会产生“响应”，即匹配更新为新的值 12345678910111213141516171819// 我们的数据对象var data = &#123; a: 1 &#125;// 该对象被加入到一个 Vue 实例中var vm = new Vue(&#123; data: data&#125;)// 获得这个实例上的 property// 返回源数据中对应的字段vm.a == data.a // =&gt; true// 设置 property 也会影响到原始数据vm.a = 2data.a // =&gt; 2// ……反之亦然data.a = 3vm.a // =&gt; 3 当这些数据改变时，视图会进行重渲染。值得注意的是只有当实例被创建时就已经存在于 data 中的 property 才是响应式的。也就是说如果你添加一个新的 property，比如 1vm.b = &#x27;hi&#x27; 那么对 b 的改动将不会触发任何视图的更新。如果你知道你会在晚些时候需要一个 property，但是一开始它为空或不存在，那么你仅需要设置一些初始值。比如： 1234567data: &#123; newTodoText: &#x27;&#x27;, visitCount: 0, hideCompletedTodos: false, todos: [], error: null&#125; 这里唯一的例外是使用 Object.freeze()，这会阻止修改现有的 property，也意味着响应系统无法再追踪变化 1234567891011121314151617&lt;div id=&quot;app&quot;&gt; &lt;p&gt;&#123;&#123; foo &#125;&#125;&lt;/p&gt; &lt;!-- 这里的 `foo` 不会更新！ --&gt; &lt;button v-on:click=&quot;foo = &#x27;baz&#x27;&quot;&gt;Change it&lt;/button&gt;&lt;/div&gt;&lt;script&gt;var obj = &#123; foo: &#x27;bar&#x27;&#125;Object.freeze(obj)new Vue(&#123; el: &#x27;#app&#x27;, data: obj&#125;)&lt;/script&gt; 除了数据 property，Vue 实例还暴露了一些有用的实例 property 与方法。它们都有前缀 $，以便与用户定义的 property 区分开来。例如： 12345678910111213var data = &#123; a: 1 &#125;var vm = new Vue(&#123; el: &#x27;#example&#x27;, data: data&#125;)vm.$data === data // =&gt; truevm.$el === document.getElementById(&#x27;example&#x27;) // =&gt; true// $watch 是一个实例方法vm.$watch(&#x27;a&#x27;, function (newValue, oldValue) &#123; // 这个回调将在 `vm.a` 改变后调用&#125;) 2.3 生命周期钩子 Vue 实例从创建到销毁的过程，就是生命周期。也就是从开始创建、初始化数据、编译模板、挂载Dom→渲染、更新→渲染、卸载等一系列过程，我们称这是 Vue 的生命周期 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;div id=&quot;app&quot;&gt; &#123;&#123;msg&#125;&#125;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; var vm = new Vue(&#123; el: &quot;#app&quot;, data: &#123; msg: &quot;hi vue&quot;, &#125;, //在实例初始化之后，数据观测 (data observer) 和 event/watcher 事件配置之前被调用。 beforeCreate: function() &#123; console.log(&#x27;beforeCreate&#x27;); &#125;, //在实例创建完成后被立即调用。 // 在这一步，实例已完成以下的配置：数据观测 (data observer)，属性和方法的运算，watch/event 事件回调。 // 然而，挂载阶段还没开始，$el 属性目前不可见 created: function() &#123; console.log(&#x27;created&#x27;); &#125;, //在挂载开始之前被调用：相关的渲染函数首次被调用 beforeMount: function() &#123; console.log(&#x27;beforeMount&#x27;); &#125;, //el 被新创建的 vm.$el 替换, 挂在成功 mounted: function() &#123; console.log(&#x27;mounted&#x27;); &#125;, //数据更新时调用 beforeUpdate: function() &#123; console.log(&#x27;beforeUpdate&#x27;); &#125;, //组件 DOM 已经更新, 组件更新完毕 updated: function() &#123; console.log(&#x27;updated&#x27;); &#125;, beforeDestroy() &#123; console.log(&quot;=========&quot; + &quot;beforeDestroy：销毁之前&quot; + &quot;========&quot;); console.log(this.$el); console.log(this.$data); &#125;, destroyed() &#123; console.log(&quot;==========&quot; + &quot;destroyed：销毁之后&quot; + &quot;===========&quot;); console.log(this.$el); &#125;, activated() &#123; console.log(&quot;&quot;); &#125;, deactivated() &#123; console.log(&quot;&quot;); &#125; &#125;); //3秒后修改值，触发beforeUpdate, updated事件 setTimeout(function() &#123; vm.msg = &quot;change ......&quot;; &#125;, 3000);&lt;/script&gt; 控制台依次打印： beforeCreate created beforeMount mounted beforeUpdate updated 注意事项： 123不要在选项 property 或回调上使用箭头函数，比如 created: () =&gt; console.log(this.a) 或 vm.$watch(&#x27;a&#x27;, newValue =&gt; this.myMethod())。因为箭头函数并没有 this，this 会作为变量一直向上级词法作用域查找，直至找到为止，经常导致 Uncaught TypeError: Cannot read property of undefined 或 Uncaught TypeError: this.myMethod is not a function 之类的错误。 3.3 条件与循环 v-if: 控制切换一个元素是否显示 1234567891011121314&lt;div id=&quot;vue-app&quot;&gt; &lt;p v-if=&quot;seen&quot;&gt;v-if: 如果为false，Dom将不渲染该元素&lt;/p&gt; &lt;p v-show=&quot;seen&quot;&gt;show：Dom渲染，只是通过css控制display属性是否显示&lt;/p&gt; &lt;p&gt;show: 渲染时开销大；v-if：渲染时开销小；具体场景根据实际情况选定&lt;/p&gt;&lt;/div&gt;&lt;script&gt; var vm = new Vue(&#123; el: &#x27;#vue-app&#x27;, data: &#123; seen: true &#125; &#125;) vm.seen = false;&lt;/script&gt; 当改变seen 的值为false的时候，v-if绑定的元素Dom元素不渲染 v-for: 绑定数组的数据来渲染一个项目列表 123456789101112131415161718192021222324252627282930313233343536 &lt;div id=&quot;vue-app&quot;&gt; &lt;!--遍历输出list--&gt; &lt;p&gt;遍历输出list&lt;/p&gt; &lt;ol&gt; &lt;li v-for=&quot;item in list&quot;&gt; &#123;&#123; item.name &#125;&#125; &lt;/li&gt; &lt;/ol&gt; &lt;br&gt; &lt;p&gt;遍历输出map key value-&lt;/p&gt; &lt;ul&gt; &lt;li v-for=&quot;value, key in object&quot;&gt; &#123;&#123;key&#125;&#125; : &#123;&#123; value &#125;&#125; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;script&gt; var vm = new Vue(&#123; el: &#x27;#vue-app&#x27;, data:&#123; list:[ &#123;name: &#x27;张三&#x27;&#125;, &#123;name: &#x27;李四&#x27;&#125;, &#123;name: &#x27;王五&#x27;&#125; ], object: &#123;truetruetitle: &#x27;How to do lists in Vue&#x27;,truetrueauthor: &#x27;Jane Doe&#x27;,truetruepublishedAt: &#x27;2016-04-10&#x27; &#125; &#125; &#125;) //往数组里面添加元素，页面上响应式渲染 vm.list.push(&#123;name:&#x27;赵六&#x27;&#125;); &lt;/script&gt; 3.4 处理用户输入 v-on: 事件监听器 1234567891011121314151617181920212223242526272829303132333435&lt;div id=&quot;app&quot;&gt; &lt;div id=&quot;example-1&quot;&gt; &lt;!-- 单击事件：计算器加一 --&gt; &lt;button v-on:click=&quot;counter += 1&quot;&gt; 数值 : &#123;&#123; counter &#125;&#125; &lt;/button&gt;&lt;br /&gt; &lt;!-- 双击事件 --&gt; &lt;button v-on:dblclick=&quot;greet(&#x27;abc&#x27;, $event)&quot;&gt;Greet&lt;/button&gt; &lt;/div&gt; &lt;div id=&quot;example-2&quot;&gt; &lt;p&gt;&#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;!-- 单击 --&gt; &lt;button v-on:click=&quot;reverseMessage&quot;&gt;反转消息&lt;/button&gt; &lt;/div&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; var vm = new Vue(&#123; el: &quot;#app&quot;, data: &#123; counter: 0, name: &quot;vue&quot;, message: &#x27;hello world&#x27; &#125;, methods: &#123; greet: function(str, e) &#123; alert(str); &#125;, reverseMessage: function() &#123; //更新了应用的状态，但没有触碰 DOM。所有的 DOM 操作都由 Vue 来处理，这样我们只需要关注逻辑层面即可 //this 表示vue实例对象，可以获取相关的数据信息 this.message = this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125; &#125; &#125;);&lt;/script&gt; v-model: 表单输入和应用状态之间的双向绑定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;div id=&quot;app&quot;&gt; &lt;div id=&quot;example-1&quot;&gt; &lt;input v-model=&quot;message&quot; placeholder=&quot;edit me&quot;&gt; &lt;p&gt;Message is: &#123;&#123; message &#125;&#125;&lt;/p&gt; &lt;textarea v-model=&quot;message2&quot; placeholder=&quot;add multiple lines&quot;&gt;&lt;/textarea&gt; &lt;p style=&quot;white-space: pre-line;&quot;&gt;&#123;&#123; message2 &#125;&#125;&lt;/p&gt; &lt;br /&gt; &lt;div style=&quot;margin-top:20px;&quot;&gt; &lt;input type=&quot;checkbox&quot; id=&quot;jack&quot; value=&quot;Jack&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;jack&quot;&gt;Jack&lt;/label&gt; &lt;input type=&quot;checkbox&quot; id=&quot;john&quot; value=&quot;John&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;john&quot;&gt;John&lt;/label&gt; &lt;input type=&quot;checkbox&quot; id=&quot;mike&quot; value=&quot;Mike&quot; v-model=&quot;checkedNames&quot;&gt; &lt;label for=&quot;mike&quot;&gt;Mike&lt;/label&gt; &lt;br&gt; &lt;span&gt;Checked names: &#123;&#123; checkedNames &#125;&#125;&lt;/span&gt; &lt;/div&gt; &lt;br&gt; &lt;div style=&quot;margin-top:20px;&quot;&gt; &lt;input type=&quot;radio&quot; id=&quot;one&quot; value=&quot;One&quot; v-model=&quot;picked&quot;&gt; &lt;label for=&quot;one&quot;&gt;One&lt;/label&gt; &lt;br&gt; &lt;input type=&quot;radio&quot; id=&quot;two&quot; value=&quot;Two&quot; v-model=&quot;picked&quot;&gt; &lt;label for=&quot;two&quot;&gt;Two&lt;/label&gt; &lt;br&gt; &lt;span&gt;Picked: &#123;&#123; picked &#125;&#125;&lt;/span&gt; &lt;/div&gt; &lt;button type=&quot;button&quot; @click=&quot;submit&quot;&gt;提交&lt;/button&gt; &lt;/div&gt;&lt;/div&gt;&lt;script type=&quot;text/javascript&quot;&gt; var vm = new Vue(&#123; el: &quot;#app&quot;, data: &#123; //这里可以设置初始值，也可以设置为&#x27;&#x27; 空值 message: &quot;test&quot;, message2: &quot;hi&quot;, checkedNames: [&#x27;Jack&#x27;, &#x27;John&#x27;], //多选框，值是数组格式 picked: &quot;Two&quot; &#125;, methods: &#123; submit: function() &#123; //this代表的是vue实例对象，可以通过this获取表单数据，如下 var params = &#123; message: this.message, message2: this.message2, checkedNames: this.checkedNames, picked: this.picked &#125; console.log(params); &#125; &#125; &#125;);&lt;/script&gt;","categories":[{"name":"12 前端","slug":"12-前端","permalink":"https://xiaoyuge5201.github.io/categories/12-%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://xiaoyuge5201.github.io/tags/vue/"}]},{"title":"SpringBoot参数校验","slug":"springboot-validate-params","date":"2022-06-06T03:02:20.000Z","updated":"2022-06-06T03:02:20.000Z","comments":false,"path":"springboot-validate-params/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/springboot-validate-params/index.html","excerpt":"","text":"1. 前言 在控制器类的方法里自己写校验逻辑代码也可以，只是不够优美，业界有更好的处理方法，主要有以下几种。 2. PathVariable校验 1234567/** * 使用正则表达式限制group 只能是a-zA-Z0-9_ */@GetMapping(&quot;/path/&#123;group:[a-zA-Z0-9_]+&#125;/&#123;userid&#125;&quot;)public String path(@PathVariable(&quot;group&quot;) String group, @PathVariable(&quot;userid&quot;) Integer userid) &#123; return group + &quot;:&quot; + userid;&#125; 当请求URI不满足正则表达式时，客户端将收到404错误码，不方便的是，不能通过捕获异常的方式，向前端返回统一的、自定义格式的响应参数 3. 方法参数校验 1234567@GetMapping(&quot;/validate&quot;)@ResponseBodypublic String validate1( @Size(min = 1, max = 10, message = &quot;姓名长度必须为1到10&quot;) @RequestParam(&quot;name&quot;) String name, @Min(value = 10, message = &quot;年龄最小为10&quot;) @Max(value = 100, message = &quot;年龄最大为100&quot;) @RequestParam(&quot;age&quot;) Integer age) &#123; return name + &quot;:&quot; + age;&#125; 使用@Size 、@Min、@Max等校验注解进行参数校验 如果前端传递的参数不满足规则，则跑出异常，上面代码中@size、@Min、@Max注解来源于validation-api包中。 更多注解参 参数校验注解 小节。 4.表单对象/VO对象校验 当参数是VO时，可以在VO类的属性上添加校验注解。 123456789101112131415public class User &#123; @Size(min = 1,max = 10,message = &quot;姓名长度必须为1到10&quot;) private String name; @NotEmpty private String firstName; @Min(value = 10,message = &quot;年龄最小为10&quot;)@Max(value = 100,message = &quot;年龄最大为100&quot;) private Integer age; @Future @JSONField(format=&quot;yyyy-MM-dd HH:mm:ss&quot;) private Date birth; //。。。&#125; 其中，@Future注解要求必须是相对当前时间来讲“未来的”某个时间。@Past表示过去的某个时间 12345@PostMapping(&quot;/validate2&quot;)@ResponseBodypublic User validate2(@Valid @RequestBody User user)&#123; return user;&#125; 5.自定义校验规则 5.1 自定义注解校验 需要自定义一个注解类和一个校验类。 12345678910111213141516171819import javax.validation.Constraint;import javax.validation.Payload;import java.lang.annotation.*;@Documented@Retention(RetentionPolicy.RUNTIME)@Target(&#123;ElementType.PARAMETER,ElementType.FIELD&#125;)@Constraint(validatedBy = FlagValidatorClass.class)public @interface FlagValidator &#123; // flag的有效值，多个使用,隔开 String values(); // flag无效时的提示内容 String message() default &quot;flag必须是预定义的那几个值，不能随便写&quot;; Class&lt;?&gt;[] groups() default &#123;&#125;; Class&lt;? extends Payload&gt;[] payload() default &#123;&#125;;&#125; 1234567891011121314151617181920212223242526272829303132333435import javax.validation.ConstraintValidator;import javax.validation.ConstraintValidatorContext;public class FlagValidatorClass implements ConstraintValidator&lt;FlagValidator,Object&gt; &#123; /** * FlagValidator注解规定的那些有效值 */ private String values; @Override public void initialize(FlagValidator flagValidator) &#123; this.values = flagValidator.values(); &#125; /** * 用户输入的值，必须是FlagValidator注解规定的那些值其中之一。 * 否则，校验不通过。 * @param value 用户输入的值，如从前端传入的某个值 */ @Override public boolean isValid(Object value, ConstraintValidatorContext constraintValidatorContext) &#123; // 切割获取值 String[] value_array = values.split(&quot;,&quot;); Boolean isFlag = false; for (int i = 0; i &lt; value_array.length; i++)&#123; // 存在一致就跳出循环 if (value_array[i] .equals(value))&#123; isFlag = true; break; &#125; &#125; return isFlag; &#125;&#125; 使用我们自定义的注解： 123456public class User &#123; // 前端传入的flag值必须是1或2或3，否则校验失败 @FlagValidator(values = &quot;1,2,3&quot;) private String flag ; //。。。&#125; 5.2 分组校验 123456789101112131415161718192021222324import org.hibernate.validator.constraints.Length;import javax.validation.constraints.Min;import javax.validation.constraints.NotNull;public class Resume &#123; public interface Default &#123; &#125; public interface Update &#123; &#125; @NotNull(message = &quot;id不能为空&quot;, groups = Update.class) private Long id; @NotNull(message = &quot;名字不能为空&quot;, groups = Default.class) @Length(min = 4, max = 10, message = &quot;name 长度必须在 &#123;min&#125; - &#123;max&#125; 之间&quot;, groups = Default.class) private String name; @NotNull(message = &quot;年龄不能为空&quot;, groups = Default.class) @Min(value = 18, message = &quot;年龄不能小于18岁&quot;, groups = Default.class) private Integer age; //。。。&#125; 12345678910111213141516171819/** * 使用Defaul分组进行验证 * @param resume * @return */@PostMapping(&quot;/validate5&quot;)public String addUser(@Validated(value = Resume.Default.class) @RequestBody Resume resume) &#123; return &quot;validate5&quot;;&#125;/** * 使用Default、Update分组进行验证 * @param resume * @return */@PutMapping(&quot;/validate6&quot;)public String updateUser(@Validated(value = &#123;Resume.Update.class, Resume.Default.class&#125;) @RequestBody Resume resume) &#123; return &quot;validate6&quot;;&#125; 建立了两个分组，名称分别为Default、Update。POST方法提交时使用Defaut分组的校验规则，PUT方法提交时同时使用两个分组规则。 6. 异常拦截器 通过设置全局异常处理器，统一向前端返回校验失败信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import com.scj.springbootdemo.WebResult;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.util.CollectionUtils;import org.springframework.validation.ObjectError;import org.springframework.web.bind.MethodArgumentNotValidException;import org.springframework.web.bind.annotation.ControllerAdvice;import org.springframework.web.bind.annotation.ExceptionHandler;import org.springframework.web.bind.annotation.ResponseBody;import javax.validation.ConstraintViolation;import javax.validation.ConstraintViolationException;import java.util.List;import java.util.Set;/** * 全局异常处理器 */@ControllerAdvicepublic class GlobalExceptionHandler &#123; private Logger logger = LoggerFactory.getLogger(GlobalExceptionHandler.class); /** * 用来处理bean validation异常 * @param ex 异常信息 * @return 结果 */ @ExceptionHandler(ConstraintViolationException.class) @ResponseBody public WebResult resolveConstraintViolationException(ConstraintViolationException ex)&#123; WebResult errorWebResult = new WebResult(WebResult.FAILED); Set&lt;ConstraintViolation&lt;?&gt;&gt; constraintViolations = ex.getConstraintViolations(); if(!CollectionUtils.isEmpty(constraintViolations))&#123; StringBuilder msgBuilder = new StringBuilder(); for(ConstraintViolation constraintViolation :constraintViolations)&#123; msgBuilder.append(constraintViolation.getMessage()).append(&quot;,&quot;); &#125; String errorMessage = msgBuilder.toString(); if(errorMessage.length()&gt;1)&#123; errorMessage = errorMessage.substring(0,errorMessage.length()-1); &#125; errorWebResult.setInfo(errorMessage); return errorWebResult; &#125; errorWebResult.setInfo(ex.getMessage()); return errorWebResult; &#125; @ExceptionHandler(MethodArgumentNotValidException.class) @ResponseBody public WebResult resolveMethodArgumentNotValidException(MethodArgumentNotValidException ex)&#123; WebResult errorWebResult = new WebResult(WebResult.FAILED); List&lt;ObjectError&gt; objectErrors = ex.getBindingResult().getAllErrors(); if(!CollectionUtils.isEmpty(objectErrors)) &#123; StringBuilder msgBuilder = new StringBuilder(); for (ObjectError objectError : objectErrors) &#123; msgBuilder.append(objectError.getDefaultMessage()).append(&quot;,&quot;); &#125; String errorMessage = msgBuilder.toString(); if (errorMessage.length() &gt; 1) &#123; errorMessage = errorMessage.substring(0, errorMessage.length() - 1); &#125; errorWebResult.setInfo(errorMessage); return errorWebResult; &#125; errorWebResult.setInfo(ex.getMessage()); return errorWebResult; &#125;&#125; 7.参数校验注解 Java中参数校验的注解来自三个方面，分别是 javax.validation:validation-api，对应包javax.validation.constraints org.hibernate:hibernate-validator，对应包org.hibernate.validator.constraints org.springframework:spring-context，对应包org.springframework.validation JSR 303 是Bean验证的规范 ，Hibernate Validator 是该规范的参考实现，它除了实现规范要求的注解外，还额外实现了一些注解。 7.1 validation-api中的注解： 配置项 说明 适用类型 @AssertFalse 限制必须是false boolean Boolean：not null时才校验 @AssertTrue 限制必须是true boolean Boolean：not null时才校验 @Max(value) 限制必须为一个小于等于value指定值的整数，value是long型 byte/short/int/long/float/double及其对应的包装类；包装类对象not null时才校验 @Min(value) 限制必须为一个大于等于value指定值的整数，value是long型 byte/short/int/long/float/double及其对应的包装类；包装类对象not null时才校验 @DecimalMax(value) 限制必须小于等于value指定的值，value是long型 byte/short/int/long/float/double及其对应的包装类；包装类对象not null时才校验 @DecimalMin(value) 限制必须大于等于value指定的值，value是字符串类型 byte/short/int/long/float/double及其对应的包装类；包装类对象not null时才校验 @Digits(integer, fraction) 限制必须为一个小数（其实整数也可以），且整数部分的位数不能超过integer，小数部分的位数不能超过fraction。integer和fraction可以是0。 byte/short/int/long/float/double及其对应的包装类；包装类对象not null时才校验 @Null 限制只能为null 任意对象类型（比如基本数据类型对应的包装类、String、枚举类、自定义类等）；不能是8种基本数据类型 @NotNull 限制必须不为null 任意类型（包括8种基本数据类型及其包装类、String、枚举类、自定义类等）；但是对于基本数据类型，没有意义 @Size(min, max) 限制Collection类型或String的长度必须在min到max之间，包含min和max 1.Collection类型（List/Set） 2.String @Pattern(regexp) 限制必须符合regexp指定的正则表达式 String @Future 限制必须是一个将来的日期 Date/Calendar @Past 限制必须是一个过去的日期 Date/Calendar @Valid 校验任何非原子类型，标记一个对象，表示校验对象中被注解标记的对象（不支持分组功能） 需要校验成员变量的对象，比如@ModelAttribute标记的接口入参 7.2 hibernate-validator中的注解： 下面列举的注解是hibernate-validator-5.3.6版本的。 配置项 说明 适用类型 @Length(min,max) 限制String类型长度必须在min和max之间，包含min和max String, not null时才校验 @NotBlank 验证注解的元素值不是空白（即不是null，且包含非空白字符） String @NotEmpty 验证注解的元素值不为null且不为空（即字符串非null且长度不为0、集合类型大小不为0） 1.Collection类型（List/Set） 2.String @Range(min,max) 验证注解的元素值在最小值和最大值之间 1. String(数字类型的字符串)，非null时才校验 2. byte/short/int/long/float/double及其包装类，包装类非null时才校验 @Email(regexp) 验证注解的字符串符合邮箱的正则表达式，可以使用regexp自定义正则表达式 String @CreditCardNumber 验证银行借记卡、信用卡的卡号 String（不能包含空格等特殊字符） 7.3 spring-context中的注解： 配置项 说明 适用类型 @Validated 校验非原子类型对象，或启用类中原子类型参数的校验（支持分组校验；只校验包含指定分组的注解参数） 1. Controller类 2. @ModelAttribue标记的查询条件对象类 3. @RequestBody标记的请求体对象类 7.4 注解的启用 方法中对象参数中成员变量校验注解的生效条件 @ModelAttribute标记的查询条件类参数，需要同时用@Valid或@Validated标记，类中的注解校验才会生效 @RequestBody标记的请求体对象参数，需要同时用@Valid或@Validated标记，类中的注解校验才会生效 @Valid或@Validated标记在方法或方法所属类上无效 方法中原子类型参数校验注解的生效条件 @Validated标记在方法所属类上 按照分组启用 在注解中使用groups添加启用注解的分组 在@Validated中指定启用的分组 博客参考地址：https://mp.weixin.qq.com/s/0VX6lLS133CA4NFCVOHhhw","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://xiaoyuge5201.github.io/tags/springboot/"}]},{"title":"CentOS安装Maven 3.8.6","slug":"maven-install","date":"2022-06-04T01:10:10.000Z","updated":"2022-06-04T01:10:10.000Z","comments":false,"path":"maven-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/maven-install/index.html","excerpt":"","text":"1. 下载 maven官网地址：https://maven.apache.org/download.cgi ， 右键复制链接地址，wget下载，或者下载到本地再上传到Centos 可以从镜像仓库下载：https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.8.6/binaries/ ， 12cd /usr/local/toolswget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.8.6/binaries/apache-maven-3.8.6-bin.tar.gz 2. 解压 1tar -zxvf apache-maven-3.8.6-bin.tar.gz 3. 配置环境变量 1vim /etc/profile 在末尾增加两行 12export MAVEN_HOME=/usr/local/tools/apache-maven-3.8.6export PATH=$PATH:$MAVEN_HOME/bin 编译生效 1source /etc/profile 4. 验证是否配置成功 1mvn -v 如果出现版本信息则配置成功 5. 配置本地仓库和镜像仓库 12cd /usr/local/tools/apache-maven-3.8.6/confvim setting.xml 在相应注释位置添加如下两行： 1&lt;localRepository&gt;/home/mavenRepository/&lt;/localRepository&gt; 123456&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"maven","slug":"maven","permalink":"https://xiaoyuge5201.github.io/tags/maven/"}]},{"title":"Linux安装gitlab","slug":"gitlab","date":"2022-05-28T01:56:42.000Z","updated":"2022-05-28T01:56:42.000Z","comments":true,"path":"gitlab/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/gitlab/index.html","excerpt":"","text":"1. 前言 以前在自己服务器使用的是Gitblit,官网地址：http://gitblit.github.io/gitblit, 这个只需要在服务器上启动一个tomcat,然后将下载的Gitblit的war包放置在tomcat容器里面运行即可访问。 但是由于gitblit没有CI/CD的功能，于是自己就在网上找了一些博客搭建gitblit，在这里记录一下搭建的过程。 2. 安装步骤 配置yum源 1vim /etc/yum.repos.d/gitlab-ce.repo 增加一下配置 12345[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el$releasever/gpgcheck=0enabled=1 更新本地yum缓存 1yum makecache 安装GitLab社区版 123yum install gitlab-ce #自动安装最新版本 sudo yum install gitlab-ce-x.x.x #安装指定版本 更改默认端口配置(默认为80端口) 1sudo vim /etc/gitlab/gitlab.rb 修改如下配置： 123external_url &#x27;http://ip:8888&#x27; #填写自己的主机ippuma[&#x27;port&#x27;] = 8888 #如果gitlab是13版本之前请求修改 union[&#x27;port&#x27;]=8888nginx[&#x27;listen_port&#x27;] = 8888 配置gitlab-http.conf 1sudo vim /var/opt/gitlab/nginx/conf/gitlab-http.conf 修改如下： 1234server &#123; listen *:8888; #和上面保持一致 server_name ip #填写自己的ip&#125; 放开端口 12345#开放8888端口sudo firewall-cmd --add-port=8888/tcp --permanent#重新加载防火墙sudo firewall-cmd --reload 修改配置后重新加载配置文件 1sudo gitlab-ctl reconfigure 重新gitlab 1sudo gitlab-ctl restart 修改管理员登录密码 进入gitlab-rails控制台 1sudo gitlab-rails console 查找root账号(默认只有一个root用户) 1u=User.where(id:1).first 修改密码1u.password=&#x27;xiaoyuge123&#x27; 再次确认密码1u.password_confirmation=&#x27;xiaoyuge123&#x27; 保存 1u.save! 浏览器访问：http://ip:8888 至此，Gitlab搭建完毕！ 3. GitLab常用命令 123456789sudo gitlab-ctl start # 启动所有 gitlab 组件；sudo gitlab-ctl stop # 停止所有 gitlab 组件；sudo gitlab-ctl restart # 重启所有 gitlab 组件；sudo gitlab-ctl status # 查看服务状态；sudo gitlab-ctl reconfigure # 启动服务；sudo vim /etc/gitlab/gitlab.rb # 修改默认的配置文件；gitlab-rake gitlab:check SANITIZE=true --trace # 检查gitlab；sudo gitlab-ctl tail # 查看日志；gitlab-ctl show-config #查看gitlab配置信息 4. 邮件配置 修改配置 1sudo vim /etc/gitlab/gitlab.rb 新增以下内容 1234567891011gitlab_rails[&#x27;smtp_enable&#x27;] = truegitlab_rails[&#x27;smtp_address&#x27;] = &quot;mail.midea.com&quot;gitlab_rails[&#x27;smtp_port&#x27;] = 994gitlab_rails[&#x27;smtp_user_name&#x27;] = &quot;发信邮箱&quot;gitlab_rails[&#x27;smtp_password&#x27;] = &quot;发信邮箱密码&quot;gitlab_rails[&#x27;smtp_domain&#x27;] = &quot;xxx.com&quot;gitlab_rails[&#x27;smtp_authentication&#x27;] = &quot;login&quot;gitlab_rails[&#x27;smtp_enable_starttls_auto&#x27;] = truegitlab_rails[&#x27;smtp_tls&#x27;] = trueuser[&#x27;git_user_email&#x27;] = &quot;发信邮箱&quot;gitlab_rails[&#x27;gitlab_email_from&#x27;] = &#x27;发信邮箱&#x27; 测试邮件配置是否生效 12345#重新加载配置文件sudo gitlab-ctl reconfigure #查看consolesudo gitlab-rails console 1234567891011121314151617181920212223242526--------------------------------------------------------------------------------Ruby: ruby 2.7.2p137 (2020-10-01 revision 5445e04352) [x86_64-linux]GitLab: 13.7.1 (c97c8073a0e) FOSSGitLab Shell: 13.14.0PostgreSQL: 12.4--------------------------------------------------------------------------------Loading production environment (Rails 6.0.3.3)irb(main):001:0&gt; Notify.test_email(&#x27;xxxx@midea.com&#x27;,&#x27;test&#x27;,&#x27;test&#x27;).deliver_nowNotify#test_email: processed outbound mail in 1.4msDelivered mail 5ff2cb5082e2b_e45eb53d484754@devops.mail (673.8ms)Date: Mon, 04 Jan 2021 08:01:20 +0000From: GitLab &lt;xxx@midea.com&gt;Reply-To: GitLab &lt;noreply@ip&gt;To: xxxx@midea.comMessage-ID: &lt;5ff2cb5082e2b_e45eb53d484754@devops.mail&gt;Subject: testMime-Version: 1.0Content-Type: text/html;charset=UTF-8Content-Transfer-Encoding: 7bitAuto-Submitted: auto-generatedX-Auto-Response-Suppress: All&lt;!DOCTYPE html PUBLIC &quot;-//W3C//DTD HTML 4.0 Transitional//EN&quot; &quot;http://www.w3.org/TR/REC-html40/loose.dtd&quot;&gt;&lt;html&gt;&lt;body&gt;&lt;p&gt;test&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;=&gt; #&lt;Mail::Message:199260, Multipart: false, Headers: &lt;Date: Mon, 04 Jan 2021 08:01:20 +0000&gt;, &lt;From: GitLab &lt;xxx@midea.com&gt;&gt;, &lt;Reply-To: GitLab &lt;noreply@ip&gt;&gt;, &lt;To: xxxx@midea.com&gt;, &lt;Message-ID: &lt;5ff2cb5082e2b_e45eb53d484754@devops.mail&gt;&gt;, &lt;Subject: test&gt;, &lt;Mime-Version: 1.0&gt;, &lt;Content-Type: text/html; charset=UTF-8&gt;, &lt;Content-Transfer-Encoding: 7bit&gt;, &lt;Auto-Submitted: auto-generated&gt;, &lt;X-Auto-Response-Suppress: All&gt;&gt;出现以上信息说明配置成功。 5. 性能调优 1sudo vim /etc/gitlab/gitlab.rb 新增以下内容并保存退出： 1234567unicorn[&#x27;worker_processes&#x27;] = 2 #官方建议值为CPU核数+1（服务器只部署gitLab的情况下），可提高服务器响应速度，此参数最小值为2，设为1服务器可能卡死unicorn[&#x27;work_timeout&#x27;] = 60 #设置超时时间unicorn[&#x27;worker_memory_limit_min&#x27;] = &quot;200 * 1 &lt;&lt; 20&quot; #减少最小内存unicorn[&#x27;worker_memory_limit_max&#x27;] = &quot;300 * 1 &lt;&lt; 20&quot; #减少最大内存postgresql[&#x27;shared_buffers&#x27;] = &quot;128MB&quot; #减少数据库缓存postgresql[&#x27;max_worker_processes&#x27;] = 6 #减少数据库并发数sidekiq[&#x27;concurrency&#x27;] = 15 #减少sidekiq并发数 12345#每次修改了配置，都需要重新加载sudo gitlab-ctl reconfigure#重启sudo gitlab-ctl restart 6. GitLab使用 6.1 创建Project 安装Git工具 1yum install git 生成密钥文件：使用ssh-keygen生成密钥文件 .ssh/id_rsa.pub 1ssh-keygen 在Gitlab上创建一个project 添加ssh key导入步骤2中生成的密钥文件内容 ssh key添加完成： 6.2 配置git 配置Git仓库人员 1git config --global user.name &quot;xiaoyuge&quot; local（默认，高级优先）：只影响本地仓库 global(中优先级)：只影响所有当前用户的git仓库 system（低优先级）：影响到全系统的git仓库 配置Git仓库人员email 1git config --global user.email &quot;xiaoyuge0318@qq.com&quot; 克隆项目 1git clone git@fase11h12dsa24fdv3Q:root/test.git 6.3 git常用命令 123456789101112131415161718192021222324252627282930313233343536373839404142434445#查看某个命令文档git help &lt;command&gt;git &lt;command&gt; -hgit &lt;command&gt; --help#初始化仓库git init #添加文件内容到暂存区（同时文件被跟踪）git add#添加所有文件git add .truegit rm --cached #仅从暂存区删除git rm #从暂存区与工作目录同时删除git rm $(git ls-files --deleted) #删除所有被跟踪，但是在工作目录被删除的文件git -commit -m &#x27;first commit&#x27; #从暂存区提交 -m：注释git commit -a -m &#x27;full commit&#x27; #从工作区提交git log #查看提交历史记录git log --onlinegit log --color --graphgit diff #工作区与暂存区的差异git diff --cached [&lt;reference&gt;] #暂存区与某次提交的差异，默认为HEADgit diff [&lt;reference&gt;] #工作区与某次提交的差异，默认为HEADgit checkout -- &lt;file&gt; #将文件内容从暂存区复制到工作目录#撤销暂存区内容git reset HEAD &lt;file&gt; #将文件内容从上次提交复制到缓存区git checkout HEAD -- &lt;file&gt; #将内容从上次提交复制到工作目录#对状态的跟踪:git中有两个状态：内容状态和文件状态，#内容状态标示内容文件的改变，有三个区域：工作目录，暂存区和提交区#文件状态有两个状态：已跟踪和未跟踪git status 分支操作： 12345678910111213git branch &lt;branchName&gt; #创建一个分支git branch -d &lt;branchName&gt; #删除一个分支git branch -v #显示所有分支信息git checkout &lt;branchName&gt; #通过移动HEAD检出版本，可用于切换分支git checkout -b &lt;branchName&gt; #创件一个分支并切换git checkout &lt;reference&gt; #将其移动到一个引用git checkout - #恢复到上一个分支git reset #将当前分支回退到历史某个版本git reset --mixed &lt;commit&gt; #(默认)git reset --soft&lt;commit&gt; git reset --hard &lt;commit&gt; 7. 常见问题 gitlab本身采用gitlab.example.com:80端口，如安装前服务器有启用80，安装完访问会报错。需更改gitlab的默认端口。 修改vim /etc/gitlab/gitlab.rb：external_url 'http://localhost:8888 如果就想用80端口，那没问题。如果更改了端口，后边可以自行调整nginx配置文件进行nginx反向代理设置。 日志位置：/var/log/gitlab 可以进去查看访问日志以及报错日志等，供访问查看以及异常排查。 gitlab内存消耗过大，频繁出现502： http://www.360doc.com/content/22/0130/08/65839659_1015422932.shtml gitlab-ctl tail #查看所有日志 gitlab-ctl tail nginx/gitlab_access.log #查看nginx访问日 参考博客：https://blog.csdn.net/yzd524850313/article/details/113118193 参考博客：https://zhuanlan.zhihu.com/p/338882906","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"gitlab","slug":"gitlab","permalink":"https://xiaoyuge5201.github.io/tags/gitlab/"}]},{"title":"Linux安装Apache Http Server","slug":"Apache-Http-Server","date":"2022-05-23T04:14:00.000Z","updated":"2022-05-23T04:14:00.000Z","comments":true,"path":"Apache-Http-Server/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/Apache-Http-Server/index.html","excerpt":"","text":"1. 前言 Apache Http Server又可以简称为httpd或者Apache，它是Internet使用最广泛的web服务器之一 Httpd是Apache超文本传输协议（HTTP）服务器的主程序。被设计为一个独立运行的后台进程，它会建立一个处理请求的子进程或线程的池。 通常，httpd不应该被直接调用，而应该在类Unix系统中由apachectl调用，在Windows中做为服务运行。 Httpd主要特点： （1）开放源代码 （2）跨平台使用，支持绝大多数硬件平台，支持所有的unix系统和linux系统，支持大多数windows平台 （3）支持多种web编程语言，perl，php，python，java等 （4）模块化设计，根据所需功能去安装不同的模块 （5）运行非常稳定，支持大负载访问的web站点 （6）安全性高，及时修复已发现的漏洞 1.1 Httpd版本 主要介绍httpd的两大版本： httpd-2.2 httpd-2.4 官网链接：https://httpd.apache.org/ 1.2 Httpd特性 httpd-2.2 特性： 事先创建进程 按需维持适当的进程 模块化设计，核心比较小，各种功能通过模块添加(包括PHP)，支持运行时配置，支持单独编译模块 支持多种方式的虚拟主机配置，如基于ip的虚拟主机，基于端口的虚拟主机，基于域名的虚拟主机等 支持https协议(通过mod_ ssI模块实现) 支持用户认证 支持基于IP或域名的ACL访问控制机制 支持每目录的访问控制(用户访问默认主页时不需要提供用户名和密码，但是用户访问某特定目录时需要提供用户名和支持URL重写 支持MPM (Multi Path Modules,多处理模块)。用于定义httpd的工作模型(单进程、 单进程多线程、多进程、多进程单线程、多进程多线程） httpd-2.4 特性： MPM支持运行DSO机制(Dynamic Share Object,模块的动态装/卸载机制)，以模块形式按需加载 支持event MPM, event MPM模块生产环境可用 支持异步读写 支持每个模块及每个目录分别使用各自的日志级别 每个请求相关的专业配置，使用来配置 增强版的表达式分析器 支持毫秒级的keepalive timeout 基于FQDN的虚拟主机不再需要Name Virtual Host指令 支持用户自定义变量 支持新的指令(Allow Override List) 降低对内存的消耗 1.3 httpd自带的工具 2. 源码安装 下载安装包 1234567链接: https://pan.baidu.com/s/1ip-3yUhY1XYdkD8KejgQkQ 提取码: bcub #第二种：http://archive.apache.org/dist/apr/apr-1.7.0.tar.gzhttp://archive.apache.org/dist/apr/apr-util-1.6.1.tar.gzhttps://nchc.dl.sourceforge.net/project/pcre/pcre/8.45/pcre-8.45.zip 上传安装包到服务器目录 配置依赖环境 配置apr依赖 1234567891011 #解压 tar -xf apr-1.7.0.tar.gz #进入到目录 cd apr-1.7.0#配置(设置安装在/usr/local/apr)./configure --prefix=./configure --prefix=/usr/local/apr #编译并安装 make &amp;&amp; make install 配置apr-util依赖 1234567891011 #解压tar -xf apr-util-1.6.1.tar.gz #进入到目录 cd apr-util-1.6.1/ #配置(设置安装在/usr/local/apr-util) ./configure --prefix=/usr/local/apr-util --with-apr=/usr/local/apr #编译并安装 make &amp;&amp; make install 配置pcre依赖 1234567891011#解压tar -xf pcre-8.44.tar.bz2#进入到目录cd pcre-8.44/#配置(设置安装在/usr/local/pcre)./configure --prefix=/usr/local/pcre#编译并安装make &amp;&amp; make install 配置安装httpd 1234567891011#解压tar -xf httpd-2.4.53.tar.gz#进入到目录cd httpd-2.4.53/#配置(设置安装在/usr/local/httpd)./configure --prefix=/usr/local/httpd --with-pcre=/usr/local/pcre --with-apr=/usr/local/apr --with-apr-util=/usr/local/apr-util#编译并安装：make &amp;&amp; make install 配置环境变量 1echo &#x27;PATH=/usr/local/httpd/bin:$PATH&#x27; &gt; /etc/profile.d/httpd.sh &amp;&amp; . /etc/profile.d/httpd.sh 添加Httpd服务 复制到init.d 并重命名为httpd 1cp /apps/httpd24/bin/apachectl /etc/rc.d/init.d/httpd 建立软连接，通过service命令启动/关闭 12#链接文件的S61是启动时的序号。当init.d目录下有httpd脚本后，我们就可以通过service命令来启动关闭apache了ln -s /etc/rc.d/init.d/httpd /etc/rc.d/rc3.d/S61httpd 编辑httpd脚本， 在第2行(#!/bin/sh下面)添加如下注释信息（包括#）：12# chkconfig: 2345 85 15# description: Apache 第一行的3个参数意义分别为：在哪些运行级别启动httpd（3，5）；启动序号（S61）；关闭序号（K61）。注意：第二行的描述必须要写！ 通过搜索ServerName（先Esc，在输入:/ServerName） 打开ServerName这一行注释 12345678## ServerName gives the name and port that the server uses to identify itself.# This can often be determined automatically, but we recommend you specify# it explicitly to prevent problems during startup.## If your host doesn&#x27;t have a registered DNS name, enter its IP address here.#ServerName localhost:80 所有开机模式下自启动 12#所有开机模式下自启动，另外chkconfig httpd on 表示345模式下自启动chkconfig --add httpd 启动服务 1234567891011121314151617#到bin目录开启服务cd /usr/local/httpd/bin#开启服务：./apachectl start#或者 service httpd start#或者 systemctl start httpd.service #查看运行状态systemctl status httpd.service#查询80端口：netstat -anlp | grep 80#打开浏览器输入虚拟机的IP地址如果页面显示 It works! , 则表示安装成功 3. rpm安装 下载rpm文件 1http://repo.almalinux.org/almalinux/9/AppStream/x86_64/os/Packages/ 依次安装依赖包 123456789cd /var/www/html/yum/# 依次安装以下依赖包rpm -ivh apr-1.4.8-3.el7.x86_64.rpmrpm -ivh apr-util-1.5.2-6.el7.x86_64.rpm rpm -ivh mailcap-2.1.41-2.el7.noarch.rpm rpm -ivh httpd-tools-2.4.6-40.el7.centos.x86_64.rpm rpm -ivh httpd-2.4.6-40.el7.centos.x86_64.rpm rpm -ivh postgresql-libs-9.2.13-1.el7_1.x86_64.rpm #（经测试，不需要） 启动服务 12345systemctl status httpdsystemctl start httpdsystemctl enable httpdsystemctl is-enabled httpd 配置 1234cd /etc/yum.repos.d/mkdir bakmv *.repo bak/vim local.repo 输入以下内容后保存 123456[local_server]name=This is a local repobaseurl=http://192.168.1.161:80/yum/enabled=1gpgcheck=0#gpgkey=0 创建yum仓库 12345cd /var/www/html/yum/createrepo ./ # 使用createrepo命令创建yum仓库# 注意:配置文件中路径指向到哪一层级，createrepo就在该路径的子目录下执行即可。# 例: 路径在local.repo配置文件中指向/a/b/c，那么就 cd /a/b/c createrepo ./ 清除缓存 1yum clean all 建立新缓存 1yum makecache 查看已建立好的缓存 1yum repolist 测试 1yum install -y sendmail # 若可以安装，即已安装成功 4. yum安装 检查是否已经安装httpd 1rpm -qa httpd 安装 12345 yum -y install httpd#如果提示： 没有可用软件包 httpd。#国内各大源好像把httpd移除， 导致包根本找不到，可以试下下面的命令# yum --disableexcludes=all install -y httpd 配置ServerName 1vim /etc/httpd/conf/httpd.conf 则：ServerName localhost:80 或者 ServerName 127.0.0.1:80 启动 123service httpd start #启动httpdservice httpd restart #重启httpdchkconfig httpd on #设置开机自动启动： 测试 12 touch /var/www/html/index.html#输入以下内容 12345&lt;html&gt; &lt;body&gt; Apache Http Server &lt;/body&gt;&lt;/html&gt; 访问http://ip:port/hello.html，返回结果&quot;hello&quot; 说明没问题 安装目录说明 1231.Apache默认将网站的根目录指向/var/www/html 目录2.默认的主配置文件是/etc/httpd/conf/httpd.conf3. 配置存储在的/etc/httpd/conf.d/目录 5. 常见问题 AH00534 123vim /apps/httpd24/conf/httpd.cof#加上： LoadModule mpm_prefork_module modules/mod_mpm_prefork.so#重启","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"Apache","slug":"Apache","permalink":"https://xiaoyuge5201.github.io/tags/Apache/"}]},{"title":"常见的接口测试工具","slug":"swagger","date":"2022-04-17T09:10:15.000Z","updated":"2022-04-17T09:10:15.000Z","comments":true,"path":"swagger/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/swagger/index.html","excerpt":"","text":"1. Swagger Swagger 是一个规范且完整的框架，用于生成、描述、调用和可视化 RESTful 风格的 Web 服务。 Swagger 的目标是对 REST API 定义一个标准且和语言无关的接口，可以让人和计算机拥有无须访问源码、文档或网络流量监测就可以发现和理解服务的能力。 静态的swagger跟不上频繁变更的代码，容易出现以下问题 “为什么改了这个没告诉我” “实际功能和文档上说的不一样啊” 这样会带来的问题是： Swagger，postman，MockJS只能完成软件研发流程中某个环节的功能，造成完成接口设计，文档编写，调试，测试验证等工作需要使用好几个工具； 更麻烦的是这些工具数据格式不互通，无法互相导入，造成用Swagger定义和编写完成接口后，在Postman，MockJS，Jmeter等工具还要再去手动填写一遍才能开始工作，增加了无意义的工作量。 沟通成本总是被忽略不计，但实际上不仅占据了很大时间，各种沟通不及时、沟通不到位还非常让人心累。 老板的需求来得急，老板的需求变得快，各种代码修改和变更难以及时通知和同步到团队成员手中。 2. 常见的可视化RestFul风格的服务 springfox-swagger2 springdoc Apifox（接口测试工具，非集成在项目中postman加强升级版） 3. 各个工具的使用以及风格 使用Springboot项目分别集成各个组件，看下具体的实现效果； 3.1 springfox-swagger2 引入依赖 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger2&lt;/artifactId&gt; &lt;version&gt;2.10.5&lt;/version&gt;&lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;io.springfox&lt;/groupId&gt; &lt;artifactId&gt;springfox-swagger-ui&lt;/artifactId&gt; &lt;version&gt;2.9.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt; &lt;artifactId&gt;knife4j-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.0.8&lt;/version&gt;&lt;/dependency&gt; 编写配置类 SwaggerConfiguration类 12345678910111213141516171819202122232425262728293031@Configuration@EnableSwagger2WebMvc@EnableKnife4jpublic class SwaggerConfiguration &#123; @Value(&quot;$&#123;spring.application.name&#125;&quot;) private String applicationName; @Bean(value = &quot;defaultApi&quot;) public Docket defaultApi2() &#123; //设置处理请求的包，我的controller类都在com.yugb.controller中 Predicate&lt;RequestHandler&gt; apiPackage = RequestHandlerSelectors.basePackage(&quot;com.yugb.controller&quot;); Docket docket = new Docket(DocumentationType.SWAGGER_2) .apiInfo(new ApiInfoBuilder() .version(&quot;1.0&quot;) .contact(new Contact(&quot;xiaoyuge&quot;,&quot;123&quot;, &quot;12342qq.com&quot;)) .title(applicationName + &quot; 文档中心&quot;) .description(&quot;&lt;div style=&#x27;font-size:15px;&#x27;&gt;&quot; + applicationName + &quot; RESTful APIs&lt;/div&gt;&quot;) .build()) //分组名称 .groupName(&quot;2.X版本&quot;) .select() //这里指定Controller扫描包路径 .apis(apiPackage) .paths(PathSelectors.any()) .build(); return docket; &#125;&#125; SwaggerWebMvcConfigurer 类 12345678@Configurationpublic class SwaggerWebMvcConfigurer implements WebMvcConfigurer &#123; @Override public void addResourceHandlers(ResourceHandlerRegistry registry) &#123; registry.addResourceHandler(&quot;doc.html&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/&quot;); registry.addResourceHandler(&quot;/webjars/**&quot;).addResourceLocations(&quot;classpath:/META-INF/resources/webjars/&quot;); &#125;&#125; 添加注解 我们接口文档的直接描述主要就是在Controller这一层，比如这个接口的功能，参数的名称，返回值的名称等。这些值我们都需要在Controller上通过给方法上，请求参数和返回参数上添加对应的注解，swagger才能帮我们生成相应的接口文档。 JavaBean: @ApiModel注解和 @@ApiModelProperty 注解定义了实体的名称和字段的名称 12345678910111213141516@Data@ApiModel(&quot;创建Swagger响应结果&quot;)public class SwaggerResVO &#123; @ApiModelProperty(&quot;id&quot;) private Integer id; @ApiModelProperty(&quot;姓名&quot;) private String name; @ApiModelProperty(&quot;性别&quot;) private Integer gender; @ApiModelProperty(&quot;啥啥&quot;) private String what;&#125; controller: @Api注解和 @ApiOperation注解分别标注了接口组名和接口的名称 1234567891011121314151617@RestController@RequestMapping(&quot;/swagger&quot;)@Api(value = &quot;用户接口&quot;, tags = &#123;&quot;用户接口&quot;&#125;)public class SwaggerController &#123; @ApiOperation(&quot;新增用户&quot;) @PostMapping(&quot;save&quot;) public String save(@RequestBody SwaggerReqVO req) &#123; return &quot;success&quot;; &#125; @GetMapping(&quot;getById&quot;) @ApiOperation(&quot;根据条件查询用户&quot;) public SwaggerResVO getById(@RequestBody SwaggerResVO req) &#123; return new SwaggerResVO(); &#125;&#125; 启动项目 访问 http://localhost:8080/doc.html 查看springfox-swagger2的文档中心 查看GET请求的界面 优缺点 优点：界面美观，集成方便，不同类型的接口按照controller分组，可以导出所有的接口文档！！！！！ 缺点：暂时没有遇到 3.2 springdoc 引入依赖 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springdoc&lt;/groupId&gt; &lt;artifactId&gt;springdoc-openapi-ui&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springdoc&lt;/groupId&gt; &lt;artifactId&gt;springdoc-openapi-webmvc-core&lt;/artifactId&gt; &lt;version&gt;1.5.8&lt;/version&gt; &lt;/dependency&gt; 配置yml /yaml /properties 1234567891011121314springdoc: api-docs: enabled: true groups: enabled: true path: /api-docs cache: disabled: true swagger-ui: groups-order: asc # 自定义的文档界面访问路径。默认访问路径是/swagger-ui.html path: /springdoc/docs.html # 布尔值。实现OpenApi规范的打印。 writer-with-default-pretty-printer: true 编写配置类 SpringdocOpenapiConfiguration 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788@Configurationpublic class SpringdocOpenapiConfiguration implements WebMvcConfigurer &#123; private final SwaggerProperties swaggerProperties; public SpringdocOpenapiConfiguration(SwaggerProperties swaggerProperties) &#123; this.swaggerProperties = swaggerProperties; &#125; @Bean public OpenAPI springDocOpenAPI() &#123; //配置认证、请求头参数 Components components = new Components();// Map&lt;String, Object&gt; myHeader2extensions = new HashMap&lt;&gt;(2);// myHeader2extensions.put(&quot;name&quot;, &quot;myHeader2&quot;);// components// .addSecuritySchemes(&quot;bearer-key&quot;, new SecurityScheme().type(SecurityScheme.Type.HTTP).scheme(&quot;bearer&quot;).bearerFormat(&quot;JWT&quot;))// .addSecuritySchemes(&quot;basicScheme&quot;, new SecurityScheme().type(SecurityScheme.Type.HTTP).scheme(&quot;basic&quot;))// .addHeaders(&quot;myHeader2&quot;, new Header().description(&quot;myHeader2 header&quot;).schema(new StringSchema()).extensions(myHeader2extensions))// .addParameters(&quot;myGlobalHeader&quot;, new HeaderParameter().required(true).name(&quot;My-Global-Header&quot;).description(&quot;My Global Header&quot;).schema(new StringSchema()).required(false))// ; // 接口调试路径 Server tryServer = new Server(); tryServer.setUrl(swaggerProperties.getTryHost()); return new OpenAPI() .components(components) .servers(Collections.singletonList(tryServer)) .info(new Info() .title(swaggerProperties.getApplicationName() + &quot; Api Doc&quot;) .description(swaggerProperties.getApplicationDescription()) .version(&quot;Application Version: &quot; + swaggerProperties.getApplicationVersion() + &quot;\\n Spring Boot Version: &quot; + SpringBootVersion.getVersion()) .license(new License().name(&quot;Apache 2.0&quot;).url(&quot;https://www.apache.org/licenses/LICENSE-2.0.html&quot;)) ) .externalDocs(new ExternalDocumentation() .description(&quot;SpringDoc Full Documentation&quot;) .url(&quot;https://springdoc.org/&quot;) ); &#125; /** * 添加全局的请求头参数 */// @Bean// public OpenApiCustomiser customerGlobalHeaderOpenApiCustomiser() &#123;// return openApi -&gt; openApi.getPaths().values().stream().flatMap(pathItem -&gt; pathItem.readOperations().stream())// .forEach(operation -&gt; &#123;// operation.addParametersItem(new HeaderParameter().$ref(&quot;#/components/parameters/myGlobalHeader&quot;));// &#125;);// &#125; /** * 通用拦截器排除设置，所有拦截器都会自动加springdoc-opapi相关的资源排除信息，不用在应用程序自身拦截器定义的地方去添加，算是良心解耦实现。 */ @SuppressWarnings(&quot;unchecked&quot;) @Override public void addInterceptors(InterceptorRegistry registry) &#123; try &#123; Field registrationsField = FieldUtils.getField(InterceptorRegistry.class, &quot;registrations&quot;, true); List&lt;InterceptorRegistration&gt; registrations = (List&lt;InterceptorRegistration&gt;) ReflectionUtils.getField(registrationsField, registry); if (registrations != null) &#123; for (InterceptorRegistration interceptorRegistration : registrations) &#123; interceptorRegistration.excludePathPatterns(&quot;/springdoc**/**&quot;); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; // 服务器支持跨域 @Override public void addCorsMappings(CorsRegistry registry) &#123; registry.addMapping(&quot;/**&quot;) .allowedOrigins(&quot;*&quot;) .allowedMethods(&quot;GET&quot;, &quot;POST&quot;, &quot;OPTIONS&quot;) .allowedHeaders(&quot;*&quot;) .exposedHeaders(&quot;Access-Control-Allow-Headers&quot;, &quot;Access-Control-Allow-Methods&quot;, &quot;Access-Control-Allow-Origin&quot;, &quot;Access-Control-Max-Age&quot;, &quot;X-Frame-Options&quot;) .allowCredentials(false) .maxAge(3600); &#125;&#125; SwaggerProperties 12345678910111213141516171819202122@Getter@Setter@Component@ConfigurationProperties(&quot;swagger&quot;)public class SwaggerProperties &#123;/*** 项目应用名*/private String applicationName; /** * 项目版本信息 */ private String applicationVersion; /** * 项目描述信息 */ private String applicationDescription; /** * 接口调试地址 */ private String tryHost;&#125; 编写接口方法 在controller上添加@Tag注解 在接口方法上添加@Operation 注解 在接口参数添加@Parameter 或@Parameters 注解 12345678910111213141516171819202122@Tags(&#123; @Tag(name = &quot;ExpirationWarningController&quot;, description = &quot;设备寿命到期预警&quot;),&#125;)@RestController@RequestMapping(&quot;/test&quot;)public class ExpirationWarningController &#123; //需要使用@Operation竹节 @PostMapping(&quot;/getOne/&#123;id&#125;/&#123;type&#125;&quot;) @Operation(summary = &quot;按ID查询&quot;, description = &quot;按ID查询&quot;) public ResponseResult getOne(@Parameter(description = &quot;主键ID&quot;) @PathVariable Integer id, @Parameter(description = &quot;类型&quot;) @PathVariable String type) &#123; return ResponseResult.success(expirationWarningVO); &#125; @PostMapping(&quot;/save&quot;) @ResponseBody @Operation(summary = &quot;保存&quot;, description = &quot;保存&quot;) public ResponseResult save(@RequestBody ExpirationWarning expirationWarning) &#123; return null; &#125; &#125; 在springboot启动类上加上以下配置 12345678910111213141516171819202122232425@OpenAPIDefinition( info = @Info( title = &quot;测试springdoc&quot;, version = &quot;1.0&quot; ), externalDocs = @ExternalDocumentation(description = &quot;swagger-api参考文档&quot;, url = &quot;https://github.com/swagger-api/swagger-core/wiki/Swagger-2.X---Annotations&quot; ), servers = &#123; @Server( url = &quot;http://localhost:8123/app&quot;, description = &quot;本地地址&quot; ), @Server( url = &quot;http://www.xiaoyuge.vip/app&quot;, description = &quot;公网测试环境&quot; ) &#125;)public class ResolutionApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(ResolutionApplication.class, args); logger.info(&quot;============= Start Success =============&quot;); &#125;&#125; 启动项目 访问 http://localhost:8123/app/springdoc/swagger-ui/index.html 查看springdox的文档中心 优缺点 优点：不好说 缺点：界面没有按照每个controller分组，不直观！； 3.3 Apifox 一款研发全流程，接口全周期的生产力工具，这款软件真正完成了数据流的打通，在一个软件上就能实现接口设计–接口文档–接口调试–接口修改–接口mock–接口测试–接口自动化–接口迭代的工作流闭环； 3.3.1 Apifox上的协作流程 后端在Apifox可视化接口设计界面上定义好项目各个接口及对应参数同时编写接口文档说明 前后端一起评审，修改完善接口并在同一界面顺手更新接口文档 后端使用接口调试功能调试接口 前端使用零配置高仿真mock功能对前端页面进行调试，无需手写mock规则 后端使用代码生成功能直接生成接口代码 测试在接口管理页面一键生成接口参数测试用例,并依据业务场景生成自动化测试用例，一键运行接口用例并生成接口测试报告并分享给相关人员。 前后端 都开发完，前端从Mock 数据切换到正式数据，进行联调，由于使用同一个接口数据源，且遵循了接口规范，联调顺利 由于bug修复或需求变更，接口发生了变化，后端修改提交后，前端和测试实时同步到了修改后的数据 项目经理通过权限设置给研发,产品,测试,外部合作人员分配编辑,只读等各种操作权限，维护了项目安全 项目经理通过各个接口的状态开发中,测试中,已发布来跟进项目的进度情况，把控项目风险。 3.3.2 Apifox做的增速提效优化 接口设计：从代码生成界面到可视化接口设计界面 Apifox 接口文档遵循 OpenApi 3.0 (原 Swagger)、JSON Schema 规范，可生成在线文档；零学习成本即可编写出符合RESTful风格的接口文档，新人上手快；所见即所得，不易出错 文档维护：从接口与文档分离到接口与文档合并 Apifox的接口设计界面提供了Markdown格式的文档说明区，修改完接口就如同commit代码时添加变更说明般 数据复用：从各自为政到定义一次、多次复用 接口数据复用：Swagger，Postman，MockJS，Jmeter等软件彼此之间数据不互通，数据格式不一致，接口导入非常耗时麻烦。 而Apifox能身兼多职，包揽上述软件功能，在Apifox中定义一次接口，能被后端直接用来调试，前端直接用来mock界面，测试直接执行接口自动化。 数据模型复用：可复用的数据结构，定义接口返回数据结构及请求参数数据结构（仅 JSON 和 XML 模式）时可直接引用。 同步更新，高效沟通:从沟通滞后到数据变更即时同步 Apifox为此提供了同步功能，一旦接口数据有更新发生，就会即时同步更新并通知到项目内所有成员。 Apihub 内置企业微信开放API，抖音开放API等第三方接口开放项目，接口可以直接在Apifox中调试，不需要到处找接口文档和手工填写接口 3.3.3 下载地址 官网地址： http://www.apifox.cn/?utm_medium=WCSA&amp;utm_source=xxzsq","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"swagger","slug":"swagger","permalink":"https://xiaoyuge5201.github.io/tags/swagger/"}]},{"title":"基于Springboot导出数据库表结构文档","slug":"export-database-file","date":"2022-04-15T03:46:03.000Z","updated":"2022-04-15T03:46:03.000Z","comments":true,"path":"export-database-file/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/export-database-file/index.html","excerpt":"","text":"在项目中经常会需要查询数据库所有的表以及表字段，然后可能还需要导出到Excel中，然后自己写了一个工具类，目前支持sqlserver、mysql、oracle、Postgre；如果有问题请留言！！！ 1. 引入依赖包 123456 &lt;!-- 请尽量用最新版本 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.xiaoyuge5201&lt;/groupId&gt; &lt;artifactId&gt;datasource-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt;&lt;/dependency&gt; 2. 编写Java代码 通过MyDataSourceProperties类所有本项目的数据库连接信息，导出当前连接库的数据库结构 数据库配置 yml 12345678spring: datasource: type: com.alibaba.druid.pool.DruidDataSource url: jdbc:mysql://localhost:3306/dbname?serverTimezone=GMT%2B8&amp;useSSL=false username: root password: xiaoyuge driver-class-name: com.mysql.jdbc.Driver database: dbname ##需要配置数据库名称 导出方法 123456789101112131415161718192021222324import com.github.xiaoyuge5201.config.MyDataSourceProperties;import com.github.xiaoyuge5201.util.ExportDatabaseDocument;import org.apache.catalina.servlet4preview.http.HttpServletRequest;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import javax.servlet.http.HttpServletResponse;@Controller@RequestMapping(&quot;/export&quot;)public class TestController &#123; @Autowired MyDataSourceProperties properties; /** * 注意：需要在yaml /yml 配置文件中配置spring.datasource.database 属性 */ @GetMapping(&quot;/index&quot;) public void index(HttpServletResponse response, HttpServletRequest request) &#123; ExportDatabaseDocument.export(response, request, properties); &#125;&#125; 自定义导出某个数据库的表结构信息 12345@GetMapping(&quot;/index&quot;)public void index(HttpServletResponse response, HttpServletRequest request) &#123; //手动传参 ExportDatabaseDocument.export(response, request, DatabaseDriverEnum.MYSQL.getDriver(), &quot;127.0.0.1:3306&quot;, &quot;root&quot;, &quot;xiaoyuge&quot;, &quot;dbname&quot;);&#125; 3. 导出文档 执行请求：localhost:8080/export/index 即可；导出的内容如下： 包括数据库表名、描述以及各个字段的类型、长度、默认值、描述等。。。； 另外sheet的名称为表名(表中文名)+ 4位随机值，受限于excel的sheet； 4. 数据库操作类 DataSourceClient DataSourceClient类中根据MyDataSourceProperties操作数据库 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/*** 查询所有的表结构信息** @return 表结构列表*/public List&lt;String&gt; findAllTables() &#123; return QuerySqlUtil.findAllTables(properties.getDriverClassName(), properties.getUrl(), properties.getUsername(), properties.getPassword(), properties.getDatabase());&#125;/** * 查詢數據庫表的字段信息 * * @param table 数据表 * @return 表字段列表 */public List&lt;ColumnEntity&gt; queryTableFieldsEntity(String table) &#123; return QuerySqlUtil.queryTableFieldsToColumnEntity(properties.getDriverClassName(), properties.getUrl(), properties.getUsername(), properties.getPassword(), properties.getDatabase(), table);&#125;/** * 查詢數據庫表的字段信息 * * @param table 数据表 * @return 表字段列表 */public List&lt;String&gt; queryTableFields(String table) &#123; return QuerySqlUtil.queryTableFields(properties.getDriverClassName(), properties.getUrl(), properties.getUsername(), properties.getPassword(), properties.getDatabase(), table);&#125;/** * 查询对应库下所有字段 信息 * * @return 结果 */public List&lt;ColumnEntity&gt; listColumnsByDatasourceParams() &#123; return QuerySqlUtil.listColumnsByDatasourceParams(properties.getDriverClassName(), properties.getUrl(), properties.getUsername(), properties.getPassword(), properties.getDatabase());&#125;/** * 分页查询数据表数据 * * @param table 数据表 * @param pageNo 页码 * @param limit 页容量 * @param columns 字段列表 * @throws Exception 异常信息 * @return 结果 */public JSONArray queryPageData(String table, List&lt;String&gt; columns, Integer pageNo, Integer limit) throws Exception &#123; return QuerySqlUtil.queryPageData(properties.getDriverClassName(), properties.getDatabase(), table, properties.getUrl(), properties.getUsername(), properties.getPassword(), columns, pageNo, limit);&#125;/** * 导出数据库设计文档 * * @param response 返回对象 * @param request 请求对象 */public void exportDatabaseDocument(HttpServletResponse response, HttpServletRequest request) &#123; ExportDatabaseDocument.export(response, request, properties.getDriverClassName(), properties.getUrl(), properties.getUsername(), properties.getPassword(), properties.getDatabase());&#125; 5. 数据库驱动枚举类 DatabaseDriverEnum 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374/** * 数驱动的常量 * * @author yugb */public enum DatabaseDriverEnum &#123; /** * mysql数据库 */ MYSQL(1, &quot;com.mysql.jdbc.Driver&quot;, &quot;mysql数据库&quot;), /** * Sql Server数据库 */ SQL_SERVER(2, &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;, &quot;Sql Server数据库&quot;), /** * oracle数据库 */ ORACLE(3, &quot;oracle.jdbc.driver.OracleDriver&quot;, &quot;oracle数据库&quot;), /** * postgre sql数据库 */ POSTGRE_SQL(4, &quot;org.postgresql.Driver&quot;, &quot;postgre sql数据库&quot;), /** * 达梦数据库 */ DM(5, &quot;dm.jdbc.driver.DmDriver&quot;, &quot;达梦数据库&quot;); /** * 数据库驱动类型 */ private final Integer type; /** * 数据库驱动连接 */ private final String driver; /** * 名称 */ private final String name; public Integer getType() &#123; return type; &#125; public String getDriver() &#123; return driver; &#125; public String getName() &#123; return name; &#125; DatabaseDriverEnum(Integer type, String driver, String name) &#123; this.type = type; this.driver = driver; this.name = name; &#125; /** * 根据数据库类型获取数据库驱动 * @param type 数据库类型 * @return 驱动 */ public static String getValue(Integer type) &#123; DatabaseDriverEnum[] enums = values(); for (DatabaseDriverEnum driverEnum : enums) &#123; if (driverEnum.type.equals(type)) &#123; return driverEnum.getDriver(); &#125; &#125; return null; &#125;&#125;","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"java8的lambda表达式语法","slug":"lambda","date":"2022-04-13T12:32:44.000Z","updated":"2022-04-13T12:32:44.000Z","comments":true,"path":"lambda/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/lambda/index.html","excerpt":"","text":"记录一下用到的一些java8的lambda表达式语法 1 list集合根据某个字段分组后求多个字段的和 12345678910111213 List&lt;SafeSystemVO&gt; list = new ArrayList&lt;&gt;(16);//....省略添加元素的代码//分组字段 driver_idlist.stream().collect(Collectors.groupingBy(SafeSystemVO::getDriver_id)).values().stream().map(d -&gt; &#123; SafeSystemVO vo = d.get(0); //求和1 vo.setAg_total(d.stream().map(s -&gt; BigDecimal.valueOf(s.getAg_total())).reduce(BigDecimal.ZERO, BigDecimal::add).doubleValue()); // 求和2 vo.setScore(d.stream().map(s -&gt; BigDecimal.valueOf(s.getScore())).reduce(BigDecimal.ZERO, BigDecimal::add).doubleValue()); vo.setLkj_score(d.stream().map(s -&gt; BigDecimal.valueOf(s.getLkj_score())).reduce(BigDecimal.ZERO, BigDecimal::add).doubleValue()); vo.setTotalScore(d.stream().map(s -&gt; BigDecimal.valueOf(s.getTotalScore())).reduce(BigDecimal.ZERO, BigDecimal::add).doubleValue()); return vo;&#125;).collect(Collectors.toList()); 2. list 根据某个字段分组后求单个字段的平均值，并按照分组字段排序 1234567891011121314151617181920212223242526List&lt;SafeSystemVO&gt; list = new ArrayList&lt;&gt;(16);//....省略添加元素的代码Map&lt;String, Double&gt; monthAvg = list1.stream().collect(Collectors.groupingBy(SafeSystemVO::getMonth, Collectors.averagingDouble(SafeSystemVO::getTotalScore)));// 根据Map 对象的key排序// 我的分组字段是日期，就用了下面的monthAvg.entrySet().stream().sorted((o1, o2) -&gt; &#123; try &#123; Date d1 = DateUtils.convertStringToDate(o1.getKey(), DateUtils.FM2); Date d2 = DateUtils.convertStringToDate(o2.getKey(), DateUtils.FM2); assert d1 != null; return d1.compareTo(d2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return 0;&#125;).collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue, (oldVal, newVal) -&gt; oldVal, LinkedHashMap::new));//根据Map 对象的value排序//monthResult = workShopAvg.entrySet().stream().sorted((p1, p2) -&gt; p2.getValue().compareTo(p1.getValue())).collect(Collectors.toList());//方法一：//list1.sort((o1, o2) -&gt; o1.getAge().compareTo(o2.getTotalScore())); //正序//list1.sort((o1, o2) -&gt; o2.getAge().compareTo(o1.getTotalScore())); //倒序//方法二//list1.sort(Comparator.comparing(Person::getTotalScore)); // 正序//list1.sort(Comparator.comparing(Person::getTotalScore).reversed()); // 倒序 3. list 根据字段分组求和后取 前/后10名 12345678//list对象接上面的//根据driver_id分组，求平均值Map&lt;String, Double&gt; driverScores = list3.stream() .collect(Collectors.groupingBy(SafeSystemVO::getDriver_id, Collectors.averagingDouble(SafeSystemVO::getTotalScore)));//排序后获取后10 名， 前10名的话修改sorted逻辑为：sorted((p1, p2) -&gt; p2.getValue().compareTo(p1.getValue()))List&lt;Map.Entry&lt;String, Double&gt;&gt; driverScoresTop10 = driverScores.entrySet().stream().sorted((p1, p2) -&gt; p1.getValue().compareTo(p2.getValue())).limit(10).collect(Collectors.toList()); 4. 其他 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public static void main(String[] args) &#123; //Id, name , age Person p1 = new Person(1,&quot;麻子&quot;, 31); Person p2 = new Person(2,&quot;李四&quot;, 20); Person p3 = new Person(3,&quot;王五&quot;, 26); Person p4 = new Person(3,&quot;王五&quot;, 26); List&lt;Person&gt; personList = new ArrayList&lt;Person&gt;(); personList.add(p1); personList.add(p2); personList.add(p3); personList.add(p4); //java8遍历 personList.forEach(p -&gt; System.out.println(p.getAge())); //按照person的 age进行排序 //方法一 personList.sort((o1, o2) -&gt; o1.getAge().compareTo(o2.getAge())); //正序 personList.sort((o1, o2) -&gt; o2.getAge().compareTo(o1.getAge())); //倒序 //方法二 personList.sort(Comparator.comparing(Person::getAge)); // 正序 personList.sort(Comparator.comparing(Person::getAge).reversed()); // 倒序 //多个字段排序 personList.sort(Comparator.comparing(User::getId).thenComparing(Person::getAge)); //注：若选择排序字段为null值，正序可personList.sort(Comparator.comparing(Person::getAge,Comparator.nullsFirst(Comparator.naturalOrder()))) System.out.println(&quot;========================================&quot;); //获取年龄最大的Person Person maxAgePerson = personList.stream().max(Comparator.comparing(Person::getAge)).get(); System.out.println(maxAgePerson.getAge()); System.out.println(&quot;========================================&quot;); //获取年龄最小的Person Person minAgePerson = personList.stream().min(Comparator.comparing(Person::getAge)).get(); System.out.println(minAgePerson.getAge()); //过滤出年龄是20的person，想过滤出什么条件的均可以 List&lt;Person&gt; personList1 = personList.stream().filter(person -&gt; person.getAge() == 20).collect(Collectors.toList()); //过滤-- 统计出年龄等于20的个数 long count = personList.stream().filter(person -&gt; person.getAge() == 20).count(); //过滤出年龄大约20的人 List&lt;Person&gt; personList2 = personList.stream().filter(t -&gt; t.getAge().equals(20)).collect(Collectors.toList()); //得到年龄的平均值 double asDouble = personList.stream().mapToInt(person -&gt; person.getAge()).average().getAsDouble(); //得到年龄的求和--基本类型 int sum = personList.stream().mapToInt(person -&gt; person.getAge()).sum(); //得到年龄的求和--包装类型,其中，若bigDecimal对象为null，可filter()过滤掉空指针. BigDecimal totalAge = personList.stream().map(User::getAge).reduce(BigDecimal.ZERO, BigDecimal::add); （其中，若bigDecimal对象为null，可filter()过滤掉空指针.） //去重 List&lt;Person&gt; personList3 = personList.stream().distinct().collect(Collectors.toList()); //list转map. //（其中，若集合对象key有重，可根据(k1,k2)-&gt;k1设置&lt;保留k1，舍弃k2&gt;.） Map&lt;Long, Person&gt; personMap = personList.stream().collect(Collectors.toMap(User::getId, t -&gt; t,(k1,k2)-&gt;k1)); &#125;&#125;","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"分布式消息中间件设计","slug":"message-oriented-middleware","date":"2022-03-22T08:56:31.000Z","updated":"2022-03-22T08:56:31.000Z","comments":true,"path":"message-oriented-middleware/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/message-oriented-middleware/index.html","excerpt":"","text":"1. 消息中间件概述 什么是分布式消息中间件 利用高效可靠的消息传递机制进行平台无关的数据交流； 并基于数据通信来进行分布式系统的集成； 通过提供消息和消息排队模型，它可以在分布式环境下扩展进城间的通信。 消息中间件的应用场景 跨系统数据传递、高并发流量削峰、数据异步处理… 常用的消息中间件 ActiveMQ(太老)、RabbitMQ、Kafka、 RocketMQ 本质 一种具备接受请求、保存数据、发送数据等功能的网络应用。和一般的网络应用程序的区别是它主要负责数据的接收和传递，所以性能一般高于普通程序 5大核心组成 协议 持久性机制 消息分布机制 高可用设计 高可靠设计 1.1 协议 协议是计算机之间通信时共同遵守的一组约定，确保计算机之间能够相互交流；是对数据格式和计算机之间交换数据时必须遵守的规则的正式描述 三大要素： 语法：即数据和控制信息的结构或格式 语义：即需要发出何种控制信息，完成何种动作以及作出何种响应 时序：即时间实现顺序的详细说明 消息中间件常用协议：openWire、AMQP、MQTT（物流网，快，不能持久化）、Kafka、OpenMessage等； 不能用http协议的原因是：http每次请求必须要有响应，性能不高； 1.1 AMQP协议 AMQP（Advanced Message Queuing Protocol）是高级消息队列协议；04年JPMorgan Chase(摩根大通集团)联合其他公司共同设计 特性：事物支持、持久化支持，出生金融行业，在可靠性消息处理上具备天然的优势 优秀产品 RabbitMQ、 Apache ACTIVEMQ 1.2 MQTT协议 MQTT（Message Queuing Telemetry Transport）消息队列遥测传输 是IBM开发的一个即时通讯协议，物联网系统架构中的重要组成部分； 特性：轻量、结构简单、传输快、没有事务支持、没有持久化相关设计 应用场景：适用于计算能力有限、低宽带、网络不稳定的场景 优秀产品：RabbitMQ、 Apache ACTIVEMQ 1.3 Open Message协议 OpenMessaging 是近几年有阿里发起，与雅虎、滴滴出行、Streamlio等公司共同参数传里的分布式消息中间件、流处理领域的开发应用标准。 是国内首个在全球范围内发起的分布式消息领域国际标准 特性：结构简单、解析快、有事务设计、有持久化设计 优秀产品：Apache RocketMQ 1.4 Kafka协议 Kafka协议是基于TCP的二进制协议。消息内部是通过长度来分割，由一些基本数据类型组成 特性：结构简单、解析快、无事务设计、有持久化设计 优秀产品：Apache Kafka 1.55 OpenWire协议 开放链接，ActiveMQ自定义的一种协议，ActiveMQ默认链接方式，它提供一种高效率的二进制格式来使消息高速传输 特性：结构简单、解析快、无事务设计、有持久化设计 优秀产品：Apache ActiveMQ 1.2 持久化 简单来说就是将数据存入磁盘，而不是存在内存中岁服务重启而消失，使数据能够永久保存叫做持久化 ActiveMQ RabbitMQ Kafka RocketMQ 文件系统 支持 支持 支持 支持 数据库 支持 - - - 1.3 消息分发 ActiveMQ RabbitMQ Kafka RocketMQ 发布订阅 支持 支持 支持 支持 轮询分发 支持 支持 支持 - 公平分发 - 支持 支持 - 重发 支持 支持 - 支持 消息拉取 - 支持 支持 支持 1.4 高可用 高可用性是指产品在规定的条件和规定的时刻或时间区间内处于可执行规定功能状态的能力； 当业务量大时，一台消息中间件服务器可能无法满足需求，所以需要消息中间件能够集群部署，来达到高可用的目的。 1.4.1 Master-Slave主从共享数据的部署方式 当Master收到客户端的消息后，放到共享的文件系统/数据库； 客户端访问的是Master节点，Slave节点只做备份； 1.4.2 Master-Slave主从同步部署方式 当Master收到客户端的消息后，发给其他broker同步。 1.4.3 Broker-Cluster多主集群同步部署方式 一部分消息放在broker1 ,一部分放在broker2 1.4.4 Broker-Cluster多主集群转发部署方式 转发数据或转发请求 1.4.5 Master-slave与Broker-Cluster结合 1.5 高可靠 高可靠性是指系统可以无故障地持续运行。比如一个系统从来不崩溃、报错，或者崩溃、报错的几率较低，那就是高可靠。 保证消息中间件的高可靠行，可以从一下几方面考虑 消息传输可靠： 通过协议来保证系统件数据解析的正确性 消息存储可靠： 通过持久化来保证消息存储可靠性","categories":[{"name":"05 分布式","slug":"05-分布式","permalink":"https://xiaoyuge5201.github.io/categories/05-%E5%88%86%E5%B8%83%E5%BC%8F/"}],"tags":[{"name":"消息中间件","slug":"消息中间件","permalink":"https://xiaoyuge5201.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"}]},{"title":"MinIO简介以及Linux安装MinIO","slug":"linux-minio","date":"2022-03-21T05:39:15.000Z","updated":"2022-03-21T05:39:15.000Z","comments":true,"path":"linux-minio/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/linux-minio/index.html","excerpt":"","text":"1. 什么是对象存储 对象存储服务OSS（Object Storage Service）是一种海量、安全、低成本、高可用的云存储服务，适合存放任意类型的文件。容量和处理能力弹性扩展，多种存储类型提供选择，全面优化存储成本。 最大的优势：可以存储大量的非结构话数据，例如：图片、视频、日志文件、备份数据和容器/虚拟机镜像等。 2. MinIO MinIO 是个基于Golang编写的开源对象存储套件，基于Apache License V2.0开源协议，虽然轻量，却拥有不错的性能，兼容亚马逊S3云存储服务接口。可以很简单的和其他应用结合使用，例如：NodeJS、Redis、mysql等 中文文档： http://docs.minio.org.cn/docs/master/minio-monitoring-guide 2.1 MinIO应用场景 可以作为私有云的对象存储服务来使用，也可以作为云对象存储的网关层，无缝对接Amazon S3 或者 MicroSoft Azure 。 2.2 MinIO特点 高性能 作为一款高性能存储，在标准硬件条件下，其读写速率分别可以达到55Gb/s和 35Gb/s。并且MinIO支持一个对象文件是任意大小（几KB到最大5T不等） 可扩展 不同MinIO集群可以组成联邦，并形成一个全局的命名空间，并且支持跨越多个数据中心 云原生 容器化、基于K8S的编排、多租户支持 Amazon S3兼容 使用Amazon S3 V2/V4 API。可以使用Minio SDK，Minio Client，AWS SDK 和 AWS CLI 访问Minio服务器。 可对接多种后端存储 除了Minio自己的文件系统，还支持 DAS、 JBODs、NAS、Google云存储和 Azure Blob存储。 SDK支持 GO SDK： https://github.com/minio/minio-go JavaSDK： https://github.com/minio/minio-java PythonSDK： https://github.com/minio/minio-py Lambda计算 Minio服务器通过其兼容AWS SNS / SQS的事件通知服务触发Lambda功能。支持的目标是消息队列，如Kafka，NATS，AMQP，MQTT，Webhooks以及Elasticsearch，Redis，Postgres和MySQL等数据库 图形化界面 有操作页面 功能简单 不容易出错，快速启动 支持纠删码 MinIO使用纠删码、Checksum来防止硬件错误和静默数据污染。在最高冗余度配置下，即使丢失1/2的磁盘也能恢复数据 2.3 存储机制 MinIO 使用纠删码erasure code、校验和checksum。 即使丢一半数据（N/2）的硬盘，仍然可以恢复数据。 校验和checksum 保护数据免受硬件故障和无声数据损坏 纠删码erasure code 纠删码是一种恢复丢失和损坏数据的数据算法，目前纠删码技术在分布式存储系统中的应用主要有三类：阵列纠删码（Array Code : RAID5、RAID6等）、RS（Reed-Solomon）里德-所罗门类纠删码和LDPC（LowDensity Parity Check Code） 低密度奇偶校验纠删码。 Erasure code 是一种编码技术，他可以将N份原始数据，增加m份数据，并通过n+m 份中的任意n份数据，还原为原始数据。即如果有任意小于等于m份的数据失效，仍然能通过剩下的数据还原出来。 MinIO 采用Reed-Solomon code将对象拆分成N/2数据和N/2奇偶校验快，这就意味着如果是12块盘，一个对象会分成6个数据块、6个奇偶校验块；可以丢失任意6块盘（不管是存放的数据块还是奇偶校验块），仍可以通过剩下的盘进行数据恢复 3. 安装和使用MinIO 3.1 Linux安装MinIO 下载（https://min.io/download#/linux） 1wget https://dl.min.io/server/minio/release/linux-amd64/minio 运行 12345678910 chmod +x minio ./minio server /usr/software/minio/data #将/usr/software/minio/data 替换为您希望 MinIO 存储数据的驱动器或目录的路径。#或者指定账号密码启动MINIO_ACCESS_KEY=minioadmin MINIO_SECRET_KEY=minioadmin ./minio server --config-dir /usr/software/minio/config /usr/software/minio/data #后台启动nohup ./minio server /usr/software/minio/data &gt; /usr/software/minio/minio.log 2&gt;&amp;1 &amp;##或者指定账号密码启动MINIO_ACCESS_KEY=minioadmin MINIO_SECRET_KEY=minioadmin nohup ./minio server --config-dir /usr/software/minio/config /usr/software/minio/data&gt; /usr/software/minio/minio.log 2&gt;&amp;1 &amp;# 设置启动脚本（建议） 12345mkdir miniodata touch minio.sh vi minio.sh 在sh文件中添加以下内容 123456#指定登录用户名export MINIO_ACCESS_KEY=username#指定登录密码export MINIO_SECRET_KEY=password#指定端口以及存储文件夹，并启动服务 9000访问端口， 9001 控制台界面访问端口, 这里0.0.0.0可以设置为具体的服务器IPnohup ./minio server --address &#x27;0.0.0.0:9000&#x27; --console-address &#x27;0.0.0.0:9001&#x27; ./miniodata &gt; ./miniodata/minio.log 2&gt;&amp;1&amp; 给当前用户加上执行权限 1234chmod u+x *.sh #启动sh minio.sh 3.2 安装客户端(可选) 安装 123wget https://dl.min.io/client/mc/release/linux-amd64/mcchmod +x mc./mc --help 使用命令给客户端添加一个服务端 1./mc alias set minio http://172.21.0.7:9000 minioadmin minioadmin 创建bucket，并查询所有bucket 123456[root@ww xiaoyuge]# ./mc ls minio[root@ww xiaoyuge]# ./mc mb minio/mybucketBucket created successfully `minio/mybucket`[root@ww xiaoyuge]# ./mc ls minio[2020-09-02 03:02:36 CST] 0B mybucket/[root@ww xiaoyuge]# 页面查询bucket 创建用户 1./mc admin user add minio root rootroot 给用户赋予权限 1./mc admin policy set minio readwrite user=root 3.3 使用MinIO 启动 在浏览器输入： http://localhost:9000 在输入控制打印的默认的AccessKey和SecretKey： AccessKey: minioadmin SecretKey（默认）: minioadmin 使用AccessKey 和 SecretKey 登录后台。 进入系统后，我们先要点击右上角的“+”按钮，创建一个文件桶（输入名称后，回车即可），在上传文件到这个文件桶中。Create bucket（创建文件桶），然后输入bucket名称为 test, 创建成功后再Upload file（上传文件）。 现在我们去服务器，我们启动时指定的目录去看看，可以看到一个新建的test文件目录（文件桶相当于文件目录），这里没有使用纠删码的模式，所以直接就是源文件了。当我们线上运行的项目已经有源文件了，在使用minio的时候，可以直接指定该目录为minio的文件目录就行了。 分享文件，也可以设置文件分享有效日期 访问连接会出现如下界面： 3.4 mioIO常见启动问题 启动报错“WARNING: Console endpoint is listening on a dynamic port…” 错误提示很明显，需要to choose a static port。 写了一个shell启动MinIO，在shell中使用–console-address ‘部署minio的ip:希望通过什么端口打开minio console页面’ 1234 export MINIO_ACCESS_KEY=username#指定登录密码 export MINIO_SECRET_KEY=password nohup ./minio server --address &#x27;0.0.0.0:9000&#x27; --console-address &#x27;0.0.0.0:9001&#x27; ./miniodata &gt; ./miniodata/minio.log 2&gt;&amp;1&amp; 4. SpringBoot 集成minIO 项目源代码地址：https://gitee.com/xiaoyuge520/minio-demo， 下面是应用主要功能截图展示： 在minIO的控制台界面选择对应的bucket可以查看到刚提交的内容 同理，删除也是一样！！！","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://xiaoyuge5201.github.io/tags/Linux/"}]},{"title":"Linux系统下查找文件命令总结","slug":"linux-search-file","date":"2022-03-19T05:52:39.000Z","updated":"2022-03-19T05:52:39.000Z","comments":true,"path":"linux-search-file/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/linux-search-file/index.html","excerpt":"","text":"1. which查命令绝对路径 which 从环境变量PATH中定位/返回与指定名字相匹配的可执行文件所在的路径 原理：执行which命令时，which会在当前环境变量PATH中依次寻找能够匹配所找命令名字的可执行文件名，不加 - a选项，返回第一个匹配的可执行文件路径， 否则依次返回满足条件的所有可执行文件的路径名 适用场合： 一般用于查找命令/可执行文件所在的路径。有时候可能在多个路径下存在相同的命令，该命令可用于查找当前所执行的命令到底是哪一个位置处的命令。 2. whereis查找特定文件 whereis 命令用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径， 该命令只能用于程序名的搜索 - b #定位可执行文件 - m #定位帮助文件 - s 定位源代码文件 - u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件 - B 指定搜索可执行文件的路径。 - M 指定搜索帮助文件的路径。 - S 指定搜索源代码文件的路径 原理： whereis命令首先会去掉filename中的前缀空格和以.开头的任何字符，然后再在数据库（var/lib/slocate/slocate.db）中查找与上述处理后的filename相匹配的二进制文件、源文件和帮助手册文件,使用之前可以使用updatedb命令手动更新数据库。 适用场合： 二进制文件、源文件和帮助手册文件路径的查找。 和find 相比，Whereis 查找的速度非常快，这是因为Linux系统会将系统内的所有文件都记录在一个数据库文件中，当使用whereis (或者locate) 会从数据库查找数据，而不是像find命令那样，通过遍历硬盘来查找文件，效率更高！ 3. locate缓存查找文件 locate 搜素一个数据库（/var/lib/mlocate/mlocate.db）,这个数据库中国呢包含本地所有文件信息，Linux系统自动创建这个数据库，并且每天更新依次，所以使用locate命令查不到最新变动过的文件，为了避免这种情况，可以在使用locate之前，先使用updatedb命令，手动更新数据库，updatedb命令会根据/etc/updatedb.conf来更新文件。 原理： 默认情况下(当filename中不包含通配符*)，locate会给出所有与 * filename*相匹配的文件的路径。 适用场合： 没有文件类型性质的模糊查找（你只记得某个文件的部分名称）。 4. find 遍历文件查找 语法： -name #按文件名查找(find /etc/ -name “???” 查找/etc目录下，开头是6个任意字符的文件， *.log以log结尾的文件；&quot;[1-3].txt&quot;指定范围以txt结尾的文件（包括 1.txt,2.txt,3.txt）) -size #按大小查找（find /etc/ -size +1M 查询大于1M的文件(find /etc/ -size -10K)，注意：如果没有+ -， 则是精确到1M,加上+ - 表示范围； find /etc/ -size +1k -a -size -10k 查找1-10K的文件） -user #按属主查找（find /opt/ -user xiaoyuge 查找/opt属于xiaoyuge用户的文件；注意，系统要存在该用户，否则会报错） -perm #按权限查找（find /opt/ -perm 0644 查找/opt目录权限是644文件） -type #按类型查找（find /usr/bin/ -type f 查找/usr/bin下类型是二进制文件） -time #按天查找 atime n #将n*24小时内访问过的文件列出(access) ctime n #将n*24小时内状态发生改变的文件列出（change） find /etc/ -ctime +7 在7天之前,属性被修改过的文件 mtime n #将n*24小时内被修改过的文件列出(modify) newer file #把比file还要心的文件列出 amin n #将n 分钟内访问过的文件列出(access) find /etc/ -mmin -120 在120分钟内，内容被修改的文件 cmin n #将n 分钟内状态发生改变的文件列出（change） mmin n #将n 分钟内被修改过的文件列出(modify) -inum #按i节点查找 有一些文件的硬链接数量很多，有相同的i节点，查找其中一个文件的i节点号，一次性删除。 -exec #查找后执行命令 原理： 遍历当前工作目录及其子目录，find命令是在硬盘上遍历查找，非常耗硬盘资源，查找效率相比whereis和locate较低。 适用场合： 能用which、whereis和locate的时候尽量不要用find. 5. 4种命令对比","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://xiaoyuge5201.github.io/tags/Linux/"}]},{"title":"docker安装nginx","slug":"docker-nginx","date":"2021-12-09T13:47:19.000Z","updated":"2021-12-09T13:47:19.000Z","comments":true,"path":"docker-nginx/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/docker-nginx/index.html","excerpt":"","text":"1.查看所有的镜像 1docker search nginx 2.拉取最新版本的镜像 1234docker pull nginx#或者指定最新版本docker pull nginx:latest 3.使用命令查看本地镜像，确定nginx镜像已下载到本地 1docker images 4. 创建挂载目录 1mkdir -p /data/nginx/&#123;conf,conf.d,html,logs&#125; 5. 创建配置文件 1touch nginx.cnf 6. Nginx详情配置请参考：https://xiaoyuge.work/2021/12/05/nginx-02/ 7. 查看容器 1234docker ps -a# docker stop xxx 停止某个容器运行# docker rm xxx 删除容器 8.启动容器，挂载配置文件 1docker run --name mynginx -d -p 80:80 -v /data/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v /data/nginx/logs:/var/log/nginx -d docker.io/nginx 可以通过命令docker exec -it nginx-test bash进入容器内容修改配置 9.安装完毕，访问地址 http://localhost:8080，出现如下内容，安装成功！！！ 10.域名解析配置 我买的是阿里云的服务器以及域名，上面操作后，忘记在阿里云控制台中去配置 11.配置多个二级域名 在第8步的时候将docker容器中的nginx配置映射到了目录/data/nginx/conf下面； 修改nginx.conf 123456789101112131415161718192021222324user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; #引入conf.d下面所有的配置文件 include /etc/nginx/conf.d/*.conf;&#125; 然后在conf.d目录下面创建了两个子域名反向代理配置文件,其他的域名代理相同，只要改server_name和proxy_pass代理端口，配置文件需以.conf结尾 note.xiaoyuge520.vip.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243######## Nginx的main(全局配置)文件#指定nginx运行的用户及用户组,默认为nobody#user nobody;#开启的线程数，一般跟逻辑CPU核数一致worker_processes 1;events &#123;#设置工作模式为epoll,除此之外还有select,poll,kqueue,rtsig和/dev/poll模式#use epoll; #定义每个进程的最大连接数,受系统进程的最大打开文件数量限制。 worker_connections 1024;&#125;#######Nginx的Http服务器配置,Gzip配置http &#123; include mime.types; #核心模块指令，智力默认设置为二进制流，也就是当文件类型未定义时使用这种方式 default_type application/octet-stream; #开启高效文件传输模式 sendfile on; keepalive_timeout 65; ########Nginx的server虚拟主机配置 server &#123; #监听端口为 80 listen 80; #设置主机域名 server_name note.xiaoyuge520.vip; #设置访问的语言编码 #charset koi8-r; #设置虚拟主机访问日志的存放路径及日志的格式为main #access_log logs/host.access.log main; #设置虚拟主机的基本信息 location / &#123; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://47.101.130.163:8086/note; # 代理ip:端口 &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; record.xiaoyuge520.vip.conf 1234567891011121314151617181920212223242526worker_processes 1;events &#123; worker_connections 1024;&#125;#######Nginx的Http服务器配置,Gzip配置http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name record.xiaoyuge520.vip; #access_log logs/host.access.log main; location / &#123; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://47.101.130.163:8888/record; # 代理ip:端口 &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 如果挂载之后容器运行正常却依然不能通过域名访问可尝试以下操作 12345678910#查看所有容器,获取nginx的container 名称docker ps -a#向名称为nginx的容器发送脚本命令, mynginx 为容器名称docker exec -it mynginx /bin/bash#重新加载配置命令cd /etc/nginx/conf.dservice nginx reload#检查配置文件路径是否正确 每一次更改配置文件都需要重启容器 12345678# 重启nginx容器docker restart nginx #查看容器状态docker ps #如果挂载失败，查看nginx容器log,显示错误信息，根据错误信息 更改配置文件等docker logs -t nginx 以上配置完成之后能够通过域名访问网站，但是css样式却被nginx解析成text/plain，打开控制台可看到warn信息 解决nginx将css文件解析为text/plain 方法一： ngin.conf中http添加： 12include /etc/nginx/mime.types;default_type application/octet-stream; 注：此办法并不能使我网站的css正确解析，因为在拷贝nginx镜像中的原配置文件时，就已经添加mime.types了。却依然不能正确解析。 方法二：解析成功，原因未知 1将index.html中&lt;!DOCTYPE html&gt;去掉。 通过域名访问：成功！！","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xiaoyuge5201.github.io/tags/Docker/"}]},{"title":"docker安装mysql","slug":"docker-mysql","date":"2021-12-09T13:32:36.000Z","updated":"2021-12-09T13:32:36.000Z","comments":true,"path":"docker-mysql/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/docker-mysql/index.html","excerpt":"","text":"1.查看所有的镜像 1docker search mysql 2.拉取最新版本的镜像 1234docker pull mysql#或者指定版本docker pull mysql:8.0.16 3.创建数据目录和配置文件 123mkdir -p /usr/mysql/conf /usr/mysql/datachmod -R 777 /usr/mysql/ 4.创建配置文件 在上面创建的配置文件目录/usr/mysql/conf下创建MySQL的配置文件my.cnf 123touch my.cnf;vim /usr/mysql/conf/my.cnf; 添加以下内容到上述创建的配置文件中 123456789101112131415161718[client]#socket = /usr/mysql/mysqld.sockdefault-character-set = utf8mb4[mysqld]#pid-file = /var/run/mysqld/mysqld.pid#socket = /var/run/mysqld/mysqld.sock#datadir = /var/lib/mysql#socket = /usr/mysql/mysqld.sock#pid-file = /usr/mysql/mysqld.piddatadir = /usr/mysql/datacharacter_set_server = utf8mb4collation_server = utf8mb4_binsecure-file-priv= NULL# Disabling symbolic-links is recommended to prevent assorted security riskssymbolic-links=0# Custom config should go here!includedir /etc/mysql/conf.d/ 5.启动创建容器 1docker run --restart=unless-stopped -d --name mysql -v /usr/mysql/conf/my.cnf:/etc/mysql/my.cnf -v /usr/mysql/data:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=xiaoyuge mysql 参数解释： 123456–name mysql5.7 容器别名-p 3307:3306 映射容器端口号和宿主机端口号（本机3307端口映射容器3306端口）-v /data/mysql/datadir:/var/lib/mysql 目录映射（挂载宿主机目录和 docker容器中的目录，前面是宿主机目录，后面是容器内部目录）-v /data/mysql/conf.d:/etc/mysql/conf.d 目录映射（mysql配置目录）-d 后台运行-e 环境参数，MYSQL_ROOT_PASSWORD设置root用户的密码 执行上述命令后，执行查询容器的命令就可以看到创建的mysql容器 1docker ps -a 常见问题 1.远程无法链接 上述虽然安装好了mysql，但是使用远程的Navicat连接时提示错误，不能正确连接mysql，此时需要修改按照下面说的步骤修改一下mysql的密码模式以及主机等内容才可以。 修改mysql密码以及可访问主机 进入容器内部 1docker exec -it mysql /bin/bash 连接mysql 1mysql -uroot -p 使用mysql库 1use mysql; 修改访问主机以及密码等，设置为所有主机可访问 123ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;新密码&#x27;;#注意： mysql_native_password，mysql8.x版本必须使用这种模式，否则navicate无法正确连接 刷新 123flush privileges;exit; 远程使用Navicat连接数据库 2.docker启动mysql容器报错 1docker run --restart=unless-stopped -d --name mysql -v /usr/mysql/conf/my.cnf:/etc/mysql/my.cnf -v /usr/mysql/data:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=xiaoyuge mysql 启动时提示：Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: Running hook #0:: error running hook: signal: segmentation fault (core dumped), stdout: , stderr:: unknown 解决方式： 1sudo yum upgrade 或者可以试下 12345rm -rf /usr/mysql/conf/my.cnftouch /usr/mysql/conf/my.cnfvim my.cnf#然后重新配置一下","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://xiaoyuge5201.github.io/tags/Docker/"}]},{"title":"docker安装","slug":"docker-01","date":"2021-12-09T13:20:57.000Z","updated":"2021-12-09T13:20:57.000Z","comments":true,"path":"docker-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/docker-01/index.html","excerpt":"","text":"新的服务器没有安装docker，使用docker命令时提示：docker: command not found错误信息 1. 更新yum包 1yum update 2.安装依赖软件包 1yum install -y yum-utils device-mapper-persistent-data lvm2 3.设置yum源 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 4.安装docker 12#默认安装最新的docker稳定版本。yum install docker-ce 5.启动docker服务 1systemctl start docker 6.设置开机自启动 1systemctl enable docker 7. 查看docker版本信息 1docker version 至此，解决。 8. 停止所有容器 1docker stop $(docker ps -a | awk &#x27;&#123; print $1&#125;&#x27; | tail -n +2)","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://xiaoyuge5201.github.io/tags/docker/"}]},{"title":"Nginx基础篇（四）Nginx实现反向代理","slug":"nginx-04","date":"2021-12-05T08:52:42.000Z","updated":"2021-12-05T08:52:42.000Z","comments":true,"path":"nginx-04/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-04/index.html","excerpt":"","text":"1. 正向代理和反向代理 正向代理： 正向代理类似一个跳板机，代理访问外部资源 比如我们国内访问谷歌，直接访问访问不到，我们可以通过一个正向代理服务器，请求发到代理服，代理服务器能够访问谷歌，这样由代理去谷歌取到返回数据，再返回给我们，这样我们就能访问谷歌了 正向代理的用途： （1）访问原来无法访问的资源，如google （2） 可以做缓存，加速访问资源 （3）对客户端访问授权，上网进行认证 （4）代理可以记录用户访问记录（上网行为管理），对外隐藏用户信息 正向代理即是客户端代理, 代理客户端, 服务端不知道实际发起请求的客户端 反向代理： 反向代理（Reverse Proxy）实际运行方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器 反向代理的作用： （1）保证内网的安全，阻止web攻击，大型网站，通常将反向代理作为公网访问地址，Web服务器是内网 （2）负载均衡，通过反向代理服务器来优化网站的负载 反向代理即是服务端代理, 代理服务端, 客户端不知道实际提供服务的服务端 2. Nginx配置反向代理 在http-&gt; server块中配置server_name 1234567891011121314151617server &#123; #监听端口为 80 listen 80; #设置主机域名 server_name http://192.168.44.99; #设置虚拟主机的基本信息 location / &#123; proxy_pass http://192.168.44.1:9096; ###最重要的配置，转发到目标地址， 也可以配置服务器组，然后upstream一个服务器组 proxy_method POST; #设置转发请求的格式 #Nginx在header里面增加一个自定义字段 Host， 用于存放当前客户端IP地址 proxy_set_header Host $host; #获取客户端的真实IP地址设置到header中的字段名为X-Real-IP里面 proxy_set_header X-Real-IP $remote_addr; #获取所有转发请求的IP信息列表 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; &#125;&#125; 那么访问http://192.168.44.99 ， nginx会将请求转发给目标服务器http://192.168.44.1:9096 2.1 location 匹配规则 匹配规则从上到下，匹配规则越宽松； 模式 含义 location=/uri = 表示精确匹配，只有完全匹配才能生效 location ^~ /uri ^~ 开头对URL路径进行前缀匹配，并且在正则之前 location ~ pattern 开头表示区分大小写的正则匹配 location ~* pattern 开头表示不区分大小写的正则匹配 location /uri 不带任何修饰符，也表示前缀匹配，但是在正则匹配之后 location / 通用匹配，任何未匹配到其他location的请求都会匹配到，相当于switch中的default","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"Nginx基础篇（三）实现虚拟主机","slug":"nginx-03","date":"2021-12-05T08:31:25.000Z","updated":"2021-12-05T08:31:25.000Z","comments":true,"path":"nginx-03/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-03/index.html","excerpt":"","text":"1. 虚拟主机Virtual Host 一种在单一主机或主机群上，实现多网域服务的方法，可以运行多个网站或服务的技术，虚拟主机之间完全独立，并可由用户自行管理虚拟并非指不存在，而是指空间是由实体的服务器延伸而来，其硬件系统可以是基于服务器群，或者单个服务器 使用域名访问虚拟主机，虚拟主机会给一个文件路径，然后部署自己的内容；访问域名时就会访问改文件夹下的某 个资源 2. 使用Nginx配置虚拟主机 在nginx下建立一个ygb的文件夹，里面新建一个index.html 在nginx.conf配置下http -&gt; server块内配置 12345678910111213141516171819202122server &#123; #监听端口为 80 listen 80; #设置主机域名 server_name www.xiaoyuge.work; #设置访问的语言编码 #charset koi8-r; #设置虚拟主机访问日志的存放路径及日志的格式为main #access_log logs/host.access.log main; # 这个是域名反问的虚拟主机的文件路径 root /usr/local/nginx/data/ygb #设置虚拟主机的基本信息 location / &#123; #设置虚拟主机默认访问的网页 index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125;&#125; 启动,然后在浏览器访问域名www.xiaoyuge.work 1./nginx -c ./nginx.conf","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"Nginx基础篇（二）安装","slug":"nginx-02","date":"2021-12-05T08:10:44.000Z","updated":"2021-12-05T08:10:44.000Z","comments":true,"path":"nginx-02/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-02/index.html","excerpt":"","text":"1.Nginx安装 安装nginx前首先要确认系统中是否安装了gcc 、pcre-devel、zlib-devel、openssl-devel 1234#1、rpm包安装的，可以用 rpm -qa 看到，如果要查找某软件包是否安装，用 rpm -qa | grep &quot;软件或者包的名字&quot;#2、以deb包安装的，可以用 dpkg -l 看到。如果是查找指定软件包，用 dpkg -l | grep &quot;软件或者包的名字&quot;#3、yum方法安装的，可以用 yum list installed 查找，如果是查找指定包，用 yum list installed | grep &quot;软件名或者包名&quot;yum list installed | grep &quot;gcc&quot; 安装依赖包 1yum -y install gcc pcre-devel zlib-devel openssl openssl-devel 下载并解压安装包 1234567//创建nginx存放文件夹cd /usr/local#下载tar包wget http://nginx.org/download/nginx-1.23.2.tar.gztar -xvf nginx-1.23.2.tar.gzmv nginx-1.23.2 nginx# 配置 123456cd nginxmkdir logs./configure --prefix=/usr/local/nginxmakemake install 测试是否安装成功 12cd /usr/local/nginx./sbin/nginx -t 配置nginx.conf 12345678910111213141516171819202122232425262728293031vim /usr/local/nginx/cong/nginx.conf#修改如下server &#123; listen 80; server_name localhost; # 注意设定 root路径是有dist的， 如果需要配置多个静态资源，只允许有一个root ，其他的使用alias别名 location / &#123; root /usr/local/webapp/dist; index /index.html; &#125; #配置多个静态资源 其他的使用alias别名 location /log &#123; alias html/log; index index.html index.htm; autoindex on; &#125; location /ruoyi &#123; root /usr/local/nginx/html/dist; try_files $uri $uri/ /index.html; index index.html index.htm; &#125; #跨域 ip和port自行替换 location /adminApi &#123; proxy_pass http://ip:port; &#125;&#125; 启动 123#启动nginxcd /usr/local/nginx/sbin./nginx 常见问题 nginx启动提示：nginx: [emerg] bind() to 0.0.0.0:8080 failed (98: Address already in use) 修改端口 123456789101112131415#首先进入nginx/conf目录（根据自己的目录来写）# vi /usr/nginx/conf/nginx.conf#修改nginx.conf，将8080端口修改为其他端口号 server &#123; listen 8080; server_name localhost;#更换端口之后，然后重启nginx就可以了server &#123; listen 8888; server_name localhost; ...&#125; 取消占用端口号进程 123456#查看被占用的端口netstat -nlp|grep :8080tcp 0 0 0.0.0.0:8888 0.0.0.0:* LISTEN 24594/nginx: master#结束进程24594kill -9 24594#然后再重启nginx就可以了 常用命令 12345678910111213141516171819202122232425262728293031323334353637383940#修改配置后重新启动./nginx -s reload#如果出现：nginx: [error] open() ＂/usr/local/nginx/logs/nginx.pid＂ failed/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf#再次启动即可#查看nginx进程是否启动ps -ef|grep nginx#平滑启动nginxkill -HUP#主进程号或进程号文件路径 或者使用/usr/nginx/sbin/nginx -s reload#注意，修改了配置文件后最好先检查一下修改过的配置文件是否正 确，以免重启后Nginx出现错误影响服务器稳定运行。#判断Nginx配置是否正确命令如下：nginx -t -c /usr/nginx/conf/nginx.conf#或者使用/usr/nginx/sbin/nginx -t#重启nginx reload/usr/local/nginx/sbin/nginx -s reload service nginx restart#检查 nginx.conf 配置文件是否有错/usr/local/nginx/sbin/nginx -t#nginx启动命令:/usr/local/nginx/sbin/nginx#指定配置文件启动/usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf#关闭命令:/usr/local/nginx/sbin/nginx -s stop#重启命令:/usr/local/nginx/sbin/nginx -s reload#启动./nginx#关闭./nginx -s stop 启动后访问localhost 效果如下： 2.Nginx配置 12345678910111213141516171819202122232425...... 全局块events &#123;true//events 块&#125;###数据库配置stream &#123; server &#123; listen 3306; proxy_pass db; &#125; upstream db &#123; server 192.168.18.130:3305; server 192.168.18.129:3305; &#125; &#125;http&#123; ##http全局块true server+&#123;truetruelocation +[]true&#125;&#125; 2.1配置内容规则 官网配置教程：https://nginx.org/en/docs/dirindex.html 变量应用：https://nginx.org/en/docs/varindex.html 用#表示注释 每行配置的结尾需要加上分号 如果配置项值中包括语法符号，比如空格符，那么需要使用单引号或者双引号行括住配置项值，否则ngin x会报语法错误 单位简写： K或者k千字节（kilo byte, KB） M或者m兆字节（megabyte MB） ms(毫秒)，s(秒)， m(分)， h(小时) ， d (天)， w（周）， M（月，包含30天），y（年） 2.2 详细配置内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157######## Nginx的main(全局配置)文件#指定nginx运行的用户及用户组,默认为nobody#user nobody;#开启的线程数，一般跟逻辑CPU核数一致worker_processes 1;#定位全局错误日志文件，级别以notice显示，还有debug,info,warn,error,crit模式，debug输出最多，crir输出最少，根据实际环境而定#error_log logs/error.log;#error_log logs/error.log notice;#error_log logs/error.log info;#指定进程id的存储文件位置#pid logs/nginx.pid;#指定一个nginx进程打开的最多文件描述符数目，受系统进程的最大打开文件数量限制#worker_rlimit_nofile 65535events &#123; #设置工作模式为epoll,除此之外还有select,poll,kqueue,rtsig和/dev/poll模式 #use epoll; #定义每个进程的最大连接数,受系统进程的最大打开文件数量限制。 worker_connections 1024;&#125;###数据库的负载均衡stream &#123; upstream mysql_nginx &#123; hash $remote_addr consistent; server 192.168.18.128:3306 weight=5 max_fails=3 fail_timeout=30s; server 192.168.18.129:3306; server 192.168.18.130:3306; ##last_conn; #最小连接 &#125; server &#123; listen 3306; # 数据库服务器监听端口 proxy_connect_timeout 10s; proxy_timeout 300s; # 设置客户端和代理服务之间的超时时间，如果5分钟内没操作将自动断开。 proxy_pass mysql_nginx; &#125;&#125;#######Nginx的Http服务器配置,Gzip配置http &#123; #主模块指令，实现对配置文件所包含的文件的设定，可以减少主配置文件的复杂度，DNS主配置文件中的zonerfc1912,acl基本上都是用include语句。 include mime.types; #核心模块指令，智力默认设置为二进制流，也就是当文件类型未定义时使用这种方式 default_type application/octet-stream; #下面代码为日志格式的设定，main为日志格式的名称，可自行设置，后面引用 #log_format main &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27; # &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27; # &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;; #引用日志main， main是log-format的格式，在上面配置了；后面可以加上日志缓冲区大小，写满了就flush到磁盘中buffer = 1M; #access_log logs/access.log main; #设置允许客户端请求的最大的单个文件字节数 #client_max_body_size 20M; #指定来自客户端请求头的headebuffer大小 #client_header_buffer_size 32k; #指定连接请求试图写入缓存文件的目录路径 #client_body_temp_path /dev/shm/client_body_temp; #指定客户端请求中较大的消息头的缓存最大数量和大小，目前设置为4个32KB #large client_header_buffers 4 32k; #开启高效文件传输模式 sendfile on; #开启防止网络阻塞 #tcp_nopush on; #开启防止网络阻塞 #tcp_nodelay on; #设置客户端连接保存活动的超时时间 #keepalive_timeout 0; keepalive_timeout 65; #设置客户端请求读取超时时间 #client_header_timeout 10; #设置客户端请求主体读取超时时间 #client_body_timeout 10; #用于设置相应客户端的超时时间 #send_timeout ####HttpGZip模块配置 #httpGzip modules #开启gzip压缩 #gzip on; #设置允许压缩的页面最小字节数 #gzip_min_length 1k; #申请4个单位为16K的内存作为压缩结果流缓存 #gzip_buffers 4 16k; #设置识别http协议的版本，默认为1.1 #gzip_http_version 1.1; #指定gzip压缩比，1-9数字越小，压缩比越小，速度越快 #gzip_comp_level 2; #指定压缩的类型 #gzip_types text/plain application/x-javascript text/css application/xml; #让前端的缓存服务器进过gzip压缩的页面 #gzip_vary on; #########Nginx的server虚拟主机配置 server &#123; #监听端口为 80 listen 80; #设置主机域名 server_name localhost; #设置访问的语言编码 #charset koi8-r; #设置虚拟主机访问日志的存放路径及日志的格式为main #access_log logs/host.access.log main; #设置虚拟主机的基本信息 location / &#123; #设置虚拟主机的网站根目录 root html; #设置虚拟主机默认访问的网页 index index.html index.htm; &#125; #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ &#123; # proxy_pass http://127.0.0.1; #&#125; # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ &#123; # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #&#125; # deny access to .htaccess files, if Apache&#x27;s document root # concurs with nginx&#x27;s one # #location ~ /\\.ht &#123; # deny all; #&#125; &#125;&#125; 2.3 日志 在nginx同级目录下logs文件夹 access.log 正常日志 error.log 错误日期 需要在nginx.conf中的http模块配置access_log","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"Nginx基础篇（一）扫盲","slug":"nginx-md","date":"2021-12-05T06:56:36.000Z","updated":"2021-12-05T06:56:36.000Z","comments":true,"path":"nginx-md/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/nginx-md/index.html","excerpt":"","text":"1.Nginx发展史 作者：logo Sysoev 2000年地洞，c语言编写 2004年开源 2011年成立商业公司 2013 发布商业版本Nginx plus 2019.5月F5 Networks收购nginx 2019.12被Rambler集团起诉 2.Nginx与其他web服务器对比 Nginx与Apache HTTP server project区别 用来响应用户请求的web服务器 Nginx 和tomcat区别 Nginx是HTTP Server，主要是用于访问一些静态资源，可以用做代理服务器 Tomcat是Application Server应用服务器，用来存放和运行程序； HTTP Server 和Application Server区别与联系 3. HTTP知识 3.1 IP和端口 120.77.38.160:80 0为A类，10为B类，110为C类，1110为D类，1111为E类。D类地址供组播使用，E类地址保留。 端口是：0～65535 3.2 域名 协议、子域名、顶级域名、域名类型、资源路径、参数 12345678#https 协议默认端口443 可以省略https://www.baidu.com:443#user：子域名， com为域名类型（cn中国， us美国...）； 3623252831 为资源路径https://user.qzone.qq.com/3623252831#？号后面为参数https://baike.baidu.com/item/测试/232323?fr=asdfasdf 域名(domainName)和IP的关系以及域名的组成 域名：https://www.baidu.com:443/member/query?far=adsfad http/ https: 协议 baidu: 顶级域名 Com： 域名类型 www: 子域名，可以有多级：user.qzone.qq.com/232323 far=asdfa: 参数 member/query: 资源路径 DNS(domain name server)将域名转化为ip+port 3.3 HTTP协议的特点 简单快速 灵活 无连接（一次请求，连接关闭） 无状态（每次请求都和之前的请求无关） 3.4 HTTP协议的请求格式 12345Request URL: https://prtas.videocc.net/v2/view?pid=1638687363047X1327470&amp;vid=8c8d9388d0b4c16f41ef557fba23dede_8&amp;uid=8c8d9388d0&amp;flow=0&amp;ts=1638688553584&amp;href=aHR0cHM6Ly9rZS5ndXBhb2VkdS5jbi9wbGF5LzI4OD9waGFzZUlkPTU&amp;duration=1278&amp;cts=789&amp;sign=fcf19468eff088e983796d5826268f2d&amp;sd=1190&amp;pd=788&amp;pn=HTML5&amp;pv=v1.15.0&amp;sid=ZDIzZGM4ODUtNDM2My00MTQ3LWJmYTktY2M3MDgwM2U0NDc5&amp;param1=&amp;param2=MTc2MjEyODQ5OTg&amp;param3=&amp;cataid=1591268435818Request Method: GETStatus Code: 200 Remote Address: 221.231.81.238:443Referrer Policy: strict-origin-when-cross-origin 请求行 请求类型 Request Method GET: 请求指定的页面细腻，并返回尸体主题 HEAD: 类似于GET请求，只不过返回的相应中没有具体的内容，用于获取报头 POST：想指定资源提交数据进行处理请求，数据被高喊在请求体中 PUT: 从客户端想服务器传送的数据取代指定的文档的内容 DELETE: 请求服务器删除指定的页面 CONNECT: HTTP/1.1协议中预留给能够将连接方式改为管道方式的代理服务器 OPTIONS: 允许客户端查看服务器的性能 TRACE: 回显服务器收到的请求，主要用于测试后诊断 请求头 空行和请求数据 3.5 HTTP协议的返回格式 状态行、小洗头、空行和响应正文 1234567891011HTTP/1.1 200Server: nginx/1.20.1Date: Sun, 05 Dec 2021 07:24:45 GMTContent-Type: application/json;charset=UTF-8Transfer-Encoding: chunkedConnection: keep-aliveAccess-Control-Allow-Origin: https://ke.gupaoedu.cnAccess-Control-Allow-Credentials: trueAccess-Control-Allow-Methods: PUT,POST,GET,DELETE,OPTIONS,PATCHAccess-Control-Allow-Headers: DNT,web-token,app-token,Authorization,Accept,Origin,Keep-Alive,User-Agent,X-Mx-ReqToken,X-Data-Type,X-Auth-Token,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,token,showerrAccess-Control-Max-Age: 86400 Http格式响应状态码有哪些 1xx: informational(信息性状态码) 接受的请求正在处理 2xx: success(成功状态码) 请求正常处理完毕 3xx：redirection（重定向状态码）需要进行复检操作以完成请求 4xx：client error（客户端错误状态码） 服务器无法处理请求 5xx: server error（服务器错误错误状态码） 服务器处理请求出错 3.6 通用头字段Common Header 字段 含义 Cache-control 控制缓存的行为 Connection 控制不再转发给代理的收不字段、管理持久连接 Date 创建报文的日期时间 Pragma 报文指令 Trailer 报文末端的首部一览 Transfer-Encoding 指定报文主题的传输编码方式 Upgrade 升级为其他协议 Via 代理服务器的相关信息 Warning 错误通知 3.7 响应头字段Response Header 字段 含义 Accept-Ranges 是否接收字节范围请求 Age 推算资源创建经过的时间 ETag 资源的匹配信息 Location 另客户端重定向至指定的URI Proxy-Authenticate 代理服务器对客户端的认证信息 Retry-After 对再次发起请求的时机要求 Vary 代理服务器缓存的管理信息 WWW-Authenticate 服务器对客户端的认证信息 3.8 实体头字段 Entity Header 字段 含义 Allow 资源科支持的http方法 Connect-Encoding 实体主体适用的编码格式 Content-Language 实体主体的自然语言 Content-length 实体主体的大小 Content-Location 替代敌营资源的URI Content-MD5 实体主体的报文摘要 Content-Range 实体主体的位置范围 Content-Type 实体主体的媒体类型 Expires 实体主体过期的日期时间 Last-Modified 资源的最后修改日期时间","categories":[{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"}],"tags":[{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"}]},{"title":"二分法查找题","slug":"algorithm-dinary-search","date":"2021-11-26T13:57:32.000Z","updated":"2021-11-26T13:57:32.000Z","comments":true,"path":"algorithm-dinary-search/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/algorithm-dinary-search/index.html","excerpt":"","text":"1. 第一个错误的版本 1.1 题目描述 你是产品经理，目前正在带领一个团队开发新的产品。不幸的是，你的产品的最新版本没有通过质量检测。由于每个版本都是基于之前的版本开发的，所以错误的版本之后的所有版本都是错的。 假设你有 n 个版本 [1, 2, …, n]，你想找出导致之后所有版本出错的第一个错误的版本。 你可以通过调用bool isBadVersion(version)接口来判断版本号 version 是否在单元测试中出错。实现一个函数来查找第一个错误的版本。你应该尽量减少对调用 API 的次数。 示例 1234567输入：n = 5, bad = 4输出：4解释： 调用 isBadVersion(3) -&gt; false 调用 isBadVersion(5) -&gt; true 调用 isBadVersion(4) -&gt; true所以，4 是第一个错误的版本 1.2 解题思路 当一个版本为正确版本，则该版本之前的所有版本均为正确版本；当一个版本为错误版本，则该版本之后的所有版本均为错误版本。我们可以利用这个性质进行二分查找。 具体地，将左右边界分别初始化为 1和 n ，其中 n 是给定的版本数量。设定左右边界之后，每次我们都依据左右边界找到其中间的版本，检查其是否为正确版本。如果该版本为正确版本，那么第一个错误的版本必然位于该版本的右侧，我们缩紧左边界；否则第一个错误的版本必然位于该版本及该版本的左侧，我们缩紧右边界。 这样我们每判断一次都可以缩紧一次边界，而每次缩紧时两边界距离将变为原来的一半，因此我们至多只需要缩紧 O(logn) 次。 1.3 代码 123456789101112131415public int firstBadVersion(int n) &#123; int left = 1, right = n; while (left &lt; right)&#123; int mid = left + (right - left) / 2; // 防止计算时溢出 if (isBadVersion(mid))&#123; // 答案在区间 [left, mid] 中 right = mid; //如果中间版本是错误的版本，那么它之后的都是错误的; &#125;else &#123; // 答案在区间 [mid+1, right] 中 left = mid + 1; &#125; &#125; //此时有 left == right,退出了while循环 return left;&#125;","categories":[{"name":"10 算法","slug":"10-算法","permalink":"https://xiaoyuge5201.github.io/categories/10-%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiaoyuge5201.github.io/tags/algorithm/"}]},{"title":"Mysql添加/修改/删除字段","slug":"mysql-column","date":"2021-11-15T13:13:42.000Z","updated":"2021-11-15T13:13:42.000Z","comments":true,"path":"mysql-column/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql-column/index.html","excerpt":"","text":"1. 添加字段 1.1 在末尾添加字段 1）语法： 1ALTER TABLE &lt;表名&gt; ADD &lt;字段名&gt; &lt;数据类型&gt; [约束条件]; 语法格式的说明： &lt;表名&gt; 为数据表的名字； &lt;字段名&gt; 为所要添加的字段的名字； &lt;数据类型&gt; 为所要添加的字段能存储数据的数据类型； [约束条件] 是可选的，用来对添加的字段进行约束。 这种语法格式默认在表的最后位置（最后一列的后面）添加新字段 2）示例：在user表末尾添加字段phone 1ALTER TABLE user ADD phone VARCHAR(11) DEFAULT NULL COMMENT &#x27;电话号码&#x27;; 1.2 在开头添加字段 1）语法： 1ALTER TABLE &lt;表名&gt; ADD &lt;字段名&gt; &lt;数据类型&gt; [约束条件] FIRST; FIRST 关键字一般放在语句的末尾 2）示例：在user表开头添加字段user_id 1ALTER TABLE user ADD user_id VARCHAR(32) NOT NULL COMMENT &#x27;用户主键&#x27; FIRST; 1.3 在中间添加字段 1）语法： 1ALTER TABLE &lt;表名&gt; ADD &lt;字段名&gt; &lt;数据类型&gt; [约束条件] AFTER &lt;已经存在的字段名&gt;; AFTER 的作用是将新字段添加到某个已有字段后面。 注意：只能在某个已有字段的后面添加新字段，不能在它的前面添加新字段 2）示例：在user表的user_id字段后添加username字段 1ALTER TABLE user ADD username VARCHAR(30) DEFAULT NULL COMMENT &#x27;用户名&#x27; AFTER `user_id`; 2. 修改字段 2.1 修改字段属性 1）语法： 1ALTER TABLE &lt;表名&gt; MODIFY &lt;字段名&gt; &lt;数据类型&gt; [约束条件]; 2）示例1：修改字段属性 12-- 将email字段VARCHAR(50)修改成VARCHAR(200)ALTER TABLE user MODIFY email VARCHAR(200) NOT NULL DEFAULT &#x27;email@163.com&#x27;; 注意：修改时如果不带完整性约束条件，原有的约束条件将丢失，如果想保留修改时就得带上完整性约束条件 3）示例2： 将email移到phone后面 1ALTER TABLE user MODIFY email VARCHAR(50) AFTER `phone`; 4）示例3：放置第一个，保留原完成性约束条件 1ALTER TABLE user`MODIFY email VARCHAR(50) NOT NULL DEFAULT &#x27;test@163.com&#x27; FIRST; 5）示例4：修改成大小写敏感，即查询区分大小写 1ALTER TABLE user MODIFY username VARCHAR(30) BINARY CHARACTER SET utf8 COLLATE utf8_bin DEFAULT NULL COMMENT &#x27;用户名&#x27;; 2.2 修改字段名称和属性 1）语法： 1ALTER TABLE &lt;表名&gt; CHANGE &lt;原字段名&gt; &lt;新字段名&gt; &lt;数据类型&gt; [约束条件]; 2）示例：将username字段修改成user_name 1ALTER TABLE user CHANGE username user_name VARCHAR(30) DEFAULT NULL COMMENT &#x27;用户名&#x27;; 2.3 添加、删除默认值 1）语法： 12345-- 添加默认值ALTER TABLE &lt;表名&gt; ALTER &lt;字段名&gt; SET DEFAULT &lt;默认值&gt;;-- 删除默认值ALTER TABLE &lt;表名&gt; ALTER &lt;字段名&gt; DROP DEFAULT; 2）示例：给sex添加默认值 1ALTER TABLE USER ALTER sex SET DEFAULT &#x27;难&#x27;; 3）示例：删除sex默认值 1ALTER TABLE user ALTER sex DROP DEFAULT; 2.4 添加、删除主键 语法： 12345-- 添加主键ALTER TABLE &lt;表名&gt; ADD [CONSTRAINT &lt;约束名&gt;] PRIMARY KEY (&lt;字段名称,...&gt;);-- 删除主键ALTER TABLE &lt;表名&gt; DROP PRIMARY KEY; 2）示例：添加主键 1ALTER TABLE user ADD PRIMARY KEY (user_id) 3）示例：添加复合主键 1ALTER TABLE user_role ADD PRIMARY KEY (user_id, role_id); 4）示例：删除主键 1ALTER TABLE user DROP PRIMARY KEY; 5）示例：删除带自增长属性的主键 1234-- 先用MODIFY删除自增长属性，注意MODIFY不能去掉主键属性ALTER TABLE test MODIFY id INT UNSIGNED;-- 再来删除主键ALTER TABLE test DROP PRIMARY KEY; 2.5 添加、删除唯一索引 1）语法： 12345-- 添加唯一性约束ALTER TABLE &lt;表名&gt; ADD [CONSTANT &lt;约束名&gt;] UNIQUE [INDEX | KEY] [索引名称](&lt;字段名称,...&gt;)-- 删除唯一性约束ALTER TABLE &lt;表名&gt; DROP [INDEX | KEY] [索引名称]; 2）示例：为username添加唯一性约束，如果没有指定索引名称，系统会以字段名建立索引 1ALTER TABLE user ADD UNIQUE(username); 3）示例：为username添加唯一性约束，并指定索引名称 1ALTER TABLE user ADD UNION KEY uni_username(username); 4）示例：查看索引 1SHOW CREATE TABLE user; 5）示例：添加联合UNIQUE 1ALTER TABLE user ADD UNIQUE INDEX uni_nickname_username(nickname, username); 6）示例：删除索引 123ALTER TABLE user DROP INDEX username;ALTER TABLE user DROP KEY uni_username;ALTER TABLE user DROP INDEX uni_nickname_username; 2.6 修改表的存储引擎 1）语法： 1ALTER TABLE &lt;表名&gt; ENGINE=&lt;存储引擎名称&gt; 2）示例： 12ALTER TABLE user ENGINE=MyISAM;ALTER TABLE user ENGINE=INNODB; 2.7 修改自增长值 1）语法： 1ALTER TABLE &lt;表名&gt; AUTO_INCREMENT=[值]; 2）示例： 1ALTER TABLE user AUTO_INCREMENT= 100; 博客原文链接：https://www.cnblogs.com/Jimc/p/12979319.html 如有侵权，请联系删除！","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"TIDB设置sql_mode","slug":"sql-model","date":"2021-11-15T12:57:46.000Z","updated":"2021-11-15T12:57:46.000Z","comments":true,"path":"sql-model/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/sql-model/index.html","excerpt":"","text":"1. 使用命令查询当前sql_mode 123select @@sql_mode-- 或者select @@GLOBAL.sql_mode sql_mode常用值： ONLY_FULL_GROUP_BY 对于GROUP BY聚合操作，如果在SELECT中的列，没有在GROUP BY中出现，那么这个SQL是不合法的，因为列不在GROUP BY从句中 NO_AUTO_VALUE_ON_ZERO 该值影响自增长列的插入。默认设置下，插入0或NULL代表生成下一个自增长值。如果用户希望插入的值为0，而该列又是自增长的，那么这个选项就有用了。 STRICT_TRANS_TABLES 在该模式下，如果一个值不能插入到一个事务中，则中断当前的操作，对非事务表不做限制 NO_ZERO_IN_DATE 在严格模式下，不允许日期和月份为零 NO_ZERO_DATE 设置该值，mysql数据库不允许插入零日期，插入零日期会抛出错误而不是警告 ERROR_FOR_DIVISION_BY_ZERO 在insert或update过程中，如果数据被零除，则产生错误而非警告。如果未给出该模式，那么数据被零除时Mysql返回NULL NO_AUTO_CREATE_USER 禁止GRANT创建密码为空的用户 NO_ENGINE_SUBSTITUTION 如果需要的存储引擎被禁用或未编译，那么抛出错误。不设置此值时，用默认的存储引擎替代，并抛出一个异常 PIPES_AS_CONCAT 将&quot;||&quot;视为字符串的连接操作符而非或运算符，这和Oracle数据库是一样是，也和字符串的拼接函数Concat想类似 ANSI_QUOTES 启用ANSI_QUOTES后，不能用双引号来引用字符串，因为它被解释为识别符 2. 临时设置（新session仍然使用之前的sql_mode） 1set sql_mode=‘ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES’; 3. 全局设置（新session仍然使用修改后的sql_mode） 1set @@global.sql_mode=&#x27;ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE‘；","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"tidb","slug":"tidb","permalink":"https://xiaoyuge5201.github.io/tags/tidb/"}]},{"title":"Java中将List列表转换为字符串","slug":"list-to-string","date":"2021-10-10T09:48:19.000Z","updated":"2021-10-10T09:48:19.000Z","comments":true,"path":"list-to-string/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/list-to-string/index.html","excerpt":"","text":"1. toString() 方法 List.toString()是最简单的，但它在开头和结尾添加方括号，每个字符串用逗号分隔符分隔。 缺点是我们不能用另一个分隔符替换逗号，也不能去掉方括号 12345678910111213141516171819public class ListToStringUsingToStringExample &#123; public static void main(String args) &#123; // creating a list with strings. List&lt;String&gt; list = Arrays.asList(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;); // converting List&lt;String&gt; to String using toString() method String stringFromList = list.toString(); // priting the string System.out.println(&quot;String : &quot;+stringFromList); &#125;&#125;// 输出：String : [One, Two, Three, Four, Five] 2. Java 8 String.join() java 8 String添加了一个特殊的方法String.join()以将集合转换为具有给定分隔符的字符串 123456789101112131415161718192021222324252627public class ListToStringUsingString_JoinExample &#123; public static void main(String args) &#123; // creating a list with strings. List&lt;String&gt; list = Arrays.asList(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;); // converting List&lt;String&gt; to String using toString() method String stringFromList = String.join(&quot;~&quot;, list); // priting the string System.out.println(&quot;String with tilde delimiter: &quot;+stringFromList); // delimiting with pipe | symbol. String stringPipe = String.join(&quot;|&quot;, list); // printing System.out.println(&quot;String with pipe delimiter : &quot;+stringPipe); &#125;&#125;//输出：// String with tilde delimiter: One~Two~Three~Four~Five// String with pipe delimiter : One|Two|Three|Four|Five 3. Collectors.joining() Collectors.join()方法来自 java 8 stream api。Collctors.joining()方法将分隔符、前缀和后缀作为参数。此方法将列表转换为具有给定分隔符、前缀和后缀的字符串。 查看以下有关使用不同分隔符的 join() 方法的示例。但是，String.join() 方法不提供前缀和后缀选项。 123456789101112131415161718192021222324public class ListToStringUsingString_JoinExample &#123; public static void main(String args) &#123; // creating a list with strings. List&lt;String&gt; list = Arrays.asList(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;); // using java 8 Collectors.joining with delimiter, prefix and suffix String joiningString = list.stream().collect(Collectors.joining(&quot;-&quot;, &quot;&#123;&quot;, &quot;&#125;&quot;)); // printing System.out.println(&quot;Collectors.joining string : &quot;+joiningString); String joiningString3 = list.stream().collect(Collectors.joining(&quot;@&quot;, &quot;&quot;, &quot;&quot;)); // printing System.out.println(&quot;Collectors.joining string with @ separator : &quot;+joiningString3); &#125;&#125;//输出：//Collectors.joining string : &#123;One-Two-Three-Four-Five&#125;//Collectors.joining string with @ separator : One@Two@Three@Four@Five 4. Apache Commons StringUtils.join() 使用来自 apache commons 包的外部库。该库有一个方法StringUtils.join() ，它采用类似于 String.join() 方法的列表和分隔符 12345678910111213141516171819202122232425public class ListToStringUsingStringUtils_JoinExample &#123; public static void main(String args) &#123; // creating a list with strings. List&lt;String&gt; list = Arrays.asList(&quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;); // using java 8 Collectors.joining with delimiter, prefix and suffix String joiningString = StringUtils.join(list, &quot;^&quot;); // printing System.out.println(&quot;StringUtils.join string with ^ delimiter : &quot;+joiningString); String joiningString3 = StringUtils.join(list, &quot;$&quot;); // printing System.out.println(&quot;StringUtils.join string with @ separator : &quot;+joiningString3); &#125;&#125;//输出：// StringUtils.join string with ^ delimiter : One^Two^Three^Four^Five// StringUtils.join string with @ separator : One$Two$Three$Four$Five","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"集合","slug":"集合","permalink":"https://xiaoyuge5201.github.io/tags/%E9%9B%86%E5%90%88/"}]},{"title":"linux关机命令","slug":"shutdown","date":"2021-10-02T03:39:17.000Z","updated":"2021-10-02T03:39:17.000Z","comments":true,"path":"shutdown/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/shutdown/index.html","excerpt":"","text":"1. shutdown命令 shutdown会给系统计划一个时间关机。它可以被用于停止、关机、重启机器。 你可以指定一个时间字符串（通常是 now或者用hh:mm 指定小时/分钟）作为第一个参数。 shutdown命令示例： 12345678910111213shutdownshutdown now #立即关机shutdown 13:20 # 下午13：20关机shutdown -p now ### 关闭机器shutdown -H now ### 停止机器 shutdown -r09:35 ### 在 09:35am 重启机器shutdown -c ## 取消关机 2. halt 命令 halt通知硬件来停止所有的 CPU 功能，但是仍然保持通电。你可以用它使系统处于低层维护状态。 注意在有些情况会它会完全关闭系统。 halt 命令示例： 12345halt ### 停止机器halt -p ### 关闭机器halt --reboot ### 重启机器 3.poweroff 命令 poweroff会发送一个 ACPI 信号来通知系统关机 12345poweroff ### 关闭机器poweroff --halt ### 停止机器poweroff --reboot ### 重启机器 4.reboot 命令 reboot 通知系统重启。 12345reboot ### 重启机器reboot --halt ### 停止机器reboot -p ### 关闭机器 5. init 命令 一. init是Linux系统操作中不可缺少的程序之一。 所谓的init进程，它是一个由内核启动的用户级进程。 内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。所以,init始终是第一个进程（其进程编号始终为1）。 内核会在过去曾使用过init的几个地方查找它，它的正确位置（对Linux系统来说）是/sbin/init。如果内核找不到init，它就会试着运行/bin/sh，如果运行失败，系统的启动也会失败。 二. init一共分为7个级别，这7个级别的所代表的含义如下 0：停机或者关机（千万不能将initdefault设置为0） 1：单用户模式，只root用户进行维护 2：多用户模式，不能使用NFS(Net File System) 3：完全多用户模式（标准的运行级别） 4：安全模式 5：图形化（即图形界面） 6：重启（千万不要把initdefault设置为6）","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"shutdown","slug":"shutdown","permalink":"https://xiaoyuge5201.github.io/tags/shutdown/"}]},{"title":"安装Centos系统以及配置IP","slug":"Installing-CentOS-system","date":"2021-10-01T08:47:11.000Z","updated":"2021-10-01T08:47:11.000Z","comments":true,"path":"Installing-CentOS-system/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/Installing-CentOS-system/index.html","excerpt":"","text":"1. 系统安装 安装 选择语言 设置时区以及软件安装 选择GNOME桌面，开发工具。然后点击左上角的完成 配置分区，选址本地标准磁盘，并且在分区中勾选&quot;我要配置分区&quot;,“我想让额外空间可用”；初学者可以使用自动配置分区 开始安装 设置Root账户 等待安装成功,安装成功后会提示重启； 2. 永久关闭防火墙 1234567891011121314151617一、下面是red hat/CentOs7关闭防火墙的命令!#1:查看防火状态systemctl status firewalldservice iptables status#2:暂时关闭防火墙systemctl stop firewalldservice iptables stop#3:永久关闭防火墙systemctl disable firewalldchkconfig iptables off#4:重启防火墙systemctl enable firewalldservice iptables restart 3. 配置SELinux SELinux是Linux 内核中提供的强制访问控制系统。selinux有disabled、permissive、enforcing 三种选择： disabled ：不启用控制系统。 permissive：开启控制系统，但是处于警告模式。即使你违反了策略的话它让你继续操作，但是把你的违反的内容记录下来。 Enforcing：开启控制系统，处于强制状态。一旦违反了策略，就无法继续操作下去 使用命令： 12cd /etc/sysconfig/vim selinux 4.修改ip配置文件 进入文件目录 1cd /etc/sysconfig/network-scripts/ #进入配置文件 写入配置信息并保存退出 1vim ifcfg-ens33 #编辑配置文件ifcfg-ens33 如果要设置固定IP的话，常见设置属性有：BOOTPROTO、ONBOOT、IPADDR、NETMASK、GATEWAY 12345678910111213141516171819202122#以下为配置文件的内容TYPE=&quot;Ethernet&quot; #网卡类型（通常是Ethemet以太网）PROXY_METHOD=&quot;none&quot; #代理方式：为关闭状态BROWSER_ONLY=&quot;no&quot; #只是浏览器：否BOOTPROTO=&quot;static&quot; #网卡的引导协议【static：静态IP(指定静态后IP地址就固定了,不建议采用动态分配) dhcp：动态IP none：不指定，不指定容易出现各种各样的网络受限】DEFROUTE=&quot;yes&quot; #默认路由IPV4_FAILURE_FATAL=&quot;no&quot; #是否开启IPV4致命错误检测IPV6INIT=&quot;yes&quot; #IPV6是否自动初始化：是（现在还未用到IPV6，不会有任何影响）IPV6_AUTOCONF=&quot;yes&quot; #IPV6是否自动配置：是（现在还未用到IPV6，不会有任何影响）IPV6_DEFROUTE=&quot;yes&quot; #IPV6是否可以为默认路由：是（现在还未用到IPV6，不会有任何影响）IPV6_FAILURE_FATAL=&quot;no&quot; #是否开启IPV6致命错误检测IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot; #IPV6地址生成模型NAME=&quot;ens33&quot; #网卡物理设备名称UUID=&quot;ab60d501-535b-49f5-a76b-3336a4120f64&quot; #通用唯一识别码，每一个网卡都会有，不能重复，否则两台linux机器只有一台可上网,可不写DEVICE=&quot;ens33&quot; #网卡设备名称，必须和‘NAME’值一样ONBOOT=&quot;yes&quot; #是否开机启动(如果yes则开机后自动加载使用当前配置文件)，要想网卡开机就启动或通过 `systemctl restart network`控制网卡,必须设置为 `yes`IPADDR=192.168.1.111 # 本机IP 设置固定IP 对应上面的BOOTPROTONETMASK=255.255.255.0 #子网掩码 ,可不写GATEWAY=192.168.137.2 #默认网关 ,可不写DNS1=8.8.8.8 # 可不写DNS2=8.8.8.5 # 可不写ZONE=public # 可不写 重启网络服务 1service network restart #重启网卡 查看IP 1ip addr 重启系统 1reboot","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"linux","slug":"linux","permalink":"https://xiaoyuge5201.github.io/tags/linux/"}]},{"title":"LockSupport线程工具类","slug":"locksupport","date":"2021-09-25T04:59:36.000Z","updated":"2021-09-25T04:59:36.000Z","comments":true,"path":"locksupport/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/locksupport/index.html","excerpt":"","text":"1. 概要 LockSupport位于java.util.concurrent（简称juc）包中，是一个编程工具类， 主要是为了阻塞和唤醒线程用的。所有的方法都是静态方法，可以让线程在任意位置阻塞，也可以在任意位置唤醒 主要的方法： park(阻塞线程) 和 unpark(启动唤醒线程) 关于线程等待/唤醒的方法： 方式1：使用Object中的wait()方法让线程等待，使用Object中的notify()方法唤醒线程 使用juc包中Condition的await()方法让线程等待，使用signal()方法唤醒线程 2. wait/notify 示例1 1234567891011121314151617181920212223242526public class ObjectDemo &#123; static final Object lock = new Object(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; synchronized (lock)&#123; System.out.println(Thread.currentThread().getName()+&quot;: &quot;+System.currentTimeMillis()+&quot; start&quot;); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;: &quot;+System.currentTimeMillis()+&quot; 被唤醒&quot;); &#125; &#125;); t1.setName(&quot;t1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(5); synchronized (lock)&#123; lock.notify(); &#125; &#125;&#125; 输出： 12t1: 1667620040963 startt1: 1667620045967 被唤醒 t1 线程调用lock.wait()方法让t1线程等待，主线程休眠5s后，调用lock.notify()方法唤醒t1线程，然后输出信息，程序正常退出。 示例2 如果将上面代码块中的两个synchronized去掉，发现调用wait()方法和notify()方法都会报错 12345678910111213141516171819202122public class ObjectDemo &#123; static final Object lock = new Object(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + System.currentTimeMillis() + &quot; start&quot;); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;: &quot; + System.currentTimeMillis() + &quot; 被唤醒&quot;); &#125;); t1.setName(&quot;thread1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(5); lock.notify(); &#125;&#125; 123456789thread1: 1667624638968 startException in thread &quot;thread1&quot; java.lang.IllegalMonitorStateException at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:502) at org.example.ObjectDemo.lambda$main$0(ObjectDemo.java:13) at java.lang.Thread.run(Thread.java:748)Exception in thread &quot;main&quot; java.lang.IllegalMonitorStateException at java.lang.Object.notify(Native Method) at org.example.ObjectDemo.main(ObjectDemo.java:27) 原因： Object类中的wait、notify、notifyAll用于线程等待和唤醒的方法，都必须在同步代码块中运行（必须使用关键字synchronized） 示例3 唤醒方法在等待方法之前 1234567891011121314151617181920212223242526272829303132public class ObjectDemo &#123; static final Object lock = new Object(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; synchronized (lock)&#123; System.out.println(Thread.currentThread().getName()+&quot;: &quot;+System.currentTimeMillis()+&quot; start&quot;); try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()+&quot;: &quot;+System.currentTimeMillis()+&quot; 被唤醒&quot;); &#125; &#125;); t1.setName(&quot;thread1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(1); synchronized (lock)&#123; lock.notify(); &#125; System.out.println(&quot;lock.notify 执行完毕&quot;); &#125;&#125; 输出： 12lock.notify 执行完毕thread1: 1667625571660 start 输出上面2行之后，程序一直无法结束，t1线程调用wait()方法之前先调用了notify()方法，导致等待的线程无法被唤醒了 唤醒方法在等待方法之前执行，线程无法被唤醒，将上面休眠1s的时间改成大于线程中休眠的时间即可； 关于Object类中的用户线程等待和唤醒的方法，总结一下： wait()/notify()/notifyAll()方法都必须放在同步代码（必须在synchronized内部执行）中执行，需要先获取锁 线程唤醒的方法（notify、notifyAll）需要在等待的方法（wait）之后执行，等待中的线程才可能会被唤醒，否则无法唤醒 3. condition实现线程等待和唤醒 示例1 1234567891011121314151617181920212223242526272829303132public class ConditionDemo &#123; static ReentrantLock lock = new ReentrantLock(); static Condition condition = lock.newCondition(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; lock.lock(); try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; start&quot;); try &#123; condition.await(); //进入等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; 被唤醒&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;); t1.setName(&quot;t1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(5); lock.lock(); try &#123; condition.signal(); //唤醒 t1线程 &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; 输出： 12t1:1667712939347 startt1:1667712944350 被唤醒 t1 线程制动之后，调用condition.await()方法将线程处于等待中，主线程休眠5秒之后调用condition.signal()方法将t1线程唤醒； 示例2 1234567891011121314151617181920212223242526public class ConditionDemo &#123; static ReentrantLock lock = new ReentrantLock(); static Condition condition = lock.newCondition(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; start&quot;); try &#123; condition.await(); //进入等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; 被唤醒&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;); t1.setName(&quot;t1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(5); condition.signal(); //唤醒 t1线程 &#125;&#125; 输出： 12345678910t1:1667713155895 startException in thread &quot;t1&quot; java.lang.IllegalMonitorStateException at java.util.concurrent.locks.ReentrantLock$Sync.tryRelease(ReentrantLock.java:151) at java.util.concurrent.locks.AbstractQueuedSynchronizer.release(AbstractQueuedSynchronizer.java:1261) at java.util.concurrent.locks.ReentrantLock.unlock(ReentrantLock.java:457) at org.example.ConditionDemo.lambda$main$0(ConditionDemo.java:23) at java.lang.Thread.run(Thread.java:748)Exception in thread &quot;main&quot; java.lang.IllegalMonitorStateException at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.signal(AbstractQueuedSynchronizer.java:1939) at org.example.ConditionDemo.main(ConditionDemo.java:30) 有异常发生， condition.await();和 condition.signal();都触发了 IllegalMonitorStateException异常。 原因：调用condition中线程等待和唤醒的方法的前提是必须要先获取lock的锁。 示例3 唤醒代码在等待之前执行 123456789101112131415161718192021222324252627282930313233343536373839public class ConditionDemo &#123; static ReentrantLock lock = new ReentrantLock(); static Condition condition = lock.newCondition(); public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; try &#123; System.out.println(&quot;进入线程t1&quot;); try &#123; TimeUnit.SECONDS.sleep(5); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.lock(); System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; start&quot;); try &#123; condition.await(); //进入等待 &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; 被唤醒&quot;); &#125; finally &#123; lock.unlock(); &#125; &#125;); t1.setName(&quot;t1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(1); lock.lock(); try&#123; condition.signal(); //唤醒 t1线程 &#125;finally &#123; lock.unlock(); &#125; System.out.println(System.currentTimeMillis() +&quot; condition.signal;执行完毕&quot;); &#125;&#125; 输出： 123进入线程t11667714134893 condition.signal;执行完毕t1:1667714138893 start 输出上面2行之后，程序无法结束，代码结合输出可以看出signal()方法在await()方法之前执行的，最终t1线程无法被唤醒，导致程序无法结束。 关于Condition中方法使用总结： 使用Condition中的线程等待和唤醒方法之前，需要先获取锁。否者会报 IllegalMonitorStateException异常 signal()方法先于await()方法之前调用，线程无法被唤醒 Object和Condition的局限性 Object和Condition的局限性 关于Object和Condtion中线程等待和唤醒的局限性，有以下几点： 2种方式中的让线程等待和唤醒的方法能够执行的先决条件是：线程需要先获取锁 唤醒方法需要在等待方法之后调用，线程才能够被唤醒 关于这2点，LockSupport都不需要，就能实现线程的等待和唤醒。下面我们来说一下LockSupport类。 4. LockSupport LockSupport类可以阻塞当前线程以及唤醒指定被阻塞的线程，主要是通过park()和unpark(thread)方法来实现阻塞和唤醒线程操作的 每个线程都有一个许可（permit），permit只有两个值 1 和 0（默认） 当调用unpark(thread)方法，就会将thread线程的许可permit设置为1（多次调用结果一致） 当嗲用park()方法，如果当前线程的permit是1， 那么将permit 设置为0，并立即返回；如果当前park方法会被唤醒，然后会将permit再次设置为0，并返回； 注意：因为permit默认是0，所以一开始调用park()方法，线程必定会被阻塞。调用unpark(thread)方法后，会自动唤醒thread线程，即park方法立即返回。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151//源码package java.util.concurrent.locks;import sun.misc.Unsafe; public class LockSupport &#123; private LockSupport() &#123;&#125; // Cannot be instantiated. private static void setBlocker(Thread t, Object arg) &#123; // Even though volatile, hotspot doesn&#x27;t need a write barrier here. UNSAFE.putObject(t, parkBlockerOffset, arg); &#125; /** * @param thread the thread to unpark, or &#123;@code null&#125;, in which case * this operation has no effect */ public static void unpark(Thread thread) &#123; if (thread != null) UNSAFE.unpark(thread); &#125; /** * 阻塞当前线程 * blocker是用来记录线程被阻塞时被谁阻塞的。用于线程监控和分析工具来定位原因的。 * @param blocker the synchronization object responsible for this * thread parking * @since 1.6 */ public static void park(Object blocker) &#123; Thread t = Thread.currentThread(); //setBlocker作用是记录t线程是被broker阻塞的 setBlocker(t, blocker); //UNSAFE是一个非常强大的类，他的的操作是基于底层的 UNSAFE.park(false, 0L); setBlocker(t, null); &#125; /** * 暂停当前线程，有超时时间 * blocker是用来记录线程被阻塞时被谁阻塞的。用于线程监控和分析工具来定位原因的。 * @param blocker the synchronization object responsible for this * thread parking * @param nanos the maximum number of nanoseconds to wait * @since 1.6 */ public static void parkNanos(Object blocker, long nanos) &#123; if (nanos &gt; 0) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, nanos); setBlocker(t, null); &#125; &#125; /** * 暂停当前线程，知道某个时间 * blocker是用来记录线程被阻塞时被谁阻塞的。用于线程监控和分析工具来定位原因的。 * @param blocker the synchronization object responsible for this * thread parking * @param deadline the absolute time, in milliseconds from the Epoch, * to wait until * @since 1.6 */ public static void parkUntil(Object blocker, long deadline) &#123; Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(true, deadline); setBlocker(t, null); &#125; /** * Returns the blocker object supplied to the most recent * invocation of a park method that has not yet unblocked, or null * if not blocked. The value returned is just a momentary * snapshot -- the thread may have since unblocked or blocked on a * different blocker object. * * @param t the thread * @return the blocker * @throws NullPointerException if argument is null * @since 1.6 */ public static Object getBlocker(Thread t) &#123; if (t == null) throw new NullPointerException(); return UNSAFE.getObjectVolatile(t, parkBlockerOffset); &#125; /** * 无期限暂停当前线程 */ public static void park() &#123; UNSAFE.park(false, 0L); &#125; /** * 暂停当前线程，不过有超时时间限制 */ public static void parkNanos(long nanos) &#123; if (nanos &gt; 0) UNSAFE.park(false, nanos); &#125; /** * 暂停当前线程，知道某个时间 * @param deadline 暂停结束时间 */ public static void parkUntil(long deadline) &#123; UNSAFE.park(true, deadline); &#125; /** * Returns the pseudo-randomly initialized or updated secondary seed. * Copied from ThreadLocalRandom due to package access restrictions. */ static final int nextSecondarySeed() &#123; int r; Thread t = Thread.currentThread(); if ((r = UNSAFE.getInt(t, SECONDARY)) != 0) &#123; r ^= r &lt;&lt; 13; // xorshift r ^= r &gt;&gt;&gt; 17; r ^= r &lt;&lt; 5; &#125; else if ((r = java.util.concurrent.ThreadLocalRandom.current().nextInt()) == 0) r = 1; // avoid zero UNSAFE.putInt(t, SECONDARY, r); return r; &#125; // Hotspot implementation via intrinsics API private static final sun.misc.Unsafe UNSAFE; private static final long parkBlockerOffset; private static final long SEED; private static final long PROBE; private static final long SECONDARY; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class&lt;?&gt; tk = Thread.class; parkBlockerOffset = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;parkBlocker&quot;)); SEED = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomSeed&quot;)); PROBE = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomProbe&quot;)); SECONDARY = UNSAFE.objectFieldOffset (tk.getDeclaredField(&quot;threadLocalRandomSecondarySeed&quot;)); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125;&#125; 4.1 示例 示例一 主线程线程等待5秒之后，唤醒t1线程 123456789101112131415public class LockSupportDemo1 &#123; public static void main(String[] args) throws InterruptedException &#123; Thread t1 = new Thread(() -&gt; &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; start&quot;); LockSupport.park();//阻塞当前线程 System.out.println(Thread.currentThread().getName() + &quot;:&quot; + System.currentTimeMillis() + &quot; 被唤醒&quot;); &#125;); t1.setName(&quot;t1&quot;); t1.start(); //休眠5秒 TimeUnit.SECONDS.sleep(5); LockSupport.unpark(t1); System.out.println(System.currentTimeMillis() + &quot; lock.unpart 执行完毕&quot;); &#125;&#125; 输出： 123t1:1667727256993 start1667727261994 lock.unpart 执行完毕t1:1667727261994 被唤醒 t1 中调用LockSupport.park()方法让当前线程t1等待，主线程休眠5秒后，调用LockSupport.unpart(t1)将线程唤醒 LockSupport.park();无参数，内部直接会让当前线程处于等待中；unpark方法传递了一个线程对象作为参数，表示将对应的线程唤醒。 4.3 先interrupt在park 12345678910111213141516171819202122232425public class LockSupportTest &#123; public static class MyThread extends Thread&#123; @Override public void run() &#123; System.out.println(getName() + &quot;进入线程&quot;); LockSupport.park(); System.out.println(&quot;运行结束&quot;); System.out.println(&quot;是否中断：&quot;+Thread.currentThread().isInterrupted()); &#125; &#125; public static void main(String[] args) &#123; MyThread thread = new MyThread(); thread.start(); System.out.println(&quot;线程启动了，但是在内部进行了park&quot;); thread.interrupt(); System.out.println(&quot;main 线程结束&quot;); &#125;&#125;//输出// 线程启动了，但是在内部进行了park// main 线程结束// Thread-0进入线程// 运行结束 4.2 先park在interrupt 1234567891011121314151617181920public static class MyThread extends Thread&#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(getName() + &quot;进入线程&quot;); LockSupport.park(); System.out.println(&quot;运行结束&quot;); &#125;&#125;/** * 输出： * 线程启动了，但是在内部进行了park * main 线程结束 * Thread-0进入线程 * 运行结束 */ 5. 线程等待和唤醒方式对比 方式1：Object中的wait、notify、notifyAll方法 方式2：juc中Condition接口提供的await、signal、signalAll方法 方式3：juc中的LockSupport提供的park、unpark方法 LockSupport是用来阻塞和环线线程的，wait/notify同样也是，那么两者的区别是什么？ wait和notify都是Object中的方法，在调用这两个方法前必须获得锁对象，但是park不需要获取某个对象的锁就可以锁住线程 notify只能随机选择一个线程唤醒，无法唤醒指定的线程，unpark可以唤醒一个指定的线程 6. 趣味题 用两个线程，一个输出字母，一个输出数字交替输出如：1A2B3C4D… 1234567891011121314151617181920212223242526public class ThreadDemoTest &#123; static Thread t1 = null, t2 = null; public static void main(String[] args) &#123; char[] a = &quot;1234567&quot;.toCharArray(); char[] b = &quot;ABCDEFG&quot;.toCharArray(); t1 = new Thread(() -&gt; &#123; for (char i : a) &#123; System.out.print(i); LockSupport.unpark(t2); LockSupport.park(); &#125; &#125;, &quot;t1&quot;); t2 = new Thread(() -&gt; &#123; for (char i : b) &#123; LockSupport.park(); System.out.print(i); LockSupport.unpark(t1); &#125; &#125;, &quot;t1&quot;); t1.start(); t2.start(); &#125;&#125;//输出： 1A2B3C4D5E6F7G 使用自旋锁也可以实现上面的结果 12345678910111213141516171819202122232425262728public class CasTest &#123; //定义枚举，包含两个变量 enum ReadyToRun&#123;T1, T2&#125;; static volatile ReadyToRun r = ReadyToRun.T1; public static void main(String[] args) &#123; char[] a = &quot;1234567&quot;.toCharArray(); char[] b = &quot;ABCDEFG&quot;.toCharArray(); new Thread(()-&gt;&#123; for (char c : a)&#123; //当r不为T1时， 空转占着cpu等待，然后输出字符，将r的值设置为T2 while (r != ReadyToRun.T1)&#123;&#125; System.out.print(c+&quot; &quot;); r = ReadyToRun.T2; &#125; &#125;,&quot;t1&quot;).start(); new Thread(()-&gt;&#123; for (char c : b)&#123; while (r != ReadyToRun.T2)&#123;&#125; System.out.print(c+&quot; &quot;); r = ReadyToRun.T1; &#125; &#125;,&quot;t2&quot;).start(); &#125;&#125;","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"}]},{"title":"mysql行列转置","slug":"mysql行列转置","date":"2021-08-25T02:36:37.000Z","updated":"2021-08-25T02:36:37.000Z","comments":true,"path":"mysql行列转置/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql%E8%A1%8C%E5%88%97%E8%BD%AC%E7%BD%AE/index.html","excerpt":"","text":"数据表： 行转列结果为： 数据库表语句： 1234567891011121314151617181920create table t_score( id int primary key auto_increment, name varchar(20) not null, #名字 Subject varchar(10) not null, #科目 Fraction double default 0 #分数);INSERT INTO `t_score`(name,Subject,Fraction) VALUES (&#x27;王海&#x27;, &#x27;语文&#x27;, 86), (&#x27;王海&#x27;, &#x27;数学&#x27;, 83), (&#x27;王海&#x27;, &#x27;英语&#x27;, 93), (&#x27;陶俊&#x27;, &#x27;语文&#x27;, 88), (&#x27;陶俊&#x27;, &#x27;数学&#x27;, 84), (&#x27;陶俊&#x27;, &#x27;英语&#x27;, 94), (&#x27;刘可&#x27;, &#x27;语文&#x27;, 80), (&#x27;刘可&#x27;, &#x27;数学&#x27;, 86), (&#x27;刘可&#x27;, &#x27;英语&#x27;, 88), (&#x27;李春&#x27;, &#x27;语文&#x27;, 89), (&#x27;李春&#x27;, &#x27;数学&#x27;, 80), (&#x27;李春&#x27;, &#x27;英语&#x27;, 87); 方法一：使用if 12345678910111213141516171819select name as 名字 , sum(if(Subject=&#x27;语文&#x27;,Fraction,0)) as 语文, sum(if(Subject=&#x27;数学&#x27;,Fraction,0))as 数学, sum(if(Subject=&#x27;英语&#x27;,Fraction,0))as 英语, round(AVG(Fraction),2) as 平均分, SUM(Fraction) as 总分from t_score group by name-- 如果不用求总分的话，不需要下面的unionunion( select name as 名字 , sum(语文) Chinese,sum(数学) Math,sum(英语) English,round(AVG(总分),2)as 平均分,sum(总分) score from( select &#x27;TOTAL&#x27; as name, sum(if(Subject=&#x27;语文&#x27;,Fraction,0)) as 语文, sum(if(Subject=&#x27;数学&#x27;,Fraction,0))as 数学, sum(if(Subject=&#x27;英语&#x27;,Fraction,0))as 英语, SUM(Fraction) as 总分 from t_score group by Subject )t GROUP BY t.`name`) 方法二：使用case 1234567891011121314151617select name as name,sum(case when Subject = &#x27;语文&#x27; then Fraction end) as Chinese,sum(case when Subject = &#x27;数学&#x27; then Fraction end) as Math,sum(case when Subject = &#x27;英语&#x27; then Fraction end) as English,sum(fraction)as scorefrom t_score group by name-- 如果不用求总分的话，不需要下面的unionUNION ALL( select name as Name,sum(Chinese) as Chinese,sum(Math) as Math,sum(English) as English,sum(score) as score from( select &#x27;TOTAL&#x27; as name, sum(case when Subject = &#x27;语文&#x27; then Fraction end) as Chinese, sum(case when Subject = &#x27;数学&#x27; then Fraction end) as Math, sum(case when Subject = &#x27;英语&#x27; then Fraction end) as English, sum(fraction)as score from t_score group by Subject,name)t GROUP BY t.`name`) 方法三：使用with rollup 在group分组字段的基础上在进行统计数据； 12345678select -- coalesce(name,&#x27;TOTAL&#x27;) name, ifnull(name,&#x27;TOTAL&#x27;) name, sum(if(Subject=&#x27;语文&#x27;,Fraction,0)) as 语文, sum(if(Subject=&#x27;英语&#x27;,Fraction,0)) as 英语, sum(if(Subject=&#x27;数学&#x27;,Fraction,0))as 数学, sum(Fraction) 总分from t_score group by name with rollup","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"转换成小写字母","slug":"algoright-2","date":"2021-08-22T05:15:12.000Z","updated":"2021-08-22T05:15:12.000Z","comments":true,"path":"algoright-2/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/algoright-2/index.html","excerpt":"","text":"实现函数 ToLowerCase()，该函数接收一个字符串参数 str，并将该字符串中的大写字母转换成小写字母，之后返回新的字符串。 1234567示例 1：输入: &quot;Hello&quot;输出: &quot;hello&quot;示例2：输入: &quot;LOVELY&quot;输出: &quot;lovely&quot; 方法一： ASCCII码 解题思路：通过ascii码表操作字符串即可,a和A相差32； a-z: 97 - 122 A-Z: 65 - 90 0-9: 48 - 57 123456789101112131415public static String toLowerCase(String str) &#123; if (str == null || str.length() == 0) &#123; return str; &#125; StringBuilder sb = new StringBuilder(); for (char ch : str.toCharArray()) &#123; // a-z：97-122 A-Z：65-90 0-9：48-57 if (ch &gt;= &#x27;A&#x27; &amp;&amp; ch &lt;= &#x27;Z&#x27;) &#123; sb.append((char)(ch + 32)); &#125; else &#123; sb.append(ch); &#125; &#125; return sb.toString();&#125; 或者： 123456789101112public static String toLowerCase(String str) &#123; if (str == null || str.length() == 0) &#123; return str; &#125; char[] ch = str.toCharArray(); for (int i = 0; i &lt; str.length(); i++) &#123; if (ch[i] &gt;= &#x27;A&#x27; &amp;&amp; ch[i] &lt;= &#x27;Z&#x27;) &#123; ch[i] += 32; &#125; &#125; return String.valueOf(ch);&#125; 方法二： 位运算 解题思路： 大写变小写、小写变大写：字符 ^= 32; 大写变小写、小写变小写：字符 |= 32; 大写变大写、小写变大写：字符 &amp;= 33; ASCII码表中大写的A是65，小写的a是97，它们的差是32 65 | 32 转为二进制（按8位来算）可以得到 0100 0001 | 0010 0000 = 0110 0001 = 97 = a 12345678910public String toLowerCase(String str) &#123; if (str == null || str.length() == 0) &#123; return str; &#125; char[] ch = str.toCharArray(); for (int i = 0; i &lt; str.length(); i++) &#123; ch[i] |= 32; &#125; return String.valueOf(ch);&#125;","categories":[{"name":"10 算法","slug":"10-算法","permalink":"https://xiaoyuge5201.github.io/categories/10-%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiaoyuge5201.github.io/tags/algorithm/"}]},{"title":"mysqldumpslow分析慢查询日志","slug":"mysqldumpslow分析慢查询日志","date":"2021-08-21T08:16:02.000Z","updated":"2021-08-21T08:16:02.000Z","comments":true,"path":"mysqldumpslow分析慢查询日志/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysqldumpslow%E5%88%86%E6%9E%90%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%97%A5%E5%BF%97/index.html","excerpt":"","text":"按照平均查询输出5行慢查询记录 1mysqldumpslow -s at -t 5 /phpstudy/data/slowquery.log -s 排序方式，可选值有c（记录次数）、t（查询时间）、l（锁定时间）、r（返回记录）、a（平均） -t 显示的记录数Spawn failed解决方式 -g 后面跟正则表达式（如 left join），不区分大小写。 -r 正序排序，即从小到大排序。 -d 调试 debug -v 查看版本 按照平均查询时间排序且只显示含有left join的记录 1mysqldumpslow -s at -g &#x27;left join&#x27; /phpstudy/data/slowquery.log","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"mysql索引","slug":"mysql-index","date":"2021-08-20T07:12:00.000Z","updated":"2021-08-20T07:12:00.000Z","comments":true,"path":"mysql-index/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql-index/index.html","excerpt":"","text":"拿汉语字典的目录页（索引）打比方，我们可以按拼音、笔画、偏旁部首等排序的目录（索引）快速查找到需要的字。 索引分单列索引和组合索引。 单列索引，即一个索引只包含单个列，一个表可以有多个单列索引，但这不是组合索引。 组合索引，即一个索引包含多个列。 创建索引时，你需要确保该索引是应用在 SQL 查询语句的条件(一般作为 WHERE 子句的条件)。 实际上，索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录。 上面都在说使用索引的好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点： 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。 1.普通索引 创建索引 12-- 如果是CHAR，VARCHAR类型，length可以小于字段实际长度；如果是BLOB和TEXT类型，必须指定 length。CREATE INDEX indexName on table_name (column_name ) 添加索引（修改表结构） 12-- 表结构已经存在了，然后使用alter修改表结构添加索引alter table table_name add INDEX indexName(column_name ) 创建表指定 12345CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, INDEX [indexName] (username(length)) ); 删除索引 1drop index [indexName] on table_name 2. 唯一索引 他和普通索引类似，不同的是：索引列的值必须唯一，但允许有控制。如果是组合索引，则列值的组合必须唯一。 创建索引 1CREATE UNIQUE INDEX indexName ON table_name (column_name (length )) 修改表结构 1ALTER table mytable ADD UNIQUE [indexName] (column_name(length)) 创建表的时候指定 12345CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, UNIQUE [indexName] (username(length)) ); 3.组合索引 修改表结构指定索引 1ALTER TABLE table_name ADD INDEX indexName (column_name1 , column_name2,...) 创建表的时候指定 12345CREATE TABLE mytable( ID INT NOT NULL, username VARCHAR(16) NOT NULL, KEY [indexName] (column_name1 , column_name2,...) ); 3.1 组合索引查询问题 问题描述：在mysql中有张表test_a，有3个字段id,name,num；对这三个字段建立组合索引，那么查询时使用其中某两个或者一个作为查询条件，是否还会走索引 根据查询字段的位置不同来决定，如查询id、id,num、id,num,name、 id, name 都可以走索引的，其他条件的查询不能走索引。 组合索引 有“最左前缀”原则。就是只从最左面的开始组合，并不是所有只要含有这三列存在的字段的查询都会用到该组合索引 12-- 添加组合索引ALTER TABLE test_a ADD INDEX &#x27;lianhe&#x27;(id, num, name) 使用三个字段id, num, name查询 123-- 只要三个条件都有，可以随意变换位置，结果都会走索引-- 优化器会将条件顺序优化成上面三种情况后执行EXPLAIN SELECT * FROM test_a WHERE id=1 AND num=&#x27;001&#x27; AND name = &#x27;1#&#x27; 从执行结果上可以看到是从走索引进行查询的 使用num, name 查询 123EXPLAIN SELECT * FROM test_a WHERE name = &#x27;1#&#x27;EXPLAIN SELECT * FROM test_a WHERE num=&#x27;001&#x27; 3. 使用id, name或者id, num 查询 1234-- 只要包括id，可以随意变换位置，结果都会走索引-- 优化器会将条件顺序优化成上面三种情况后执行--如果只有两个字段，只有id条件命中，num或者name 条件不走联合索引。EXPLAIN SELECT * FROM test_a WHERE id=1 AND name = &#x27;1#&#x27; 需要避免索引失效的情况，如：LIKE %xxx，或者条件中使用函数等。 4. 使用id查询 1EXPLAIN SELECT * FROM test_a WHERE id=1 使用name或者num查询 123EXPLAIN SELECT * FROM test_a WHERE name = &#x27;1#&#x27;EXPLAIN SELECT * FROM test_a WHERE num = &#x27;001&#x27; 3.2 创建组合索引选择规则 经常用的列优先（最左匹配原则） 离散度高的列优先（离散度高原则） 宽度小的列优先（最少空间原则） 4.使用alter命令添加索引 1234567891011-- 该语句添加一个主键，这意味着索引值必须是唯一的，且不能为NULL。ALTER TABLE tbl_name ADD PRIMARY KEY (column_list): -- 这条语句创建索引的值必须是唯一的（除了NULL外，NULL可能会出现多次）。ALTER TABLE tbl_name ADD UNIQUE index_name (column_list):-- 添加普通索引，索引值可出现多次。ALTER TABLE tbl_name ADD INDEX index_name (column_list):--该语句指定了索引为 FULLTEXT ，用于全文索引。ALTER TABLE tbl_name ADD FULLTEXT index_name (column_list):","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"二分查找","slug":"algorithm","date":"2021-08-19T14:54:43.000Z","updated":"2021-08-19T14:54:43.000Z","comments":true,"path":"algorithm/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/algorithm/index.html","excerpt":"","text":"二分法查找是一种基于比较目标值和数组中间元素的算法 如果目标值 = 中间值，则找到目标值 如果目标值 &lt; 中间值，则在左侧继续搜索 如果目标值 &gt; 中间值，则在右侧继续搜索 解题思路： 初始化指针left = 0, right=n-1; 当left &lt;= right： 比较中间元素nums[pivot]和目标值target 1.target = nums[pivot], 返回pivot 2.target &gt; nums[pivot], 则在右侧继续搜索left = pivot+1 3.target &lt; nums[pivot], 则在左侧继续搜索right = pivot+1 123456789101112131415161718192021/** * 给定一个 n 个元素有序的（升序）整型数组 nums 和一个目标值 target ， * 写一个函数搜索 nums 中的 target，如果目标值存在返回下标，否则返回 -1。 * 输入: nums = [-1,0,3,5,9,12], target = 9 * 输出: 4 * 解释: 9 出现在 nums 中并且下标为 4 */public int search(int[] nums, int target)&#123; int pivot, left =0, right = nums.length - 1; while (left &lt;= right)&#123; pivot = left + (right - left) / 2; if (nums[pivot] == target)&#123; return pivot; &#125;else if (nums[pivot] &lt; target)&#123; left = pivot + 1; &#125; else&#123; right = pivot - 1; &#125; &#125; return -1;&#125; 复杂度分析： 时间复杂度：O(logN) 空间复杂度：O(1)","categories":[{"name":"10 算法","slug":"10-算法","permalink":"https://xiaoyuge5201.github.io/categories/10-%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiaoyuge5201.github.io/tags/algorithm/"}]},{"title":"springboot读取yml配置的方式","slug":"springboot-yml","date":"2021-08-18T13:41:24.000Z","updated":"2021-08-18T13:41:24.000Z","comments":true,"path":"springboot-yml/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/springboot-yml/index.html","excerpt":"","text":"springboot项目中默认的配置文件是application.properties； 1.yml文件规则 树状结构，结构清晰 不支持tab缩进 可以使用&quot;_“或”-&quot;消协字母代替大写字母；如userName 与user-name， user_name含义是一样的（宽松绑定原则 relaxed binding）; key: value格式书写，value前面有个空格 2. 数据格式 普通的值（数字，字符串，布尔）如： 123port: 123 name: abc flag: true 字符串默认不用加上单引号或者双引号； “”：双引号；不会转义字符串里面的特殊字符；特殊字符会作为本身想表示的意思,name: “zhangsan \\n lisi”：输出；zhangsan 换行 lisi ‘’：单引号；会转义特殊字符，特殊字符最终只是一个普通的字符串数据,name: ‘zhangsan \\n lisi’：输出；zhangsan \\n lisi 对象、Map(属性和值)如： 1234567#k: v：在下一行来写对象的属性和值的关系；注意缩进(不支持tab,使用空格)server: port: 8123 tomcat: uri-encoding: utf-8 servlet: context-path: /app 数组（list， set） 1234#用- 值表示数组中的一个元素hands: - left - right 3. 读取方式 @Value注解 12server: port: 8081 12@Value(&quot;$&#123;server.port&#125;&quot;)public String port; 此处的port所在的类需要是一个组件,如果是实体类需要加上@Component @ConfigurationProperties 需要一个JavaBean 来专门映射配置的话,我们一般会使用@ConfigurationProperties来读取. 使用的使用需要@EnableConfigurationProperties注解让类被springboot扫描到； 1234567spring: datasource: druid: url: jdbc:mysql://localhost:3307/app?useUnicode=yes&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=GMT%2B8&amp;useLegacyDatetimeCode=false driver-class-name: com.mysql.jdbc.Driver username: root password: root 1234567891011//prefix 指定前缀@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)public class MyDataSourceProperties &#123; private String type; private String driverClassName; private String url; private String username; private String password; //省略getter setter方法&#125; 前缀定义了哪些外部属性将绑定到类的字段上 根据 Spring Boot 宽松的绑定规则，类的属性名称必须与外部属性的名称匹配 我们可以简单地用一个值初始化一个字段来定义一个默认值 类本身可以是包私有的 类的字段必须有公共 setter 方法 Environment Spring Environment bean 123456789101112@RestController@RequestMapping(&quot;/test&quot;)public class TestC &#123; @Autowired private Environment env; @RequestMapping(value = &quot;index&quot;, method = RequestMethod.GET) public String index() &#123; return &quot;environment : &quot;+ env.getProperty(&quot;spring.datasource.druid.url&quot;); &#125;&#125;","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"springboot","slug":"springboot","permalink":"https://xiaoyuge5201.github.io/tags/springboot/"}]},{"title":"Hexo部署出现错误Error Spawn failed解决方式","slug":"hexo-spawn-failed","date":"2021-08-15T10:28:42.000Z","updated":"2021-08-15T10:28:42.000Z","comments":true,"path":"hexo-spawn-failed/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/hexo-spawn-failed/index.html","excerpt":"","text":"部署过程中可能会出现错误: 123456789101112remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.remote: Please see https://github.blog/2020-12-15-token-authentication-requirements-for-git-operations/ for more information.fatal: unable to access &#x27;https://github.com/xiaoyuge5201/xiaoyuge5201.github.io.git/&#x27;: The requested URL returned error: 403FATAL &#123; err: Error: Spawn failed at ChildProcess.&lt;anonymous&gt; (/Users/xiaoyuge/workspace/mybolg/node_modules/hexo-util/lib/spawn.js:51:21) at ChildProcess.emit (events.js:315:20) at Process.ChildProcess._handle.onexit (internal/child_process.js:277:12) &#123; code: 128 &#125;&#125; Something&#x27;s wrong. Maybe you can find the solution here: %s https://hexo.io/docs/troubleshooting.htmlxiaoyuge@xiaoyugedeMacBook-Pro mybolg % hexo clean ####解决方式一： 1234567891011##进入站点根目录cd cd /Users/xiaoyuge/workspace/mybolg##删除git提交内容文件夹rm -rf .deploy_git/##执行git config --global core.autocrlf false##最后hexo clean &amp;&amp; hexo g &amp;&amp; hexo d ####解决方式二： 有可能是你的git repo配置地址不正确,可以将http方式变更为ssh方式（我的就是这个问题） github在2021-08-13正式启用personal access token后，原来的用户名+密码方式部署会报错，需要采用最新的token登录方式进行部署 。 具体方式参考：https://cloud.tencent.com/developer/article/1861466 1remote: Support for password authentication was removed on August 13, 2021. Please use a personal access token instead. 查看_config.yml文件， 12345deploy: type: git #repo:https://github.com/xiaoyuge5201/xiaoyuge5201.github.io.git 这是原来的路径，现在改成了下面这种 repo: git@github.com:xiaoyuge5201/xiaoyuge5201.github.io.git branch: master","categories":[{"name":"11 其他工具","slug":"11-其他工具","permalink":"https://xiaoyuge5201.github.io/categories/11-%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://xiaoyuge5201.github.io/tags/hexo/"}]},{"title":"Java守护线程和非守护线程","slug":"thread-01","date":"2021-08-15T06:14:56.000Z","updated":"2021-08-15T06:14:56.000Z","comments":true,"path":"thread-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/thread-01/index.html","excerpt":"","text":"用户线程：我们平常创建的普通线程。 守护线程：用来服务于用户线程；不需要上层逻辑介入 java线程分为守护线程和非守护线程，当java jvm检测主线程或其他子线程执行完之后，守护线程也会马上停止执行，我们可以使用Thread.setDaemon(ture或false)来设置一个线程是守护线程还是非守护线程，默认为false，可以通过Thread.isDaemon()方法查询该线程是否是守护线程 守护线程是所有的用户线程结束生命周期，守护线程才会结束生命周期，只要有一个用户线程存在，那么守护线程就不会结束，例如Java中的垃圾 回收器就是一个守护线程，只有应用程序中所有的线程结束，它才会结束。 123456789101112131415161718192021public class DaemonThread &#123; public static void main(String[] args) &#123; Thread thread = new Thread(DaemonThread::print); thread.setDaemon(true); thread.start(); System.out.println(&quot;主线程main 结束&quot;); &#125; public static void print() &#123; int counter = 1; //写一个死循环的方法来测试 while (true) &#123; try &#123; System.out.println(&quot;Counter:&quot; + counter++); Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 输出： 12主线程main 结束Counter:1 如果我们将daemon设置为非守护线程，代码如下: 1thread.setDaemon(false); 这个时候就不会退出while(true)循环了，会一直执行下去，结果如下： 1234567主线程main 结束Counter:1Counter:2Counter:3Counter:4Counter:5.... 总结：守护线程是为用户线程服务的，当用户线程全部结束，守护线程会自动结束。 注意事项： thread.setDaemon(true)必须在thread.start()之前设置，否则会跑出一个IllegalThreadStateException异常。你不能把正在运行的常规线程设置为守护线程。 在Daemon线程中产生的新线程也是Daemon的。 守护线程不能用于去访问固有资源，比如读写操作或者计算逻辑。因为它会在任何时候甚至在一个操作的中间发生中断。 Java自带的多线程框架，比如ExecutorService，会将守护线程转换为用户线程，所以如果要使用后台线程就不能用Java的线程池。 意义以及应用场景: 当主线程结束时，结束其余的子线程（守护线程）自动关闭，就免去了还要继续关闭子线程的麻烦。如：Java垃圾回收线程就是一个典型的守护线程；内存资源或者线程的管理，但是非守护线程也可以。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"守护线程","slug":"守护线程","permalink":"https://xiaoyuge5201.github.io/tags/%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B/"}]},{"title":"redis常见使用场景","slug":"redis-usage-scenario","date":"2021-08-14T09:51:30.000Z","updated":"2021-08-14T09:51:30.000Z","comments":true,"path":"redis-usage-scenario/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis-usage-scenario/index.html","excerpt":"","text":"1. 缓存 String类型 例如：热点数据缓存、对象缓存、全页缓存可以提升热点数据的访问效率 2. 数据共享分布式 String类型，因为redis是分布式的独立服务，可以在多个应用服务之间共享，例如分布式session 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.session&lt;/groupId&gt; &lt;artifactId&gt;spring-session-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; 3. 分布式锁 String类型 setnx方法，只有不存在时才能添加成功返回true 1234567891011public static boolean getLock(String key) &#123; Long flag = jedis.setnx(key, &quot;1&quot;); if (flag == 1) &#123; jedis.expire(key, 10); &#125; return flag == 1;&#125;public static void releaseLock(String key) &#123; jedis.del(key);&#125; 4. 全局ID int 类型， incrby, 利用原子性 1incrby userid 1000 分库分表的场景，一次性拿一段。 5. 计数器 int 类型，incr方法 例如：文章的阅读量、微博点赞数；允许一定的延迟，先写入redis在定时同步到数据库 第一种方法 12345678910111213141516171819202122232425@Servicepublic class TestService &#123; @Resource RedisTemplate&lt;String,Object&gt; redisTemplate; @Resource(name=&quot;redisTemplate&quot;) private ValueOperations&lt;String,Object&gt; ops; public int testRedis() &#123; try &#123; //此方法会先检查key是否存在，存在+1，不存在先初始化，再+1 ops.increment(&quot;success&quot;, 1); //return (int) ops.get(&quot;success&quot;); //使用这个会出现错误，报错信息 Caused by: org.springframework.core.serializer.support.SerializationFailedException: Failed to deserialize payload. Is the byte array a result of corresponding serialization for DefaultDeserializer?; nested exception is java.io.EOFException。 return Integer.valueOf(redisTemplate.boundValueOps(&quot;success&quot;).get(0, -1)); &#125; catch (Exception e) &#123; // TODO: handle exception e.printStackTrace(); &#125; return 0 ; &#125; &#125; 第二种方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Servicepublic class TestService &#123; @Resource RedisTemplate&lt;String,Object&gt; redisTemplate; @Resource(name=&quot;redisTemplate&quot;) private ValueOperations&lt;String,Object&gt; ops; public int testRedis() &#123; try &#123; //此方法会先检查key是否存在，存在+1，不存在先初始化，再+1 ops.increment(&quot;success&quot;, 1); //return (int) ops.get(&quot;success&quot;); //return Integer.valueOf(redisTemplate.boundValueOps(&quot;success&quot;).get(0, -1)); return (int) getKey(&quot;success&quot;); &#125; catch (Exception e) &#123; // TODO: handle exception e.printStackTrace(); &#125; return 0 ; &#125; public long getKey(final String key) &#123; return redisTemplate.execute(new RedisCallback&lt;Long&gt;() &#123; @Override public Long doInRedis(RedisConnection connection) throws DataAccessException &#123; RedisSerializer&lt;String&gt; redisSerializer = redisTemplate.getStringSerializer(); byte[] rowkey = redisSerializer.serialize(key); byte[] rowval = connection.get(rowkey); try &#123; String val = redisSerializer.deserialize(rowval); return Long.parseLong(val); &#125; catch (Exception e) &#123; return 0L; &#125; &#125; &#125;); &#125; &#125; 设置每天零点过期，重新计数 1234567//当天时间Date date = new Date();//当天零点date = DateUtils.truncate(date, Calendar.DAY_OF_MONTH);//第二天零点date = DateUtils.addDays(date, +1);redisTemplate.expireAt(&quot;success&quot;, date); 6. 限流 int类型，incr方法 以访问者的IP和其他信息作为key,访问一次增加一次次数，超过次数 则返回false 7. 位统计 String类型的bitcount 字符是以8位二进制存储的 1234567891011set k1 asetbit k1 6 1setbit k1 7 0get k1 /* 6 7 代表的a的二进制位的修改a 对应的ASCII码是97，转换为二进制数据是01100001b 对应的ASCII码是98，转换为二进制数据是01100010因为bit非常节省空间（1 MB=8388608 bit），可以用来做大数据量的统计。*/ 例如：在线用户统计，留存用户统计 123setbit onlineusers 01 setbit onlineusers 11 setbit onlineusers 20 支持按位与、按位或等等操作 1234BITOPANDdestkeykey[key...] ，对一个或多个 key 求逻辑并，并将结果保存到 destkey 。 BITOPORdestkeykey[key...] ，对一个或多个 key 求逻辑或，并将结果保存到 destkey 。 BITOPXORdestkeykey[key...] ，对一个或多个 key 求逻辑异或，并将结果保存到 destkey 。 BITOPNOTdestkeykey ，对给定 key 求逻辑非，并将结果保存到 destkey 。 计算出7天都在线的用户 1BITOP &quot;AND&quot; &quot;7_days_both_online_users&quot; &quot;day_1_online_users&quot; &quot;day_2_online_users&quot; ... &quot;day_7_online_users&quot; 8. 购物车 String 或hash。所有String可以做的hash都可以 hash类型是一个String类型的field和value的映射表，每个hash都可以存储2^32 -1键值对 使用hash做购物车：以用户id为key, 商品id为field，商品数量为value。 9. 用户消息时间线timeline list，双向链表，直接作为timeline就好了。插入有序 10. 消息队列 List提供了两个阻塞的弹出操作：blpop/brpop，可以设置超时时间 blpop：blpop key1 timeout 移除并获取列表的第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 brpop：brpop key1 timeout 移除并获取列表的最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出元素为止。 上面的操作。其实就是java的阻塞队列。学习的东西越多。学习成本越低 队列：先进先除：rpush blpop，左头右尾，右边进入队列，左边出队列 栈：先进后出：rpush brpop 11. 抽奖 自带一个随机获得值 1spop myset 12. 点赞、签到、打卡 假如上面的微博ID是t1001，用户ID是u3001 用 like:t1001 来维护 t1001 这条微博的所有点赞用户 点赞了这条微博：sadd like:t1001 u3001 取消点赞：srem like:t1001 u3001 是否点赞：sismember like:t1001 u3001 点赞的所有用户：smembers like:t1001 点赞数：scard like:t1001 13. 商品标签 用 tags:i5001 来维护商品所有的标签。 sadd tags:i5001 画面清晰细腻 sadd tags:i5001 真彩清晰显示屏 sadd tags:i5001 流程至极 14.商品筛选 123456// 获取差集sdiff set1 set2// 获取交集（intersection ）sinter set1 set2// 获取并集sunion set1 set2 1234567sadd brand:apple iPhone11sadd brand:ios iPhone11sad screensize:6.0-6.24 iPhone11sad screentype:lcd iPhone 11 筛选商品，苹果的、ios的、屏幕在6.0-6.24之间的，屏幕材质是LCD屏幕 1sinter brand:apple brand:ios screensize:6.0-6.24 screentype:lcd 15. 用户关注、推荐模型 12345## follow 关注 fans 粉丝sadd 1:follow 2sadd 2:fans 1sadd 1:fans 2sadd 2:follow 1 我关注的人也关注了他(取交集)： 1sinter 1:follow 2:fans 可能认识的人： 12345## 用户1可能认识的人(差集)：sdiff 2:follow 1:follow## 用户2可能认识的人：sdiff 1:follow 2:follow 16. 排行榜 id 为6001 的新闻点击数加1：zincrby hotNews:20190926 1 n6001 获取今天点击最多的15条：zrevrange hotNews:20190926 0 15 withscores redis不适用的场景 Redis是一种缓存技术，主要用来提高应用的性能，更多的应用场景是对数据库读数据进行缓存，减轻数据库的IO的访问压力，以下场景不太适合使用Redis: 数据规模大小角度 Redis是将数据放在内存进行缓存的，内存相对于磁盘来锁价格是比较贵的。如果成本是需要考虑的重要因素，那么大规模的数据就不太适合； 数据冷热程度角度 很多业务数据可以根据数据读的频繁程度分为热数据和冷数据；频繁使用的热数据一般适合用redis，冷数据一般不太适合用redis,如果大量的冷数据进行了缓存，那是对内存资源的浪费， 所以在应用场景上区分冷热数据，将热数据放在内存中，进而提高性能。","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"MVCC多版本并发控制","slug":"mvcc","date":"2021-08-14T03:03:40.000Z","updated":"2021-08-14T03:03:40.000Z","comments":true,"path":"mvcc/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mvcc/index.html","excerpt":"","text":"1. MVCC 全称Multi-Version Concurrency Control即多版本并发控制。MVCC是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中是心啊事务内存。 MVCC在mysql Innodb中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使读写冲突时，也能做到不加锁，非阻塞并发读 2. 当前读和快照读 当前读 像select lock in share mode（共享锁），select for update， update, insert,delete(排他锁)这些操作都是一种当前读；当前读就是读取记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。 快照读 像不加锁的select操作就是快照读，即不加锁的非阻塞读；快照读的前提是隔离级别不是串行级别（串行级别快照读会变成当前读）；快照读的实现是基于多版本并发控制（即MVCC）；可以任务MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销； 既然是基于多版本，即快照读可能读到的并不一定是最新版本的数据，有可能是之前的历史版本 MVCC就是为了实现读（快照读）-写冲突不加锁，当前读实际上是一种加锁的操作，是悲观锁的实现。 3. 当前读、快照读和MVCC的关系 MVCC多版本并发控制指的是&quot;维持一个数据的多个版本，使得读写操作没有冲突&quot;； Mysql通过快照读的方式去实现MVCC理想模型的其中一个具体非阻塞读功能，相对而言，当前读就是悲观锁的具体功能实现 MVCC模型在Mysql中具体实现有3个隐式字段：undo日志、Read View等去完成的 4. MVCC的作用与好处 数据库并发场景分为以下三种： 读-读：没有问题，也不需要并发控制 读-写：有线程安全问题，可能会造成事务隔离性问题，（脏读、幻读、不可重复读） 写-写：有线程安全问题，可能会存在更新丢失问题 MVCC带来的好处： MVCC是一种用来解决读-写冲突的无所并发控制（在MVCC提出之前采用的是采用悲观锁），也就是事务分配增长的时间戳，为每个修改保存一个版本，版本与事务时间戳关联，读操作只读该事务前的数据库快照，主要解决以下问题： 在并发读写数据库时，可以做到在读操作是不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能； 解决脏读、幻读、不可重复读等事务隔离性问题，但不能解决更新丢失问题 MVCC组合方法 MVCC + 悲观锁： MVCC解决读写冲突，悲观锁解决写写冲突 MVCC + 乐观锁： MVCC解决读写冲突，乐观锁解决写写冲突，这种方式可能最大程度的提高数据库并发性能，并解决读写冲突和写写冲突导致的问题 5. MVCC的实现原理 实现原理主要是依赖记录中的 3个隐式字段、undo日志 、ReadView 来实现的 在Mysql的InnoDB引擎中就是指在已提交读(READ COMMITTD)和可重复读(REPEATABLE READ)这两种隔离级别下的事务对于SELECT操作会访问版本链中的记录的过程 这就使得别的事务可以修改这条记录，反正每次修改都会在版本链中记录。SELECT可以去版本链中拿记录，这就实现了读-写，写-读的并发执行，提升了系统的性能。 5.1 版本链 123456begin;#触发分配TRX_IDselect * from t_role;#指定TRX_MYSQL_THREAD_ID=当前CONNECTION_ID,表示查询当前连接select TRX_ID, ROLL_PTR, ROW_ID from INFORMATION_SCHEMA.INNODB_TRX where TRX_MYSQL_THREAD_ID = CONNECTION_ID();commit; 在InnoDB引擎表中，它的聚簇索引记录中有两个必要的隐藏列： TRX_ID 6byte，这个id用来存储的每次对某条聚簇索引记录进行修改的时候的事务id roll_pointer 每次对哪条聚簇索引记录有修改的时候，都会把老版本写入undo日志中。这个roll_pointer就是存了一个指针，它指向这条聚簇索引记录的上一个版本的位置，通过它来获得上一个版本的记录信息。(注意插入操作的undo日志没有这个属性，因为它没有老版本) ROW_ID 6byte，隐含的自增ID（隐藏主键），如果数据表没有主键，InnoDB会自动以DB_ROW_ID产生一个聚簇索引 实际还有一个删除flag隐藏字段, 既记录被更新或删除并不代表真的删除，而是删除flag变了 比如现在有个事务id是60的执行的这条记录的修改语句 此时在undo日志中就存在版本链 5.2 ReadView 已提交读和可重复读的区别就在于它们生成ReadView的策略不同 ReadView中主要就是有个列表来存储我们系统中当前活跃着的读写事务，也就是begin了还未提交的事务。通过这个列表来判断记录的某个版本是否对当前事务可见。假设当前列表里的事务id为[80,100]。 如果你要访问的记录版本的事务id为50，比当前列表最小的id80小，那说明这个事务在之前就提交了，所以对当前活动的事务来说是可访问的。 如果你要访问的记录版本的事务id为90,发现此事务在列表id最大值和最小值之间，那就再判断一下是否在列表内，如果在那就说明此事务还未提交，所以版本不能被访问。如果不在那说明事务已经提交，所以版本可以被访问。 如果你要访问的记录版本的事务id为110，那比事务列表最大id100都大，那说明这个版本是在ReadView生成之后才发生的，所以不能被访问。 这些记录都是去版本链里面找的，先找最近记录，如果最近这一条记录事务id不符合条件，不可见的话，再去找上一个版本再比较当前事务的id和这个版本事务id看能不能访问，以此类推直到返回可见的版本或者结束。","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"volatile关键字","slug":"volatile","date":"2021-08-02T10:20:38.000Z","updated":"2021-08-02T10:20:38.000Z","comments":true,"path":"volatile/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/volatile/index.html","excerpt":"","text":"1.volatile作用 volatile保证有序性，可见性，不能保证原子性 禁止指令重排 保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量值，这个新值对其他线程立即可见的 不能保证原子性(线程不安全) 2. 实现原理 通过插入内存屏障指令禁止编译器和CPU对程序进行重排序。 当对声明了volatile的变量进行写操作时，JVM就会向处理器发送一条Lock前缀的指令，这条Lock前缀指令产生如下两个作用： Lock前缀指令会引起处理器缓存回写到系统内存，并使用缓存一致性机制来确保回写的原子性。 一个处理器的缓存回写到系统内存会导致其他处理器的缓存无效。处理器使用MESI控制协议去维护内部缓存和其他处理器缓存的一致性。处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。例如，在Pentium和P6 family处理器中，如果通过嗅探一个处理器来检测其他处理器打算写内存地址，而这个地址当前处于共享状态，那么正在嗅探的处理器将使它的缓存行无效，在下次访问相同内存地址时，强制执行缓存行填充 3.synchronized与volatile比较 volatile关键字是线程同步的轻量级实现，性能较synchronized好；但是volatile关键字只能用于变量，而synchronized关键字可以修饰方法以及代码块 synchronized关键字在java1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其他各种优化之后执行效率有了显著的提升； 多线程访问volatile关键字不会发生阻塞，而synchronized关键字可能会发生阻塞 volatile关键字保证数据的可见性，但是不能保证数据的原子性；synchronized关键字两者都能保证（synchronized保证原子性，有序性，可见性） volatile关键字主要用于解决变量在多个线程之间的可见性，而synchronized关键字解决的是多个线程之间访问资源的同步性。 synchronized是同步锁，同步快内的代码相当于同一时刻单线程执行 4. 可见性问题 Java虚拟机规范中定义了一种Java内存 模型（Java Memory Model，即JMM）来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的并发效果。Java内存模型的主要目标就是定义程序中各个变量的访问规则，即在虚拟机中将变量存储到内存和从内存中取出变量这样的细节。 JMM中规定所有的变量都存储在主内存（Main Memory）中，每条线程都有自己的工作内存（Work Memory），线程的工作内存中保存了该线程所使用的变量的从主内存中拷贝的副本。线程对于变量的读、写都必须在工作内存中进行，而不能直接读、写主内存中的变量。同时，本线程的工作内存的变量也无法被其他线程直接访问，必须通过主内存完成 整体内存模型如下： 4.1 synchronized synchronized关键字的语义JMM（Java Main Memory）有两个规定，保证其实现内存可见性： - 线程解锁前，必须把共享变量的最新值刷新到主内存中 - 线程加锁前，将清空工作内存中共享变量的值，从主内存中重新取值 4.2 volatile 当对volatile变量执行写操作后，JMM会把工作内存中的最新变量值强制刷新到主内存 写操作会导致其他线程中的缓存无效 这样，其他线程使用缓存时，发现本地工作内存中此变量无效，便从主内存中获取，这样获取到的变量便是最新的值，实现了线程的可见性。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"}]},{"title":"Java内存泄漏排查","slug":"outOfMemoryError","date":"2021-08-01T11:25:57.000Z","updated":"2021-08-01T11:25:57.000Z","comments":true,"path":"outOfMemoryError/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/outOfMemoryError/index.html","excerpt":"","text":"1.内存溢出 java.lang.OutOfMemoryError：是指程序在申请内存是，没有足够的内存空间供其使用，出现OutOfMemoryError 产生原因 JMM内存过小 程序不严谨，产生了过多的垃圾 具体表现在以下集中情况 内存中加载的数据量过于庞大，如一次从数据库取出过多的数据 集合类中有对对象的引用，使用完后未清空，使得JVM不能回收 代码中存在死循环或循环过多产生过多重复的对象实体 使用第三方软件的bug 启动参数内存值设定过小 常见错误提示 tomcat:java.lang.OutOfMemoryError: PermGen space tomcat:java.lang.OutOfMemoryError: Java heap space weblogic:Root cause of ServletException java.lang.OutOfMemoryError resin:java.lang.OutOfMemoryError java:java.lang.OutOfMemoryError 解决方法 增加JVM的内存大小 对于tomcat容器，找到tomcat在电脑中的安装目录，进入这个目录，然后进入bin目录中，在window环境下找到bin目录中的catalina.bat，在linux环境下找到catalina.sh。 编辑catalina.bat文件，找到JAVA_OPTS（具体来说是 set “JAVA_OPTS=%JAVA_OPTS% %LOGGING_MANAGER%”）这个选项的位置，这个参数是Java启动的时候，需要的启动参数。 也可以在操作系统的环境变量中对JAVA_OPTS进行设置，因为tomcat在启动的时候，也会读取操作系统中的环境变量的值，进行加载。 如果是修改了操作系统的环境变量，需要重启机器，再重启tomcat，如果修改的是tomcat配置文件，需要将配置文件保存，然后重启tomcat，设置就能生效了 优化程序，释放垃圾 主要思路就是避免程序体现上出现的情况。避免死循环，防止一次载入太多的数据，提高程序健壮型及时释放。因此，从根本上解决Java内存溢出的唯一方法就是修改程序，及时地释放没用的对象，释放内存空间 2. 内存泄漏 Memory Leak，是指程序在申请内存后，无法释放已申请的内存空间，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存，迟早会被占光。 在Java中，内存泄漏就是存在一些被分配的对象，这些对象有下面两个特点： 1）首先，这些对象是可达的，即在有向图中，存在通路可以与其相连； 2）其次，这些对象是无用的，即程序以后不会再使用这些对象。 如果对象满足这两个条件，这些对象就可以判定为Java中的内存泄漏，这些对象不会被GC所回收，然而它却占用内存。 关于内存泄露的处理页就是提高程序的健壮型，因为内存泄露是纯代码层面的问题 3.内存溢出和内存泄漏的联系 内存泄露会最终会导致内存溢出。 相同点：都会导致应用程序运行出现问题，性能下降或挂起。 不同点： 1) 内存泄露是导致内存溢出的原因之一，内存泄露积累起来将导致内存溢出。 2) 内存泄露可以通过完善代码来避免，内存溢出可以通过调整配置来减少发生频率，但无法彻底避免。 4.排查案例 Java的内存泄露多半是因为对象存在无效的引用，对象得不到释放，如果发现Java应用程序占用的内存出现了泄露的迹象，那么我们一般采用下面的步骤分析： 用工具生成java应用程序的heap dump（如jmap） 使用Java heap分析工具（如MAT），找出内存占用超出预期的嫌疑对象 根据情况，分析嫌疑对象和其他对象的引用关系。 分析程序的源代码，找出嫌疑对象数量过多的原因。 实际操作如下： 1.登录linux服务器，获取tomcat的pid 1ps -ef|grep java 2.利用jmap初步分析内存映射 1jmap -histo:live pid | head -7 第2行是我们业务系统的对象，通过这个对象的引用可以初步分析出到底是哪里出现了引用未被垃圾回收收集，通知开发人员优化相关代码 3.如果上面一步还无法定位到关键信息，那么需要拿到heap dump，生成离线文件，做进一步分析 1jmap -dump:live,format=b,file=heap.hprof 3514 4. 拿到heap dump文件，利用eclipse插件MAT来分析heap profile。 1.安装MAT插件 2.在eclipse里切换到Memory Analysis视图 3.用MAT打开heap profile文件。 直接看到下面Action窗口，有4种Action来分析heap profile，介绍其中最常用的2种: Histogram：这个使用的最多，跟上面的jmap -histo 命令类似，只是在MAT里面可以用GUI来展示应用系统各个类产生的实例。 Shllow Heap排序后发现 Cms_Organization 这个类占用的内存比较多（没有得到及时GC），查看引用 分析引用栈，找到无效引用，打开源码 查看源码！！！","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"内存溢出","slug":"内存溢出","permalink":"https://xiaoyuge5201.github.io/tags/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/"}]},{"title":"JAVA类加载过程","slug":"classLoad","date":"2021-07-31T03:04:02.000Z","updated":"2021-07-31T03:04:02.000Z","comments":true,"path":"classLoad/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/classLoad/index.html","excerpt":"","text":"1.类加载机制 JVM将类描述数据从.class文件中加载到内存，并对数据进行解析、初始化，最终形成被JVM直接使用的Java类型；类从被加载到JVM中开始，到卸载为止， 整个生命周期包括：加载、验证、准备、解析、初始化、使用和卸载7个阶段 加载 加载时jvm做了三件事 1)通过一个类的全限定名获取该类的二进制流 2)将这个字节流的静态存储结构转化为方法区运行时数据结构 3)在内存堆中生成一个代表该类的java.lang.class对象，最为该类数据的访问入口 验证 验证、准备、解析这三步可以看作是一个连接的过程，将类的字节码连接到JVM的运行黄台之中; 验证是为了确保class文件的字节流中包含的信息符合当前虚拟机的要求，不会威胁到jvm的安全。 验证内容如下： 文件格式的验证： 验证字节流是否符合class文件袋额规范，是否能被当前版本的虚拟机处理 元数据验证： 对字节码描述的信息进行语义分析，确保符合Java语言规范 字节码验证：通过数据流和控制流分析，确定寓意是否合法，符合逻辑的 符号引用验证：这个娇艳在解析阶段发生 准备 为类的静态变量分配内存，设置初始值，对于final static修饰的变量，直接赋值为用户的定义值。 12//准备阶段过后的初始值为0， 而不是7 public static int a = 7; 解析 解析是将常量池内的符号引用转化为直接引用（如物理内存地址指针） 初始化 初始化阶段，jvm才开始真正执行类中定义的Java代码 执行类构造器()方法的过程，类构造器方法是有编译器自动手机类中所有类变量的赋值动作和静态语句块（static块）中的语句合并产生的 当初始化一个类的时候，如果发现其父类还没有进行过初始化，需有限触发其父类的初始化 虚拟机会保证一个类的()方法在多线程环境被正确加锁和同步 2.类加载器 类加载器的主要任务：对类加载过程中的加载操作（根据一个类的全限定名读取该类的二进制字节流到JVM内部，然后转换为一个对应的java.lang.Class对象实例） 类加载器的分类 启动类加载器Bootstrap ClassLoader: 在HotSpot虚拟机中，Bootstrap ClassLoader用C++语言编写并嵌入JVM内部，主要负载加载JAVA_HOME/lib目录中的所有类，或者加载由选项-Xbootcalsspath指定的路径下的类 拓展类加载器/ExtClassLoader： ExtClassLoader继承ClassLoader类，负载加载JAVA_HOME/lib/ext目录中的所有类型，或者由参数-Xbootclasspath指定路径中的所有类型 应用程序类加载器/AppClassLoader: ExtClassLoader继承ClassLoader类，负责加载用户类路径ClassPath下的所有类型，一般情况下为程序的默认类加载器 自定义加载器: Java虚拟机规范将所有继承抽象类java.lang.ClassLoader的类加载器，定义为自定义类加载器 3. 双亲委派模型 如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式 。 除了启动类加载器以外，每个类加载器拥有一个父类加载器，用户的自定义类加载器的父类加载器是AppClassLoader； 双亲委派模型可以保证全限名指定的类，只被加载一次； 双亲委派模型不具有强制性约束，是Java设计者推荐的类加载器实现方式； 3.1 双亲委派模式优势 采用双亲委派模式的是好处是Java类随着它的类加载器一起具备了一种带有优先级的层次关系，通过这种层级关可以避免类的重复加载，当父亲已经加载了该类时，就没有必要子ClassLoader再加载一次 java核心api中定义类型不会被随意替换，假设通过网络传递一个名为java.lang.Integer的类，通过双亲委托模式传递到启动类加载器，而启动类加载器在核心Java API发现这个名字的类，发现该类已被加载，并不会重新加载网络传递的过来的java.lang.Integer，而直接返回已加载过的Integer.class，这样便可以防止核心API库被随意篡改 如果我们在classpath路径下自定义一个名为java.lang.SingleInterge类(该类是胡编的)呢？该类并不存在java.lang中，经过双亲委托模式，传递到启动类加载器中，由于父类加载器路径下并没有该类，所以不会加载，将反向委托给子类加载器加载，最终会通过系统类加载器加载该类。但是这样做是不允许，因为java.lang是核心API包，需要访问权限，强制加载将会报出如下异常 3.2 双庆委派模型实现源码 可以打开 java.lang.ClassLoader 类，其 loadClass方法如下： 12345678910111213141516171819202122232425262728293031323334353637protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class&lt;?&gt; c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 实现方式很简单，首先会检查该类是否已经被加载过了，若加载过了直接返回（默认resolve取false）；若没有被加载，则调用父类加载器的 loadClass方法，若父类加载器为空则默认使用启动类加载器作为父加载器。如果父类加载失败，则在抛出 ClassNotFoundException 异常后，在调用自己的 findClass 方法进行加载 4.自定义类加载器 加密 我们知道Java字节码是可以进行反编译的，在某些安全性高的场景，是不允许这种情况发生的。那么我们可以将编译后的代码用某种加密算法进行加密，加密后的文件就不能再用常规的类加载器去加载类了。而我们自己可以自定义类加载器在加载的时候先解密，然后在加载 动态创建 比如很有名的动态代理。 从非标准的来源加载代码 我们不用非要从class文件中获取定义此类的二进制流，还可以从数据库，从网络中，或者从zip包等。 4.1 自定义类加载器方法 类加载时根据双亲委派模型会先一层层找到父加载器，如果加载失败，则会调用当前加载器的 findClass() 方法来完成加载。因此我们自定义类加载器，有两个步骤： 1、继承 ClassLoader 2、覆写 findClass() 方法","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"ClassLoader","slug":"ClassLoader","permalink":"https://xiaoyuge5201.github.io/tags/ClassLoader/"}]},{"title":"Hexo添加评论系统Valine","slug":"valine","date":"2021-07-27T13:49:57.000Z","updated":"2021-07-27T13:49:57.000Z","comments":true,"path":"valine/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/valine/index.html","excerpt":"","text":"Hexo的评论系统有很多，常见的有以下几个 多说 网易云跟帖 畅言 来必力（LiveRe） Disqus Hypercomments valine 首先多说和网易云已经倒下了，其次畅言需要备案，Disqus，Hypercomments和LiveRe都是国外的，加载速度贼慢，甚至有被墙的可能，寻觅了很久之后，从Material主题换成next主题之后，终于找到了一个好用的评论系统，那就是 valine 我使用的是Next 6.x版本，本身就已经集成了valine，因此正常情况下是按照官方文档走就可以了，5分钟开启你的评论系统~ 注册LeanCloud 我们的评论系统其实是放在Leancloud上的，因此首先需要去注册一个账号，注册地址：https://www.leancloud.cn/ 获取AppId 注册完了之后 创建一个应用，名字随便起；然后进入应用-&gt;设置-&gt;应用凭证 中获取appid 和 appkey 配置valine 拿到appid和appkey之后，打开主题配置文件_config.yml 搜索 valine，填入appid 和 appkey 123456789101112comment: type: valine # 启用哪种评论系统 valine: # Valine. https://valine.js.org appid: your leancloud application appid appkey: your leancloud application appkey notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: Just go go # comment box placeholder avatar: mm # gravatar style meta: nick,mail #,link # custom comment header pageSize: 10 # pagination size visitor: true # Article reading statistic https://valine.js.org/visitor.html LeanCloud 安全域名配置 在LeanCloud -&gt; 设置 -&gt; 安全中心 -&gt; Web 安全域名 把你的域名加进去 查看评论数据 在数据存储 -&gt; 结构化数据 中可以查看到所有的存储的数据信息","categories":[{"name":"11 其他工具","slug":"11-其他工具","permalink":"https://xiaoyuge5201.github.io/categories/11-%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://xiaoyuge5201.github.io/tags/hexo/"}]},{"title":"mysql数据库锁","slug":"mysql数据库锁","date":"2021-07-24T08:57:10.000Z","updated":"2021-07-24T08:57:10.000Z","comments":true,"path":"mysql数据库锁/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E9%94%81/index.html","excerpt":"","text":"当数据库有事务的时候，可能会产生数据的不一致，这时就需要一些机制来保证访问的次序，这就是锁的机制； 锁的作用：用于挂你对共享资源的并发访问，保证数据库的完整性和一致性。 ##1. 不同引擎的锁以及锁分类 Mysql数据库中，InnoDB支持表、行级锁，而MyISAM支持表级锁 Mysql大致可以归纳为以下3种锁： 表级锁：开销小，加锁块，不会出现死锁，发生锁冲突的概率最高，并发度最低。 行级锁：开销大，加锁慢，会出现死锁，发生锁冲突的概率最低，并发度最高。 页面锁：开销和加锁的时间介于表级锁和行级锁之间，会出现死锁，锁粒度介于两者之间；并发度一般，一次锁定相邻一组记录 Mysql表级锁两种模式: 表共享锁（Table Read Lock）和 表独占写锁（Table Write Lock），表现如下： 对一个表的读操作，不会阻塞其他用户对同一表请求，但会阻塞对同一表的写请求； 对MyISAM的写操作，则会则色其他用户对同一表的读和写操作； MyISAM表的读操作和写操作之间，以及写操作之间是串行的。 当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作，其他线程的读、写操作都会等待。 2.加表级锁 MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁。 给MyISAM表显式加锁，一般是为了一定程度模拟事务操作，实现对某一时间点多个表的一致性读取。例如，有一个订单表orders，其中记录有订单的总金额total，同时还有一个订单明细表order_detail，其中记录有订单每一产品的金额小计subtotal，假设我们需要检查这两个表的金额合计是否相等，可能就需要执行如下两条SQL","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"mycat学习","slug":"mycat","date":"2021-07-23T08:16:49.000Z","updated":"2021-07-23T08:16:49.000Z","comments":true,"path":"mycat/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mycat/index.html","excerpt":"","text":"Mycat(分库分表中间件) 1. 数据库优化策略 重启：释放资源 SQL与索引 表与存储引擎（字段类型选择，长度设置，是否需要分表、分区） 数据库与应用架构（考虑使用缓存服务器，减轻是数据库压力；可以数据库分布式，读写分离，主从复制） 数据库与操作系统配置（修改mysql配置，使用单独服务器部署数据库） 硬件 2. 数据库演化 根据业务需要、数据量变化，随之而来的数据库的变化 数据库与应用部署在同一台服务器 单体应用架构，单数据库（数据库服务器和应用服务器分离，但是业务系统越做越大） 多应用单数据库（应用解耦） 多应用 独立数据库 但应用多数据库(分表) 3. 如何分库分表 垂直切分 单库 多库 水平切分 按照月分表或者分成实时、历史表等 分成多库 4. 分库分表带来的问题 跨库关联查询 增加冗余字段（违反了第三范式：表中的所有数据元素不但要能唯一地被主关键字所标识,而且它们之间还必须相互独立,不存在其他的函数关系） 跨数据库的同步（canal、Mq（最好）、ETL、kettle、ogg）(在某个库中同步其他数据库中表的数据，避免跨库关联查询) 全局表(广播表)：比如行政区划表，所有的系统都是一样的； API 分布式事务 Local 排序、翻页、函数计算 全局主键 雪花算法leaf redis(int 类型可以设置incby) ZookKeeper uuid(数据过长， 影响索引存储) 多数据源连接（动态数据源） 5. Mycat分库分表中间件 官网地址：http://www.mycat.org.cn/ 从阿里cobar升级而来，完全实现了mysql协议，可以当作一个mysql数据库来使用，通过JDBC支持其他数据库实现分库分表，解决了多表join、分布式事务、全局序列号、翻页查询、函数计算的问题 一个彻底开源的，面向企业应用开发的大数据库集群 支持事务、ACID、可以替代MySQL的加强版数据库 一个可以视为MySQL集群的企业级数据库，用来替代昂贵的Oracle集群 一个融合内存缓存技术、NoSQL技术、HDFS大数据的新型SQL Server 结合传统数据库和新型分布式数据仓库的新一代企业级数据库产品 一个新颖的数据库中间件产品 华为云的DDM其实也是根据mycat做的 5.1 核心概念 5.2 Mycat安装与配置 5.2.1 Mycat安装 从官网下载安装版本，解压到文件(官网建议安装在/usr/local/Mycat)后页面如下图所示： 目录解释如下： **bin：*存放window版和linux版本除了提供封装成服务的版本之外，也提供了nowrap的shell脚本命令，方便选择和修改；Linux下运行:./mycat console， 首先要chmod + x;(mycat支持的命令console、start、stop、restart、status、dump) **conf：**server.xml是mycat服务器参数调整和用户授权的配置文件，schema.xml是逻辑库定义和表以及分片定义的配置文件，rule.xml是分片规则的配置文件，分片规则的具体一些参数信息单独存放为文件，也在这个目录下，配置文件修改，需要重启mycat或者通过9066端口reload **lib:**主要存放mycat依赖的一些jar文件 logs:日志存放在mycat.log中，每天一个文件，日志的配置是在conf/log4j.xml中，根据自己的需要，可以调整输出级别为debug，方便排查问题；注意Linux下部署安装mysql，默认不忽略，需要手动到/etc/my.cnf下配置lower_case_table_names=1使Linux环境下MySQL忽略表明大小写，否则使用mycat的时候会提示找不到表的错误 5.2.2 服务启动与配置 Mycat在Linux中部署启动时，首先需要在Linux系统的环境变量中配置MYCAT_HOE,操作方式如下： vi /etc/profile，在系统环境变量文件中增加MYCAT_HOME=/usr/lib/tools/mycat 执行 source/etc/profile命令，使环境变量生效。如果是多台Linux系统中组件Mycat集群，那需要在mycat Server所在的服务器配置对其他ip和主机名的映射，配置方式如下： 经过以上两个步骤的配置，就可以到/usr/lib/tools/mycat/bin目录下执行./mycat start启动mycat服务；使用mycat status查看mycat的运行状态；如下图 5.2.2.1 安装遇到的问题 schema TESTDB refered by user root is not exist! 解决方式： 12345678&lt;!--在conf/server.xml文件中schemas中配置schema.xml文件中的schema的name值--&gt;&lt;!--user中的name为mycat服务的用户名--&gt; &lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;!--这个是mycat服务连接的密码--&gt; &lt;property name=&quot;password&quot;&gt;123456&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;xiaoyuge&lt;/property&gt;&lt;/user&gt; 5.2.3日志分析 mycat的日志文件配置为MYCAT_HOME/conf/log4j.xml 12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE log4j:configuration SYSTEM &quot;log4j.dtd&quot;&gt; &lt;log4j:configuration xmlns:log4j=&quot;http://jakarta.apache.org/log4j/&quot;&gt; &lt;appender name=&quot;ConsoleAppender&quot; class=&quot;org.apache.log4j.ConsoleAppender&quot;&gt; &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%d&#123;MM-dd HH:mm:ss.SSS&#125; %5p [%t] (%F:%L) -%m%n&quot; /&gt;true&lt;/layout&gt;true&lt;/appender&gt;true&lt;appender name=&quot;FILE&quot; class=&quot;org.apache.log4j.RollingFileAppender&quot;&gt; &lt;!--日志文件存放的目录--&gt; &lt;param name=&quot;file&quot; value=&quot;$&#123;MYCAT_HOME&#125;/logs/mycat.log&quot; /&gt; &lt;param name=&quot;Append&quot; value=&quot;false&quot;/&gt; &lt;param name=&quot;MaxFileSize&quot; value=&quot;10000KB&quot;/&gt; &lt;param name=&quot;MaxBackupIndex&quot; value=&quot;10&quot;/&gt; &lt;param name=&quot;encoding&quot; value=&quot;UTF-8&quot; /&gt; &lt;layout class=&quot;org.apache.log4j.PatternLayout&quot;&gt; &lt;param name=&quot;ConversionPattern&quot; value=&quot;%d&#123;MM/dd HH:mm:ss.SSS&#125; %5p [%t] (%F:%L) -%m%n&quot; /&gt;true&lt;/layout&gt;true&lt;/appender&gt;true&lt;root&gt; &lt;!--level是日志级别，生产环境下加以将级别调整为info/ware，如果是研究测试，碰到异常设置为debug--&gt;truetrue&lt;level value=&quot;debug&quot; /&gt;truetrue&lt;appender-ref ref=&quot;ConsoleAppender&quot; /&gt;true&lt;/root&gt;&lt;/log4j:configuration&gt; 5.2.3.1 warpper日志 目前mycat的启动时经过warpper封装成启动脚本，所以日志也会有其相关的日志文件：${MYCAT_HOME}/logs/warapper.log， 在启动的时候如果系统环境配置错误或缺少配置时，导致mycat无法启动，可以通过查看wrapper.log查看具体错误原因。 正常启动 1234567STATUS | wrapper | 2015/04/12 15:05:00 | --&gt; Wrapper Started as DaemonSTATUS | wrapper | 2015/04/12 15:05:00 | Launching a JVM...INFO | jvm 1 | 2015/04/12 15:05:01 | Wrapper (Version 3.2.3) http://wrapper.tanukisoftware.orgINFO | jvm 1 | 2015/04/12 15:05:01 | Copyright 1999-2006 Tanuki Software, Inc. AllRights Reserved.INFO | jvm 1 | 2015/04/12 15:05:01 |INFO | jvm 1 | 2015/04/12 15:05:01 | log4j 2015-04-12 15:05:01 [./conf/log4j.xml]load completed.INFO | jvm 1 | 2015/04/12 15:05:02 | MyCAT Server startup successfully. see logs in logs/mycat.log 启动异常 1234567891011STATUS | wrapper | 2015/02/14 01:43:44 | --&gt; Wrapper Started as DaemonSTATUS | wrapper | 2015/02/14 01:43:44 | Launching a JVM...INFO | jvm 1 | 2015/02/14 01:43:45 | Error: Exception thrown by the agent : java.rmi.server.ExportException:Port already in use: 1984; nested exception is:INFO | jvm 1 | 2015/02/14 01:43:45 | java.net.BindException: Address already in useERROR | wrapper | 2015/02/14 01:43:45 | JVM exited while loading the application. # 日志显示异常原因为 java.net.BindException: Address already in use,也就是端口占用，很有可能是原有服务未停止，或者 Mycat 默认端口被其他程序占用，正常启动成功后会有 mycat.log 日志，如果服务未启动成功不会有对应的日志。 也可以去修改 conf 文件夹里的 wrapper.conf 里的 wrapper.java.additional.7=-Dcom.sun.management.jmxremote.port=1984，server.xml 的&lt;property name=&quot;serverPort&quot;&gt;8066&lt;/property&gt;和&lt;property name=&quot;managerPort&quot;&gt;9066&lt;/property&gt;，这方法适合一台机器上两个 mycat 或者 1984,8066,9066 端口被其它应用占用的情况 5.2.3.2 mycat日志 5.2.4 mycat防火墙设置 白名单和SQL黑名单说明： 12345678910111213&lt;!--在 server.xml 中配置：--&gt;&lt;firewall&gt; &lt;!--ip 白名单列表，可以配置多个--&gt; &lt;whitehost&gt; &lt;!--ip 白名单 用户对应的可以访问的 ip 地址--&gt; &lt;host user=&quot;mycat&quot; host=&quot;127.0.0.1&quot;&gt;&lt;/host&gt; &lt;/whitehost&gt; &lt;!-是否开启检查黑名单列表--&gt; &lt;blacklist check=&quot;true&quot;&gt; &lt;!--黑名单允许的 权限 后面为默认--&gt; &lt;property name=&quot;selelctAllow&quot;&gt;false&lt;/property&gt; &lt;/blacklist&gt;&lt;/firewall&gt; 黑名单配置拦截明细如下： 配置项 缺省值 描述 rollbackAllow true 是否允许执行 roll back 操作,如果把 selectIntoAllow、deleteAllow、updateAllow、insertAllow、mergeAllow 都设置为 false，这就是一个只读数据源了。 selectAllow true 是否运行执行SELECT语句 selectAllColumnAllow true 是否允许执行 SELECT * FROM T 这样的语句。如果设置为 false，不允许执行 select * from t，但 select * from (select id, name from t) a。这个选项是防御程序通过调用 select *获得数据表的结构信息 selectIntoAllow true SELECT 查询中是否允许 INTO 字句 deleteAllow true 是否允许执行 DELETE 语句 updateAllow true 是否允许执行 UPDATE 语句 insertAllow true 是否允许执行 INSERT 语句 replaceAllow true 是否允许执行 REPLACE 语句 mergeAllow true 是否允许执行 MERGE 语句，这个只在 Oracle 中有用 callAllow true 是否允许通过 jdbc 的 call 语法调用存储过程 setAllow true 是否允许使用 SET 语法 truncateAllow true truncate 语句是危险，缺省打开，若需要自行关闭 createTableAllow true 是否允许创建表 alterTableAllow true 是否允许执行 Alter Table 语句 dropTableAllow true 是否允许修改表 commentAllow false 是否允许语句中存在注释，Oracle 的用户不用担心，Wall 能够识别 hints和注释的区别 noneBaseStatementAllow false 是否允许非以上基本语句的其他语句，缺省关闭，通过这个选项 就能够屏蔽 DDL。 multiStatementAllow false 是否允许一次执行多条语句，缺省关闭 useAllow true 是否允许执行 mysql 的 use 语句，缺省打开 describeAllow true 是否允许执行 mysql 的 describe 语句，缺省打开 showAllow true 是否允许执行 mysql 的 show 语句，缺省打开 commitAllow true 是否允许执行 commit 操作 拦截配置=-永真条件： 配置项 缺省值 描述 selectWhereAlwayTrueCheck true 检查 SELECT 语句的 WHERE 子句是否是一个永真条件 selectHavingAlwayTrueCheck true 检查 SELECT 语句的 HAVING 子句是否是一个永真条件 deleteWhereAlwayTrueCheck true 检查 DELETE 语句的 WHERE 子句是否是一个永真条件 deleteWhereNoneCheck false 检查 DELETE 语句是否无 where 条件，这是有风险的，但不是 SQL 注入类型的风险 updateWhereAlayTrueCheck true 检查 UPDATE 语句的 WHERE 子句是否是一个永真条件 updateWhereNoneCheck false 检查 UPDATE 语句是否无 where 条件，这是有风险的，但不是SQL 注入类型的风险 conditionAndAlwayTrueAllow false 检查查询条件(WHERE/HAVING 子句)中是否包含 AND 永真条件 conditionAndAlwayFalseAllow false 检查查询条件(WHERE/HAVING 子句)中是否包含 AND 永假条件 conditionLikeTrueAllow true 检查查询条件(WHERE/HAVING 子句)中是否包含 LIKE 永真条件 其他拦截配置： 配置项 缺省值 描述 selectIntoOutfileAllow false SELECT … INTO OUTFILE 是否允许，这个是 mysql 注入攻击的常见手段，缺省是禁止的 selectUnionCheck true 检测 SELECT UNION selectMinusCheck true 检测 SELECT MINUS selectExceptCheck true 检测 SELECT EXCEPT selectIntersectCheck true 检测 SELECT INTERSECT mustParameterized false 是否必须参数化，如果为 True，则不允许类似 WHERE ID = 1 这种不参数化的 SQL strictSyntaxCheck true 是否进行严格的语法检测，Druid SQL Parser 在某些场景不能覆盖所有的，SQL 语法，出现解析 SQL 出错，可以临时把这个选项设置为 false，同时把 SQL 反馈给 Druid 的开发者 conditionOpXorAllow false 查询条件中是否允许有 XOR 条件。XOR 不常用，很难判断永真或者永假，缺省不允许。 conditionOpBitwseAllow true 查询条件中是否允许有&quot;&amp;&quot;、&quot;~&quot;、&quot; conditionDoubleConstAllow false 查询条件中是否允许连续两个常量运算表达式 minusAllow true 是否允许 SELECT * FROM A MINUS SELECT * FROM B 这样的语句 intersectAllow true 是否允许 SELECT * FROM A INTERSECT SELECT * FROM B 这样的语句 constArithmeticAllow true 拦截常量运算的条件，比如说 WHERE FID = 3 - 1，其中&quot;3 - 1&quot;是常量运算表达式。 limitZeroAllow false 是否允许 limit 0 这样的语句 禁用对象检测配置： 配置项 缺省值 描述 tableCheck true 检测是否使用了禁用的表 schemaCheck true 检测是否使用了禁用的 Schema functionCheck true 检测是否使用了禁用的函数 objectCheck true 检测是否使用了“禁用对对象” variantCheck true 检测是否使用了“禁用的变量” readOnlyTables 空 指定的表只读，不能够在 SELECT INTO、DELETE、UPDATE、INSERT、MERGE 中作为&quot;被修改表&quot;出现 5.2.5 mycat配置文件 5.2.5.1 schema.xml配置 schema.xml作为mycat中重要的配置文件之一，管理者mycat的逻辑库、表、分片规则、DataNode以及DataSource。 5.2.5.2 scheme标签 1&lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt;&lt;/schema&gt; schema标签用于定义mycat实例中的逻辑库，mycat可以由多个逻辑库，每个逻辑库都有自己的相关配置，可以使用schema标签来划分这些不同的逻辑库。如果不配置schema标签，所有的表配置，会属于同一个默认的逻辑库。 1234567891011&lt;!--逻辑库TESTDB--&gt;&lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt;true&lt;table name=&quot;travelrecord&quot; dataNode=&quot;dn1,dn2,dn3&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt;&lt;/schema&gt;&lt;!--USERDB--&gt;&lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot;&gt;true&lt;table name=&quot;company&quot; dataNode=&quot;dn10,dn11,dn12&quot; rule=&quot;auto-sharding-long&quot; &gt;&lt;/table&gt;&lt;/schema&gt;&lt;!-- 逻辑库的概念和MySQL数据库中的database概念相同，我们在查询这两个不同的逻辑库中表的时候需要切换到该逻辑库下才可以查询到所需要的表--&gt; 在server.xml中可以配置不同的用户能够使用的schema 123456789101112131415161718192021222324&lt;user name=&quot;root&quot; defaultAccount=&quot;true&quot;&gt; &lt;property name=&quot;password&quot;&gt;root&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt; &lt;!--No MyCAT Database selected 错误前会尝试使用该schema作为schema，不设置则为null,报错 --&gt; &lt;!-- 表级 DML 权限设置 --&gt; &lt;!-- &lt;privileges check=&quot;false&quot;&gt; &lt;schema name=&quot;TESTDB&quot; dml=&quot;0110&quot; &gt; &lt;table name=&quot;tb01&quot; dml=&quot;0000&quot;&gt;&lt;/table&gt; &lt;table name=&quot;tb02&quot; dml=&quot;1111&quot;&gt;&lt;/table&gt; &lt;/schema&gt; &lt;/privileges&gt; --&gt;&lt;/user&gt;&lt;user name=&quot;user&quot;&gt; &lt;property name=&quot;password&quot;&gt;user&lt;/property&gt; &lt;property name=&quot;schemas&quot;&gt;TESTDB&lt;/property&gt; &lt;property name=&quot;readOnly&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;defaultSchema&quot;&gt;TESTDB&lt;/property&gt;&lt;/user&gt; schema标签的相关属性： 属性名 值 数量限制 dataNode 任意string （0…1） checkSQLschema Boolean （1） sqlMaxLimit Integer （1） 5.2.5.2.1 dataNode 该属性用于绑定逻辑库到某个具体的database上，1.3版本如果配置了dataNode,则不可以配置分片表，1.4可以配置默认分片，只需要配置需要分片的表即可，具体配置如下： 1&lt;!--1.3版本配置--&gt;&lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt;&lt;!—里面不能配置任何表--&gt;&lt;/schema&gt;&lt;!--1.4版本配置--&gt;&lt;schema name=&quot;USERDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn2&quot;&gt;&lt;!—配置需要分片的表--&gt; &lt;table name=“tuser” dataNode=”dn1”/&gt;&lt;/schema&gt;&lt;!-- 那么现在tuser就绑定到dn1所配置的具体database上，可以直接访问这个database，没有配置的表则会走默认的节点dn2，这里注意没有配置在分片里面的表工具查看无法显示，但是可以正常使用。--&gt; 5.2.5.2.2 checkSQLschema 当改制设置为true时，我们执行“SELECT * FROM TESTDB.travelrecord；”则mycat会把语句修改为“SELECT * FROM travelrecord;”即把表示schema的字符去掉，避免发送到后端数据库执行报ERROR 1146：Table ‘testdb.travelrecord’ doest’t exist.不过，即使设置该值为 true ，如果语句所带的是并非是 schema 指定的名字，例如：select * from db1.travelrecord; 那么 MyCat 并不会删除 db1 这个字段，如果没有定义该库的话则会报错，所以在提供 SQL语句的最好是不带这个字段。 5.2.5.2.3 sqlMaxLimit 当该值设置为某个数值时。每条执行的 SQL 语句，如果没有加上 limit 语句，MyCat 也会自动的加上所对应的值。例如设置值为 100，执行**select * from TESTDB.travelrecord;的效果为和执行select * from TESTDB.travelrecord limit 100;**相同。 设置该值的话，MyCat 默认会把查询到的信息全部都展示出来，造成过多的输出。所以，在正常使用中，还是建议加上一个值，用于减少过多的数据返回。 当然 SQL 语句中也显式的指定 limit 的大小，不受该属性的约束。 需要注意的是，如果运行的 schema 为非拆分库的，那么该属性不会生效。需要手动添加 limit 语句。 5.2 分配规则 范围分片：根据某个字设置auto-sharding-long，如果这个primaryKey超出了范围会报错 取模分片： ER分片（将父子表有关联的数据放在一个data-node里面） 全局表：所有dataNode存储相同的数据，查询的时候是随机查询某个表 type=global，查询的时候随机从某个datanode获取 非分片表：只在某个dataNode上存储,指定一个dataNode并且不写分片规则 单库分表：有个bug在实际数据库中必须要创建mycat中一摸一样的数据表，而且truncat的时候要现在dataNode先删除，才能删除的掉mycat的数据 5.3 全局ID 文件方式—0 数据库方式—1 本地时间戳----2 ZK方式----3 6. Mycat分片策略详解 连续分片与离散分片 连续分片： 范围分片 日期/事件 缺点： 存在数据热点的可能性 并发访问能力受限于单一或少量DataNode（访问集中），并不能分摊数据库访问的压力 离散： 取模（partioncount 的总数必须和分片总数相同） 枚举 一致性哈希(qs-murmur) 固定分片哈希 partitionCount: 2, 1表示有三个分片必须和节点数量一致，否则会报错，前面两个一样长 partitionLength: 256, 512表示长度为256和512 综合在一起就是前面2个分片长度为256， 最后一个为512，结果如下图所示（注意partitionCount和partitionLength的数量一定要一致） 取模范围(sharting-by-pattern)：先取模PartitionByPattern后分片 范围取模: PartitionByRangeMod(partition-rane-mod.txt) 0-2000=1 #范围在2000以内的在第一个节点（取模的结果还是本身） 2001-4000=2 #范围在2001到4000以内的再模2，结果为0在第一个节点，结果为1在第三个节点 12345678910111213141516171819202122232425262728293031323334353637- 其他优点：- 并发访问能力增强（负载到不同的节点）- 范围条件查询性能提升（并行计算）缺点：- 数据扩容比较困难，设计到数据迁移问题- 数据库连接消耗比较多分片策略的选择：1） 确定分片表2） 找出分片键3） 考虑容量、增速、业务用户如果在查询语句中没有携带分片建，那么mycat会将sql发布到所有的节点上## 7. Mycat扩缩容### 7.1 在线不停机扩缩容（双写）![image-20201107205855474](./mycat/image-20201107205855474.png)### 7.2 离线扩缩容#### 7.2.1Mysql Dump```shellmysqldump -uroot -p123456 -h127.0.0.1 -p3306 -c -t --skip-extended-insert 数据库名称 &gt; mysql.11.11.sql 7.2.2 Mycat自带工具 mycat所在环境安装mysql客户端程序 mycat的lib目录下添加mysql的jdbc驱动包（mysql-connector-java-5.7.1.jar） 对扩容缩容的表所有节点数据进行备份 复制schema.xml、rule.xml并重命名为newSchema.xml、newRule.xml 修改newSchema.xml和newRule.xml配置文件为扩容缩容后的参数 在conf/migrateTable.properties配置文件中配置分片库和分片表如：imall=table_test1 dataMigrate.sh配置mysqldump路径 停止mycat服务 执行bin/dataMigrate.sh脚本（不能用openjdk） 替换schema.xml、rule.xml 注意事项： 保证分片表迁移数据前后路由规则一致（取模–&gt;取模） 保证分片表歉意数据前后分片字段一致 全局表将被忽略 不要将非分片表配置到migrateTables.properties文件中 暂时只支持分片表使用Mysql作为数据源的扩容缩容 8. Mycat读写分离 8.1 主从复制 数据备份回复 负载均衡（读写分离） 高可用HA 8.2 主从复制形式; binlog(Binary log 二进制日志) 12--查看binglog: SHOW binlog events in &#x27;mysql-bin.000001&#x27;--show variables like &#x27;max_blog_max&#x27; binlog配置 STATEMENT: 记录每一天修改数据的sql语句（减少日志量，节约IO） ROW: 记录哪条数据被修改了，修改成什么样子了（5.7以后默认） MIXED: 结合两种方式，一般语句用STATEMENT,函数之类的使用ROW binlog格式（mysql-bin.00001等） 查看binlog 1show binlog events in &#x27;mysql-bin.00001&#x27;; 主从复制原理 SQL Thread是单线程的， 这也是所有的主从复制延迟的原因，那么relay log接受master节点的sql语句主要是用于缓冲 mycat读写分离配置 8.5 Mycat注解（hint） 注解用法： 12345/*!mycat:sql=注解sql语句*/真正执行的SQL比如说在mycat上创建表无法创建成功，可以使用注解/*!mycat:sql=select * from table_1 where id = 1*/create table test2(id int);主要注解sql可以确认mycat可以路由到子结点上，就可以执行后面的真正执行的sql语句 注解用途： 跨库关联查询 DDL或存储过程 自定义分片 读写分离 分布式事务 基于XA协议的两阶段提交 XA角色 XA实现 9. Mycat核心流程 9.1 架构图 9.2 启动流程 MycatServer启动，解析配置文件，包括服务器、分片规则等 创建工作线程，建立前端连接和后端连接 9.3 执行SQL流程 前端连接接收mysql命令 解析MySQL，mycat用的是Druid的DruidParser 获取路由 改写MySQL，例如两个条件在两个节点上，则变成两条单独的sql 与后端数据库建立连接 发送sql语句到MySQL执行 获取返回结果 处理返回结果，例如排序、计算等等 返回给客户端 Mycat高可用","categories":[{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"}],"tags":[{"name":"mycat","slug":"mycat","permalink":"https://xiaoyuge5201.github.io/tags/mycat/"}]},{"title":"mysql事务","slug":"mysql-transcation","date":"2021-07-23T08:00:57.000Z","updated":"2021-07-23T08:00:57.000Z","comments":true,"path":"mysql-transcation/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql-transcation/index.html","excerpt":"","text":"事务的定义 事务是数据管理系统DBMS执行过程中的一个逻辑单位，有一个有限的数据库操作序列构成 事务四大特性 原子性atomicity：依赖undo log做到全部失败 隔离性isolation：实现方式LBCC 和 MVCC 持久性durability ：实现方式redo log和double write 一致性consistency：通过上面的三种方式实现 数据恢复： redo log 崩溃恢复 双写缓冲（double write） Mysql中insert、delete、update 自带事务 1234show veriables like ‘autocommit’;set session autocommit = on;update xxx where set xx =1 ;commit; 结束事务两种方式：rollback commit 事务并发的三大问题 数据并发的三大问题其实都是数据库读一致性问题，必须有数据库提供一定的事务隔离机制来解决。 脏读 不可重复读 幻读 事务隔离级别 http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt MVCC思想 Read View（一致性试图） 存储内容 Read View判断规则 RC与RR read View 的区别 所以RC解决不了脏读的问题 Mysql InnoDb所得基本类型 InnoDB支持行锁 MyiSAM支持行锁 表锁和行锁的区别 锁力度：表锁 &gt; 行锁 加锁效率：表锁 &gt; 行锁 冲突概率：表锁 &gt; 行锁 并发性能：表锁 &lt; 行锁 表锁 一个事务能够给一张表加上锁的前提是：没有其他任何一个事务锁定了这张表的任意一行数据。如果没有意向锁的话，那么加表锁需要扫描表中的每行数据，大大的浪费时间； 如果在添加行锁的时候，会在表上添加意向锁，那么在添加表锁的时候就不需要去扫描所有表数据了，只需要看下表上是否由意向锁就可； 行锁 共享锁shared locks 排它锁Exclusive locks Innodb行锁锁定的是什么 锁定的是index索引，如果表中没有索引，那么Innodb会把隐藏列DB_ROW_ID当作聚集索引 加锁一定要加上条件，不然会锁表 记录锁Rcord Lock 锁定记录 间隙锁Gap Lock 锁定范围 专门用于阻塞插入，间隙锁如果没有命中的话，会锁定最后一个值到正无穷，那么在最后一个值和正无穷之间的插入都不能成功。 临健锁Next-key Lock ：锁定范围加记录 为了解决幻读的问题 事务隔离级别的实现 事务隔离级别的选择","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"mysql知识总结","slug":"mysql","date":"2021-07-23T08:00:57.000Z","updated":"2021-07-23T08:00:57.000Z","comments":true,"path":"mysql/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql/index.html","excerpt":"","text":"mysql 初识 1. 版本历史 1996年 mysql.10发布 1996年10月3.11.1发布 2000年ISAM升级成MyISAM引擎，mysql开源 2003年 Mysql4.0发布集成InnoDB存储引擎 2005年 MySQL 5.0版本发布，提供了试图，存储过程等功能 2010年MySQL5.5发布，InnoDB成为默认的存储引擎 2016年发布8.0.0版本 2. 流行分支 Maria DB Percona Server 3. SQL 执行流程 通信类型 同步 异步 连接方式 长连接 短连接 超时时间 非交互式超时时间，如JDBC程序，单位s 1SHOW GLOBAL VARIABLES LIKE &#x27;wait_timeout&#x27; 交互式超时间，如数据库工具 1SHOW GLOBAL variables LIKE &#x27;interactive_timeout&#x27; 查看连接 1show GLOBAL STATUS LIKE &#x27;Thread%&#x27; 连接名称 描述 Threads_cached 缓存中的线程 Threads_connected 连接中线程 Threads_created 创建过的线程 Threads_running 正在执行的线程 查看所有的线程 如果是root权限，可以看到所有用户发起的线程，否则只能看到自己的线程 1show processlist id ：一个表示，kill一个语句的时候可以使用 user：显示当前用户，如果不是root，这个命令就只显示你权限范围内的sql语句 host：显示这个语句是从哪个ip的端口上发出的，可以用来追踪出问题语句的用户 db：显示这个进程目前连接的是哪个数据库 commmand：显示当年连接的执行命令，一般分为休眠slee、查询query、连接connect time：此状态持续的时间，单位是秒 state： 显示使用当年连接的sql语句状态，state只是语句执行中的某一个状态，如查询：需要经过copying to tmp table、sorting result、sending data等转台才可以完成 info：显示这个sql语句，因为长度有限，所以长的sql语句就显示不全 查看最大连接数 1show variables LIKE &#x27;max_connections&#x27;; //一般默认是151，最大可以是2的14次方 mysql变量级别 global全局 1234在mysql中修改全局变量global有两种方法：1. 修改my.ini配置文件（永久有效）2. 在不修改配置文件的基础上，使用关键字global设置全局变量 set global autocommit = 1;将autocommit变量的值设置为ON，需要注意的是此方法对global全局变量的设计进对于新开启的会话有效，对已开启的会话无效，同理，如果修改回哈session变量，可以使用session关键字，如set session autocommit = 1；这个仅对本session的变量配置有效，对其他的session无效；（在MySQL服务重启之后，数据库的配置重新按照my.ini文件 初始化，global和session 的配置都会失效） session当前会话 通信协议 Unix Socket TCP/IP Named Pipes命名管道 Share Memory共享内存 通信方式 单工 半双工 全双工 MySQL 缓存 12SHOW VARIABLES LIKE &#x27;query_cache%&#x27;#默认关闭，是因为mysql要保证两次执行的sql完全一致，连空格，大小写都一致，而且当数据表中的任何一条数据发生变化，整个缓存会失效； #2. 删除数据 1. 数据删除方式 DELETE Truncate Drop 2. 执行速度 drop &gt; truncate &gt; delete 2.1 DELETE 1DELETE FROM table_name WHERE XXX DELETE 数据数据库DML操作语言，只删除数据不删除表的结构，会走事务，执行时会触发trigger 在InnoDB中，delete其实并不会真的把数据删除，mysqL实际上只是给删除的数据打个标记为删除，因此delete删除表中的数据，表文件在磁盘所占的控件不会变小，存储控件不会被释放，只是把删除的数据设置为不可见。虽然未释放磁盘控件，但是下次插入数据的时候，仍然可以重用这部分空间（重用-&gt;覆盖） delete执行时，会先把所删除数据缓存到rollback segement中，事务commit之后生效 delete from table_name 删除表的全部数据对于MyISAM会释放磁盘控件，Innodb不会释放磁盘空间 对于DELETE from table_name where xxx带条件的删除，不管是Innodb还是MyISAM都不会释放磁盘控件 delete 操作以后使用optimize table table_name会里级释放磁盘空间（不管是Innodb还是MyISAM） 123456--查看表占用磁盘空间大小select concat(round(sum(DATA_LENGTH/1024/1024),2),&#x27;M&#x27;) as table_sizefrom information_schema.tables where table_schema=&#x27;demo_db&#x27; AND table_name=&#x27;demo_table&#x27;;-- 执行空间优化语句，以及执行后的表size变化optimize table demo_table delete 操作时一行一行执行删除的，并且同时将该行的删除操作日志记录在redo和undo表空间中以便进行回滚（rollback）和重做操作，生成大量日志也会占用磁盘空间 2.2 Truncate 123--删除表数据， 不带where条件--与不带where的delete ：只删除数据，而不删除表的结构 Truncate table table_name Truncate数据数据库DDL定义语言，不走事务，原数据不放到rollback segement中，操作不触发trigger，执行后里级生效，无法找回； truncate table table_name里级释放磁盘空间不管是Innodb 和MyISAM；truncate table其实有点类似余drop table然后create，只不过这个crate table的过程做了优化，比如表结构文件之前已经有了等，所以速度上应该是接近drop table的速度 truncate 能快速清空一个表，并且重置auto_increment的值 ​ 但是对于不同的类型存储引擎需要注意的地方是： 对于MyISAM：truncate会重置auto_increment（自增序列）的值为1，而delete后表仍然保持auto_increment。 对于Innodb：truncate会重置auto_increment（自增序列）的值为1， 而delete后表仍然保持auto_increment。但是在做delete整个表之后重启mysql的话，而重启后的auto_increment会被置为1 也就是说，Innodb的表本身是无法持久保存auto_increment。delete表之后auto_increment仍然保存在内存，但是重启后就丢失了，只能从1开始，实质上重启后的auto_increment会从SELETE 1+MAX(ai_col) FROM t开始 小心使用 truncate，尤其没有备份的时候，如果误删除线上的表，记得及时联系中国民航，订票电话：400-806-9553 2.3 Drop 12-- 删除表结构以及表数据Drop table table_name drop：属于数据库DDL定义语言，同Truncate； 执行后立即生效，无法找回！ 执行后立即生效，无法找回！ 执行后立即生效，无法找回！ **drop table table_name 立刻释放磁盘空间 ，不管是 InnoDB 和 MyISAM; **drop 语句将删除表的结构被依赖的约束(constrain)、触发器(trigger)、索引(index); 依赖于该表的存储过程/函数将保留,但是变为 invalid 状态。 小心使用 drop ，要删表跑路的兄弟，请在订票成功后在执行操作！订票电话：400-806-9553 3. 总结 可以这么理解，一本书，delete是把目录撕了，truncate是把书的内容撕下来烧了，drop是把书烧了","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"锁优化","slug":"lock01","date":"2021-07-23T06:04:02.000Z","updated":"2021-07-23T06:04:02.000Z","comments":true,"path":"lock01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/lock01/index.html","excerpt":"","text":"1. 优化思路以及方法 减少锁持有时间 减小锁粒度 锁分离 锁粗化 锁消除 1.1 减少锁持有时间 12345public synchronized void syncMethod()&#123; othercode1(); mutextMethod(); othercode2();&#125; 像上述代码，在进入方法前就要得到锁，其他线程就要在外面等待。 分析：锁里面的资源在同一时间只允许一个线程执行，我们不仅要减少其他线程等待的时间，也要尽力减少线程在锁里面的执行时间，所以，尽量只有在有线程安全要求的程序代码上加锁。 1234567public void syncMethod()&#123; othercode1(); synchronized(this)&#123; metextMethod(); &#125; othercode2();&#125; 1.2 减小锁粒度 将大对象（这个对象可能会被很多线程访问）拆成小对象，大大增加并行度。 降低锁竞争，那么偏向锁、轻量级锁成功率才会提高。 最最典型的减小锁粒度的案例就是ConcurrentHashMap。在HashMap的基础上进行优化，使用了cas与synchronized来确保安全性，在保证安全性的基础上为了充分利用线程资源，更是巧妙的设计了多线程同扩容的模式。 1.3 锁分离 最常见的锁分离就是读写锁ReadWriteLock，根据功能进行分离成读锁和写锁。这样读读不互斥，读写互斥，写写互斥。既保证了线程安全，又提高了性能。 分析：读写分离这种思想可以延伸到我们其他的设计中，只要操作上互不影响，那锁就可以进行分离，比如：LinkedBlockingQueue 从头部获取数据，从尾部放入数据，使用两把锁。 1.4 锁粗化 通常情况下，为了保证多线程间的有效并发，会要求每个线程持有锁的时间尽量短，即在使用完公共资源后，应该立即释放锁，只有这样，等待在这个锁上的其他线程才能尽早的获取资源执行任务；但是凡事都有一个度，如果对同一个锁不停的进行请求、同步和释放，其本身也会消耗系统宝贵的资源，反而不利于性能的优化。 123456789public void demoMethod()&#123; synchronized&#123; //dow sth.true&#125; //....做其他不需要同步的工作，但能很快执行完毕 synchronized&#123; //do sth. &#125;&#125; 这种情况，根据锁粗化的思想，应该合并： 1234567public void demoMethod()&#123; //整合成一次锁请求,前提时中间哪些不需要同步的工作很快就执行完成 synchronized(lock)&#123; //do sth. //....做其他不需要同步的工作，但能很快执行完毕 &#125;&#125; 再举一个极端的例子： 12345for(int i =0; i &lt; circle; i++)&#123; synchronized(lock)&#123; //..... &#125;&#125; 在一个循环内不同得获得锁。虽然JDK内部会对这个代码做些优化，但是还不如直接写成： 1234synchronized(lock)&#123; for(int i =0; i &lt; circle; i++)&#123; &#125;&#125; 当然如果有需求说，这样的循环太久，需要给其他线程不要等待太久，那只能写成上面那种。如果没有这样类似的需求，还是直接写成后者那种比较好。 分析: 锁粗化是JVM默认启动的一种机制，锁粗化针对的是对连续的区域进行分段加锁这种场景，JVM会自发进行优化。但作为开发者而言在满足业务的情况下，应该减少锁的使用。 1.5 锁消除 锁消除是在编译器级别的事情。在即时编译器(JIT)时，如果发现不可能被共享的对象，则可以消除这些对象的锁操作。也许你会觉得奇怪，既然有些对象不可能被多线程访问，那为什么要加锁呢？写代码时直接不加锁不就好了。 但是有时，这些锁并不是程序员所写的，有的是JDK实现中就有锁的，比如Vector和StringBuffer这样的类，它们中的很多方法都是有锁的。当我们在一些不会有线程安全的情况下使用这些类的方法时，达到某些条件时，编译器会将锁消除来提高性能。 1234567891011121314public static void main(String args[]) throws InterrruptedException&#123; long start = System.currentTimeTimeMillis(); for(int i = 0;i &lt; 20000; i++)&#123; createStringBuffer(&quot;JVM&quot;,&quot;asdfasdfasdf&quot;); &#125; long bufferCost = System.currentTimeTimeMillis() - start; System.out.println(&quot;createStringBuffer:&quot;+bufferCost+&quot;ms&quot;);&#125;public static String createStringBuffer(String s1, String s2)&#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb.toString();&#125; 上述代码中的StringBuffer.append是一个同步操作，但是StringBuffer却是一个局部变量，并且方法也并没有把StringBuffer返回，所以不可能会有多线程去访问它。那么此时StringBuffer中的同步操作就是没有意义的。 开启锁消除是在JVM参数上设置的，当然需要在server模式下： 1-server -XX:+DoEscapeAnalysis -XX:+EliminateLocks 并且要开启逃逸分析。 逃逸分析的作用呢，就是看看变量是否有可能逃出作用域的范围。 比如上述的StringBuffer，上述代码中craeteStringBuffer的返回是一个String，所以这个局部变量StringBuffer在其他地方都不会被使用。如果将craeteStringBuffer改成 123456public static StringBuffer createStringBuffer(String s1, String s2)&#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); return sb;&#125; 那么这个 StringBuffer被返回后，是有可能被任何其他地方所使用的（譬如被主函数将返回结果put进map啊等等）。那么JVM的逃逸分析可以分析出，这个局部变量 StringBuffer逃出了它的作用域。 所以基于逃逸分析，JVM可以判断，如果这个局部变量StringBuffer并没有逃出它的作用域，那么可以确定这个StringBuffer并不会被多线程所访问，那么就可以把这些多余的锁给去掉来提高性能。 当JVM参数为： 1-server -XX:+DoEscapeAnalysis -XX:+EliminateLocks 输出： 1createStringBuffer: 302ms JVM参数为： 1-server -XX:+DoEscapeAnalysis -XX:-EliminateLocks 输出： 1createStringBuffer: 660ms 显然，锁消除的效果还是很明显的。","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"lock","slug":"lock","permalink":"https://xiaoyuge5201.github.io/tags/lock/"}]},{"title":"ElasticSearch安装","slug":"ElasticSearch","date":"2021-07-23T05:41:24.000Z","updated":"2021-07-23T05:41:24.000Z","comments":true,"path":"ElasticSearch/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/ElasticSearch/index.html","excerpt":"","text":"1. JDK14安装 下载jdk14： https://jdk.java.net/14/ 将文件存放在linux系统某文件夹内 解压 1tar -zxvf openjdk-14.0.2_linux-x64_bin.tar.gz 配置环境变量 123456vim /etc/profile##在文件最末尾添加，其中JAVA_HOME是jdk解压后的文件路径JAVA_HOME=/usr/lib/tools/jdk-14.0.2PATH=$JAVA_HOME/bin:$PATHCLASSPATH=$JAVA_HOME/jre/lib/ext:$JAVA_HOME/lib/tools.jarexport PATH JAVA_HOME CLASSPATH 保存后，更新配置文件 1source /etc/profile 查看JDK是否配置完成 1java -version 出现下图表示安装成功！ 2. ElasticSearch安装 解压tar.gz包 1tar -zxvf elasticsearch-7.8.0-linux-x86_64.tar.gz 添加elasticsearch用户 1useradd elastic 赋予elastic search操作文件夹的权限 1chown -R elastic:elastic /usr/lib/tools/elasticsearch-7.8.0/* 查看本机的hostname 12hostname#localhost.localdomain 修改elastic search配置 12 cd ./elasticsearch-7.8.0/configvim elasticsearch.yml elasticsearch.yml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495# ======================== Elasticsearch Configuration =========================## NOTE: Elasticsearch comes with reasonable defaults for most settings.# Before you set out to tweak and tune the configuration, make sure you# understand what are you trying to accomplish and the consequences.## The primary way of configuring a node is via this file. This template lists# the most important settings you may want to configure for a production cluster.## Please consult the documentation for further information on configuration options:# https://www.elastic.co/guide/en/elasticsearch/reference/index.html## ---------------------------------- Cluster -----------------------------------## Use a descriptive name for your cluster:#集群名称，默认可以不修改，此处 xiaoyugecluster.name: xiaoyuge# ------------------------------------ Node ------------------------------------## Use a descriptive name for the node:#节点名称，必须修改 ，默认修改为当前机器名称，若是多实例则需要区分node.name: xiaoyuge-local1## Add custom attributes to the node:##node.attr.rack: r1## ----------------------------------- Paths ------------------------------------## Path to directory where to store the data (separate multiple locations by comma):##数据目录与日志目录，默认在当前运行程序下，生产环境需要指定#path.data: /path/to/data## Path to log files:##path.logs: /path/to/logs## ----------------------------------- Memory -----------------------------------## Lock the memory on startup:#内存交换锁定，此处需要操作系统设置才生效#bootstrap.memory_lock: true## Make sure that the heap size is set to about half the memory available# on the system and that the owner of the process is allowed to use this# limit.## Elasticsearch performs poorly when the system is swapping the memory.## ---------------------------------- Network -----------------------------------## Set the bind address to a specific IP (IPv4 or IPv6):#IP 地址，默认是 local，仅限本机访问，外网不可访问，设置 0.0.0.0 通用做法network.host: 192.168.135.111## Set a custom port for HTTP:#访问端口，默认 9200，9300，建议明确指定http.port: 9200transport.port: 9300## For more information, consult the network module documentation.## --------------------------------- Discovery ----------------------------------## Pass an initial list of hosts to perform discovery when this node is started:# The default list of hosts is [&quot;127.0.0.1&quot;, &quot;[::1]&quot;]## 集群发现配置discovery.seed_hosts: [&quot;192.168.135.111:9300&quot;]## Bootstrap the cluster using an initial set of master-eligible nodes:#cluster.initial_master_nodes: [&quot;192.168.135.111:9300&quot;]## For more information, consult the discovery and cluster formation module documentation.## ---------------------------------- Gateway -----------------------------------## Block initial recovery after a full cluster restart until N nodes are started:##gateway.recover_after_nodes: 3## For more information, consult the gateway module documentation.## ---------------------------------- Various -----------------------------------## Require explicit names when deleting indices:##防止批量删除索引action.destructive_requires_name: true#设置密码xpack.security.enabled: truexpack.license.self_generated.type: trialxpack.security.transport.ssl.enabled: truehttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;http.cors.allow-headers: Authorization 切换为elastic search用户，然后启动elastic search 12su elastic #切换用户./bin/elasticsearch -d #后台启动 设置密码 12345678910111213141516171819202122232425./bin/elasticsearch-setup-passwords interactive#执行设置用户名和密码的命令,这里需要为4个用户分别设置密码，elastic, kibana, logstash_system,beats_systemInitiating the setup of passwords for reserved users elastic,kibana,logstash_system,beats_system.You will be prompted to enter passwords as the process progresses.Please confirm that you would like to continue [y/N]yEnter password for [elastic]:passwords must be at least [6] characters longTry again.Enter password for [elastic]:Reenter password for [elastic]:Passwords do not match.Try again.Enter password for [elastic]:Reenter password for [elastic]:Enter password for [kibana]:Reenter password for [kibana]:Enter password for [logstash_system]:Reenter password for [logstash_system]:Enter password for [beats_system]:Reenter password for [beats_system]:Changed password for user [kibana]Changed password for user [logstash_system]Changed password for user [beats_system]Changed password for user [elastic] 常见异常： 1234 #java.lang.RuntimeException: can not run elasticsearch as root #切换为elastic search用户，不能用root项目启动 su elastic 12345#Exception in thread &quot;main&quot; java.nio.file.AccessDeniedException: /usr/lib/tools/elasticsearch-7.8.0/config/elasticsearch.keystore#elastic search用户没有操作该文件夹的权限 su rootchown -R elastic:elastic /usr/lib/tools/elasticsearch-7.8.0/* 123456789101112131415161718ERROR: [2] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65535][2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] #分配内存不够#1. 修改 /etc/security/limits.confsudo vi /etc/security/limits.conf#在文件末尾加上* soft nofile 65536* hard nofile 65536* soft nproc 4096* hard nproc 4096#2. 修改 /etc/sysctl.confsudo vi /etc/sysctl.conf#在文件末尾增加vm.max_map_count=262144#3. 配置重新生效sysctl -p 效果如下： 3. Kibana安装 解压文件 1tar -zxvf kibana-7.8.0-linux-x86_64.tar.gz 修改配置文件 1vim ./config/kibana.yml kibana.yml: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Kibana is served by a back end server. This setting specifies the port to use.##访问端口，默认无需修改server.port: 5601# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.# The default is &#x27;localhost&#x27;, which usually means remote machines will not be able to connect.# To allow connections from remote users, set this parameter to a non-loopback address.#访问地址 IP，默认本地 ;如果需要外网访问，则配置0.0.0.0server.host: &quot;0.0.0.0&quot;# Enables you to specify a path to mount Kibana at if you are running behind a proxy.# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath# from requests it receives, and to prevent a deprecation warning at startup.# This setting cannot end in a slash.#server.basePath: &quot;&quot;# Specifies whether Kibana should rewrite requests that are prefixed with# `server.basePath` or require that they are rewritten by your reverse proxy.# This setting was effectively always `false` before Kibana 6.3 and will# default to `true` starting in Kibana 7.0.#server.rewriteBasePath: false# The maximum payload size in bytes for incoming server requests.#server.maxPayloadBytes: 1048576# The Kibana server&#x27;s name. This is used for display purposes.#server.name: &quot;your-hostname&quot;# The URLs of the Elasticsearch instances to use for all your queries.# ES 服务指向，集群下配置多个elasticsearch.hosts: [&quot;http://192.168.135.111:9200&quot;]# When this setting&#x27;s value is true Kibana uses the hostname specified in the server.host# setting. When the value of this setting is false, Kibana uses the hostname of the host# that connects to this Kibana instance.#elasticsearch.preserveHost: true# Kibana uses an index in Elasticsearch to store saved searches, visualizations and# dashboards. Kibana creates a new index if the index doesn&#x27;t already exist.# Kibana 元数据存储索引名字，默认.kibana 无需修改#kibana.index: &quot;.kibana&quot;# The default application to load.#kibana.defaultAppId: &quot;home&quot;# If your Elasticsearch is protected with basic authentication, these settings provide# the username and password that the Kibana server uses to perform maintenance on the Kibana 启动 1234 # 当前窗口内启动 ./bin/kibana# #后台进程启动nohup ./bin/kibana &amp; 效果如下 遇见问题： root启动报错 12#切换到elastic账户su xiaoyuge elastic用户权限不足 Babel could not write cache to file: /usr/share/kibana/optimize/.babel_register_cache.json 1234567#切换到root用户su root #赋予elastic账户 xiaoyuge操作权限chown -R xiaoyuge /usr/local/kibana-7.7.1-linux-x86_64#切换为elastic账户su xiaoyuge#再次启动即可","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"ELK","slug":"ELK","permalink":"https://xiaoyuge5201.github.io/tags/ELK/"}]},{"title":"vue学习","slug":"vue","date":"2021-07-23T03:40:44.000Z","updated":"2021-07-23T03:40:44.000Z","comments":true,"path":"vue/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/vue/index.html","excerpt":"","text":"Object.freeze()，这会阻止修改现有的 property，也意味着响应系统无法再追踪变化。 1. export 用于规定模块的对外接口，export输出变量和方法、类 变量 1234567// profile.jsexport var firstName = &#x27;Michael&#x27;;export var lastName = &#x27;Jackson&#x27;;export var year = 1958;//简写--优先使用export &#123;firstName, lastName, year&#125; 方法 123//如果想为输入的变量重新命名， 可以使用AS 关键字重新命名import &#123; buildMenus as buildMenus&#125; from &#x27;@/api/menu&#x27;;//import命令接受一对大括号，里面指定要从其他模块导入的变量名。大括号里面的变量名，必须与被导入模块（profile.js）对外接口的名称相同 2. export default 为模块指定默认输出， 使用import命令的时候，用户需要知道所要加载的变量名和函数名，否则无法加载；了解模块有哪些方法和属性比较麻烦，使用export default命令，为模块指定默认输出 1234// export-default.jsexport default function () &#123; console.log(&#x27;foo&#x27;);&#125; 上面代码是一个模块文件export-default.js。默认输出1个函数； 与export命令的区别：其他模块加载该模块是，import命令可以为该匿名函数指定任意名字 123// import-default.jsimport customName from &#x27;./export-default&#x27;;customName(); // &#x27;foo&#x27; 上面代码的import命令，可以用任意名称指向export-default.js输出的方法，这时就不需要知道原模块输出的函数名。需要注意的是，这时import命令后面，不使用大括号。 本质上，export default就是输出一个叫做default的变量或方法，然后系统允许你为它取任意名字。所以，下面的写法是有效的。 123456789101112// modules.jsfunction add(x, y) &#123; return x * y;&#125;export &#123;add as default&#125;;// 等同于// export default add;// app.jsimport &#123; default as foo &#125; from &#x27;modules&#x27;;// 等同于// import foo from &#x27;modules&#x27;; 正是因为export default命令其实只是输出一个叫做default的变量，所以它后面不能跟变量声明语句。 总结： export命令对外接口是有名称的且import命令从模块导入的变量名与被导入模块对外接口的名称相同，而export default命令对外输出的变量名可以是任意的，这时import命令后面，不使用大括号。 export default命令用于指定模块的默认输出。显然，一个模块只能有一个默认输出，因此export default命令只能使用一次。所以，import命令后面才不用加大括号，因为只可能唯一对应export default命令。 12345678910111213141516171819202122232425262728293031323334//menu.js//get请求获取所有的菜单信息export function buildMenus() &#123; return request(&#123; url: &#x27;api/menus/build&#x27;, method: &#x27;get&#x27; &#125;)&#125;//post 请求保存数据export function add(data) &#123; return request(&#123; url: &#x27;api/menus&#x27;, method: &#x27;post&#x27;, data &#125;)&#125;//delete 请求删除数据export function del(id) &#123; return request(&#123; url: &#x27;api/menus/&#x27; + id, method: &#x27;delete&#x27; &#125;)&#125;//put请求修改数据export function edit(data) &#123; return request(&#123; url: &#x27;api/menus&#x27;, method: &#x27;put&#x27;, data &#125;)&#125;//app.vueimport &#123; buildMenus &#125; from &#x27;@/api/menu&#x27;; 3. Const、var、let ES5 中作用域有：全局作用域、函数作用域。没有块作用域的概念。 ES6 中新增了块级作用域。块作用域由 { } 包括，if语句和 for语句里面的{ }也属于块作用域 12345678910111213141516171819202122232425&#123; var a = 1; console.log(a); // 1&#125;console.log(a); // 1// 通过var定义的变量可以跨块作用域访问到。(function A() &#123; var b = 2; console.log(b); // 2&#125;)();// console.log(b); // 报错，// 可见，通过var定义的变量不能跨函数作用域访问到if(true) &#123; var c = 3;&#125;console.log(c); // 3for(var i = 0; i &lt; 4; i ++) &#123; var d = 5;&#125;;console.log(i); // 4 (循环结束i已经是4，所以此处i为4)console.log(d); // 5// if语句和for语句中用var定义的变量可以在外面访问到，// 可见，if语句和for语句属于块作用域，不属于函数作用域 三者的区别： var定义的变量，没有块的概念，可以跨块访问, 不能跨函数访问。 let定义的变量，只能在块作用域里访问，不能跨块访问，也不能跨函数访问。 const用来定义常量，使用时必须初始化(即必须赋值)，只能在块作用域里访问，而且不能修改。 1234567891011121314151617181920212223242526272829303132// 块作用域&#123; var a = 1; let b = 2; const c = 3; // c = 4; // 报错 var aa; let bb; // const cc; // 报错 console.log(a); // 1 console.log(b); // 2 console.log(c); // 3 console.log(aa); // undefined console.log(bb); // undefined&#125;console.log(a); // 1// console.log(b); // 报错// console.log(c); // 报错// 函数作用域(function A() &#123; var d = 5; let e = 6; const f = 7; console.log(d); // 5 console.log(e); // 6 console.log(f); // 7 &#125;)();// console.log(d); // 报错// console.log(e); // 报错// console.log(f); // 报错 注意：const定义的对象属性是否可以改变 123456const person = &#123; name : &#x27;jiuke&#x27;, sex : &#x27;男&#x27;&#125;person.name = &#x27;test&#x27;console.log(person.name)//person对象的name属性确实被修改了 因为对象是引用类型的，person中保存的仅是对象的指针，这就意味着，const仅保证指针不发生改变，修改对象的属性不会改变对象的指针，所以是被允许的。也就是说const定义的引用类型只要指针不发生改变，其他的不论如何改变都是允许的。 然后我们试着修改一下指针，让person指向一个新对象，果然报错 123456789const person = &#123; name : &#x27;jiuke&#x27;, sex : &#x27;男&#x27;&#125;person = &#123; name : &#x27;test&#x27;, sex : &#x27;男&#x27;&#125;//报错 4. promise promise用途：异步编程的一种解决方案。 优点：比传统的解决方案——回调函数和事件——更合理和更强大。 三种状态：pending（进行中）、fulfilled（已成功）和rejected（已失败）。 123456789101112131415161718//基本用法：const promise = new Promise(function(resolve, reject) &#123; resolve(value);//表示异步操作成功 reject(error);//表示异步操作失败&#125;);//promise常用的几个方法//1. 异步状态为成功时调用第一个函数，为失败时调用第二个函数。then方法的第二个参数可选。promise.then(value =&gt; &#123;&#125;,error =&gt; &#123;&#125;);//2. 异步状态为失败时调用。promise.catch(error =&gt; &#123;&#125;);//3. promise异步状态为失败时或then方法中抛出错误都会执行catch方法。promise.then(value =&gt; &#123;&#125;,error =&gt; &#123;&#125;).catch(error =&gt; &#123;&#125;);//4. 不管状态如何都会执行的操作。promise.finally(() =&gt; &#123;&#125;); 5. 生命周期 6. 模版语法 v-once 执行一次性插值，当数据变化的时候，该内容不会更新；可能会影响该节点其他的数据绑定 1&lt;span v-once&gt;这个将不会改变: &#123;&#123; msg &#125;&#125;&lt;/span&gt; v-html 双大括号会将数据解释为普通文本，而非 HTML 代码。为了输出真正的 HTML，你需要使用v-html; 1234var rawHtml = &quot;&lt;span&gt;这是个使用v-htmls&lt;/span&gt;&quot;&lt;p&gt;Using mustaches: &#123;&#123; rawHtml &#125;&#125;&lt;/p&gt;&lt;p&gt;Using v-html directive: &lt;span v-html=&quot;rawHtml&quot;&gt;&lt;/span&gt;&lt;/p&gt; Attribute Mustache ({}) 语法不能作用在 HTML attribute 上，遇到这种情况应该使用 v-bind 指令： 12345&lt;div v-bind:id=&quot;dynamicId&quot;&gt;&lt;/div&gt;//isButtonDisabled 的值是 null、undefined 或 false，则 disabled attribute 甚至不会被包含在渲染出来的 &lt;button&gt; 元素中&lt;button v-bind:disabled=&quot;isButtonDisabled&quot;&gt;Button&lt;/button&gt; 三元表达式 1234567891011121314&#123;&#123; number + 1 &#125;&#125;&#123;&#123; ok ? &#x27;YES&#x27; : &#x27;NO&#x27; &#125;&#125;&#123;&#123; message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125;&#125;&lt;div v-bind:id=&quot;&#x27;list-&#x27; + id&quot;&gt;&lt;/div&gt;//这些表达式会在所属 Vue 实例的数据作用域下作为 JavaScript 被解析。有个限制就是，每个绑定都只能包含单个表达式，所以下面的例子都不会生效。&lt;!-- 这是语句，不是表达式 --&gt;&#123;&#123; var a = 1 &#125;&#125;&lt;!-- 流控制也不会生效，请使用三元表达式 --&gt;&#123;&#123; if (ok) &#123; return message &#125; &#125;&#125; 7. 指令Directives 指令 (Directives) 是带有 v- 前缀的特殊 attribute。指令 attribute 的值预期是单个 JavaScript 表达式 (v-for 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。 12//v-if 指令将根据表达式 seen 的值的真假来插入/移除 &lt;p&gt; 元素。&lt;p v-if=&quot;seen&quot;&gt;现在你看到我了&lt;/p&gt; 参数 一些指令能够接收一个“参数”，在指令名称之后以冒号表示。例如，v-bind 指令可以用于响应式地更新 HTML attribute 1234//href 是参数，告知 v-bind 指令将该元素的 href attribute 与表达式 url 的值绑定&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt; 动态参数 2.6.0 开始，可以用方括号括起来的 JavaScript 表达式作为一个指令的参数 12345&lt;a v-bind:[attributeName] = &quot;url&quot;&gt;&lt;/a&gt;&lt;!-- 这里的attributeName会被作为一个javascript表达式进行动态赋值，求得的值会作为最终的参数来使用如果VUE实例有一个data. property. attributeName， 其值为href， 那么绑定将等价于v-bind:href---&gt; 绑定处理函数： 1&lt;a v-on:[eventName]=&quot;dosomething&quot;&gt;&lt;/a&gt; 对动态参数的值的约束 动态参数预期会求出一个字符串，异常情况下值为 null。这个特殊的 null 值可以被显性地用于移除绑定。任何其它非字符串类型的值都将会触发一个警告。 对动态参数表达式的约束 动态参数表达式有一些语法约束，因为某些字符，如空格和引号，放在 HTML attribute 名里是无效的。例如： 修饰符 修饰符（modifier）是以半角句号. 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定；例如.prevent修饰符告诉v-on指令对触发的事件调用event.preventDefault(); 123&lt;form v-on:submit.prevent = &quot;onSubmit&quot;&gt; &lt;/form&gt; 缩写 123456789101112131415161718&lt;!-- 完整语法 --&gt;&lt;a v-bind:href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a :href=&quot;url&quot;&gt;...&lt;/a&gt;&lt;!-- 动态参数的缩写 (2.6.0+) --&gt;&lt;a :[key]=&quot;url&quot;&gt; ... &lt;/a&gt;&lt;!-- 完整语法 --&gt;&lt;a v-on:click=&quot;doSomething&quot;&gt;...&lt;/a&gt;&lt;!-- 缩写 --&gt;&lt;a @click=&quot;doSomething&quot;&gt;...&lt;/a&gt;&lt;!-- 动态参数的缩写 (2.6.0+) --&gt;&lt;a @[event]=&quot;doSomething&quot;&gt; ... &lt;/a&gt; : 与 @ 对于 attribute 名来说都是合法字符，在所有支持 Vue 的浏览器都能被正确地解析。而且，它们不会出现在最终渲染的标记中。 8. 计算属性 123&lt;div id=&quot;example&quot;&gt; &#123;&#123; message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125;&#125;&lt;/div&gt; 这里是想要显示变量 message 的翻转字符串。当你想要在模板中的多处包含此翻转字符串时，就会更加难以处理。 所以，对于任何复杂逻辑，你都应当使用计算属性 例如： 1234&lt;div id=&quot;example&quot;&gt; &lt;p&gt;Original message: &quot;&#123;&#123; message &#125;&#125;&quot;&lt;/p&gt; &lt;p&gt;Computed reversed message: &quot;&#123;&#123; reversedMessage &#125;&#125;&quot;&lt;/p&gt;&lt;/div&gt; 1234567891011121314151617var vm = new Vue(&#123; el: &#x27;#example&#x27;, data: &#123; message: &#x27;Hello&#x27; &#125;, computed: &#123; // 计算属性的 getter reversedMessage: function () &#123; // `this` 指向 vm 实例 return this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125; &#125;&#125;)//页面显示：//Original message: &quot;Hello&quot;//Computed reversed message: &quot;olleH&quot; 声明了一个计算属性reversedMessage；我们提供的函数将用作property vm.reversedMessage的getter函数 123console.log(vm.reversedMessage) // olleHvm.message = &#x27;Goodbye&#x27;console.log(vm.reversedMessage) // =&gt; &#x27;eybdooG&#x27; 你可以打开浏览器的控制台，自行修改例子中的 vm。vm.reversedMessage 的值始终取决于 vm.message 的值。 你可以像绑定普通 property 一样在模板中绑定计算属性。Vue 知道 vm.reversedMessage 依赖于 vm.message，因此当 vm.message 发生改变时，所有依赖 vm.reversedMessage 的绑定也会更新。以声明的方式创建了这种依赖关系：计算属性的 getter 函数是没有副作用 (side effect) 的。 计算属性 VS 方法 使用表达式中调用方法同样可以达到上面的结果 1&lt;p&gt;Reversed message: &quot;&#123;&#123; reversedMessage() &#125;&#125;&quot;&lt;/p&gt; 123456// 在组件中methods: &#123; reversedMessage: function () &#123; return this.message.split(&#x27;&#x27;).reverse().join(&#x27;&#x27;) &#125;&#125; 我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是计算属性是基于它们的响应式依赖进行缓存的。只在相关响应式依赖发生改变时它们才会重新求值。这就意味着只要 message 还没有发生改变，多次访问 reversedMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。 这也同样意味着下面的计算属性将不再更新，因为 Date.now() 不是响应式依赖： 12345computed: &#123; now: function () &#123; return Date.now() &#125;&#125; 相比之下，每当触发重新渲染时，调用方法将总会再次执行函数。 我们为什么需要缓存？假设我们有一个性能开销比较大的计算属性 A，它需要遍历一个巨大的数组并做大量的计算。然后我们可能有其他的计算属性依赖于 A。如果没有缓存，我们将不可避免的多次执行 A 的 getter！如果你不希望有缓存，请用方法来替代。 计算属性 VS 侦听属性 侦听属性：vue提供了一种更通用的方式来观察和响应vue实例上的数据变动；当有一些数据需要随着其他数据变动而变动时；很容易滥用watch;通常更好的做法是使用计算属性而不是命令式的watch回调； 1&lt;div id=&quot;demo&quot;&gt;&#123;&#123; fullName &#125;&#125;&lt;/div&gt; 123456789101112131415161718192021222324var vm = new Vue(&#123; el: &#x27;#demo&#x27;, data: &#123; firstName: &#x27;Foo&#x27;, lastName: &#x27;Bar&#x27;, fullName: &#x27;Foo Bar&#x27; &#125;, //侦听属性watch watch: &#123; firstName: function (val) &#123; this.fullName = val + &#x27; &#x27; + this.lastName &#125;, lastName: function (val) &#123; this.fullName = this.firstName + &#x27; &#x27; + val &#125; &#125;, //计算属性 computed: &#123; fullName: function () &#123; return this.firstName + &#x27; &#x27; + this.lastName &#125; &#125;&#125;) 计算属性的setter 计算属性默认只有getter，自己可以提供一个setter 1234567891011121314computed: &#123; fullName: &#123; // getter get: function () &#123; return this.firstName + &#x27; &#x27; + this.lastName &#125;, // setter set: function (newValue) &#123; var names = newValue.split(&#x27; &#x27;) this.firstName = names[0] this.lastName = names[names.length - 1] &#125; &#125;&#125; 现在再运行 vm.fullName = 'John Doe' 时，setter 会被调用，vm.firstName 和 vm.lastName 也会相应地被更新。 9. 侦听器 当需要在数据变化时执行异步或开销较大的操作时，watch是最有用的；同时也可以自定义侦听器； 1234567&lt;div id=&quot;watch-example&quot;&gt; &lt;p&gt; Ask a yes/no question: &lt;input v-model=&quot;question&quot;&gt; &lt;/p&gt; &lt;p&gt;&#123;&#123; answer &#125;&#125;&lt;/p&gt;&lt;/div&gt; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!-- 因为 AJAX 库和通用工具的生态已经相当丰富，Vue 核心代码没有重复 --&gt;&lt;!-- 提供这些功能以保持精简。这也可以让你自由选择自己更熟悉的工具。 --&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdn.jsdelivr.net/npm/lodash@4.13.1/lodash.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt;var watchExampleVM = new Vue(&#123; el: &#x27;#watch-example&#x27;, data: &#123; question: &#x27;&#x27;, answer: &#x27;I cannot give you an answer until you ask a question!&#x27; &#125;, watch: &#123; // 如果 `question` 发生改变，这个函数就会运行 question: function (newQuestion, oldQuestion) &#123; this.answer = &#x27;Waiting for you to stop typing...&#x27; this.debouncedGetAnswer() &#125; &#125;, created: function () &#123; // `_.debounce` 是一个通过 Lodash 限制操作频率的函数。 // 在这个例子中，我们希望限制访问 yesno.wtf/api 的频率 // AJAX 请求直到用户输入完毕才会发出。想要了解更多关于 // `_.debounce` 函数 (及其近亲 `_.throttle`) 的知识， // 请参考：https://lodash.com/docs#debounce this.debouncedGetAnswer = _.debounce(this.getAnswer, 500) &#125;, methods: &#123; getAnswer: function () &#123; if (this.question.indexOf(&#x27;?&#x27;) === -1) &#123; this.answer = &#x27;Questions usually contain a question mark. ;-)&#x27; return &#125; this.answer = &#x27;Thinking...&#x27; var vm = this axios.get(&#x27;https://yesno.wtf/api&#x27;) .then(function (response) &#123; vm.answer = _.capitalize(response.data.answer) &#125;) //异常捕获 .catch(function (error) &#123; vm.answer = &#x27;Error! Could not reach the API. &#x27; + error &#125;) &#125; &#125;&#125;)&lt;/script&gt; 使用 watch 选项允许我们执行异步操作 (访问一个 API)，限制我们执行该操作的频率，并在我们得到最终结果前，设置中间状态。这些都是计算属性无法做到的 10. class与style绑定 将 v-bind 用于 class 和 style 时，Vue.js 做了专门的增强。表达式结果的类型除了字符串之外，还可以是对象或数组 10.1 绑定html class 10.1.1对象语法 方式一：内联 123456789&lt;div class=&quot;static&quot; v-bind:class=&quot;&#123; active: isActive, &#x27;text-danger&#x27;: hasError &#125;&quot;&gt;&lt;/div&gt;//datadata: &#123; isActive: true, hasError: false&#125; 方式二：绑定的数据对象不必内联定义在模板里 12345678&lt;div v-bind:class=&quot;classObject&quot;&gt;&lt;/div&gt;//vue datadata: &#123; classObject: &#123; active: true, &#x27;text-danger&#x27;: false &#125;&#125; 方式三：绑定一个返回对象的计算属性（常用） 123456789101112131415&lt;div v-bind:class=&quot;classObject&quot;&gt;&lt;/div&gt;//vue datadata: &#123; isActive: true, error: null&#125;,computed: &#123; classObject: function () &#123; return &#123; active: this.isActive &amp;&amp; !this.error, &#x27;text-danger&#x27;: this.error &amp;&amp; this.error.type === &#x27;fatal&#x27; &#125; &#125;&#125; 10.1.2 数组语法","categories":[{"name":"12 前端","slug":"12-前端","permalink":"https://xiaoyuge5201.github.io/categories/12-%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"vue","slug":"vue","permalink":"https://xiaoyuge5201.github.io/tags/vue/"}]},{"title":"springCloud之FeignClient访问微服务接口缓慢","slug":"bug-sprigCloud","date":"2021-07-03T09:08:10.000Z","updated":"2021-07-03T09:08:10.000Z","comments":true,"path":"bug-sprigCloud/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/bug-sprigCloud/index.html","excerpt":"","text":"问题描述 逻辑是A服务调用B服务(AB在同一个局域网内)。 经过反复测试，有一个访问缓慢的现象，具体表现为： 程序启动第一次访问初始化1.2秒左右，还可以理解。 但后面访问还是要1.1秒左右（格式化到SSS毫秒打印日志监控的）。 但如果连续访问几次，后面几次又是几十毫秒。过一会再访问，或者换浏览器换post工具请求，又会1.2秒左右。 原因排查1 查看连接查实的接口 发现接口调用的是这个地址，其实是别人启动项目的时候吧自己的ip注册到了eureka注册中心，导致接口有时候走的是getway，有时候又是走的别人的接口 解决方法：eureka.client.register-with-eureka 为false 这样就不会注册到eureka注册中心了 原因排查2 查看日志 查看是否是hystrix 配置的时间小于了ribbon的时间","categories":[{"name":"02 Spring","slug":"02-Spring","permalink":"https://xiaoyuge5201.github.io/categories/02-Spring/"}],"tags":[{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://xiaoyuge5201.github.io/tags/SpringCloud/"}]},{"title":"ConcurrentHashMap线程安全","slug":"ConcurrentHashMap","date":"2021-07-02T08:17:29.000Z","updated":"2021-07-02T08:17:29.000Z","comments":true,"path":"ConcurrentHashMap/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/ConcurrentHashMap/index.html","excerpt":"","text":"##1. jdk1.7 ConcurrentHashMap jdk1.7 ConcurrentHashMap是由一个Segment数组和多个HashEntry数组组成 其实就是将HashMap分为多个小HashMap,每个Segment元素维护一个小HashMap,目的是锁分离，本来实现同步，直接可以是对整个HashMap加锁，但是加锁粒度太大，影响并发性能，所以变换成此结构，仅仅对Segment元素加锁，降低锁粒度，提高并发性能 ###1.1 初始化过程 由于变换成Segment数组+HashEntry数组，所以初始化时，需要依次对Segment数组和HashEntry数组初始化 Segment数组初始化 初始化时，使用右移一位，乘以2的计算方式，保证ssize是2的幂次方，小于指定参数concurrencyLevel的最大2的幂次方 1234567int sshift = 0;//记录Segment数组大小int ssize = 1;while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1;&#125; HashEntry数组初始化 跟Segment数组初始化方式相同 1234int cap = 1;while(cap &lt; c)&#123; cap &lt;&lt;=1;&#125; 1.2 put操作 对于插入操作，需要两次Hash映射去定位数据存储位置 首先通过第一次hash过程，定位Segment位置 然后通过第二次hash过程定位HashEntry位置 Segment继承ReentrantLock,在数据插入指定HashEntry过程的时候会尝试调用ReentrantLock的tryLock方法获取锁，如果获取成功就直接插入相应位置，如果有线程获取该Segment的锁，当前线程就会以自旋方式去继续调用tryLock方法去获取锁，超过指定次数就挂起，等待唤醒。 1.3 get操作 也是两次Hash映射，相对于put操作，少了加锁过程 1.4 size操作 size操作就是计算ConcurrentHashMap的大小，有两种方案 给每个Segment都加上锁(相当于给整个Map加上锁)，然后计算size返回 不加锁的模式，尝试多次计算ConcurrentHashMap的size,最多三次，比较前后计算的结果，结果一致就认为当前没有元素加入，计算结果是准确的。(查看计算出size的前后modCount的数值有没有发生变化，modCount的值用于记录元素变化的操作。如put，remove，clear) 2. jdk1.8 ConcurrentHashMap jdk1.8ConcurrentHashMap是数组+链表，或者数组+红黑树结构,并发控制使用Synchronized关键字和CAS操作 2.1关键概念点 sizeCtl变量(volatile修饰) 通过CAS操作+volatile, 控制数组初始化和扩容操作 -1 代表正在初始化 -N 前16位记录数组容量，后16位记录扩容线程大小+1，是个负数 正数0，表示未初始化 正数，0.75*当前数组大小 &lt;key,value&gt;键值对，封装为Node对象 table变量(volatile)：也就是所说的数组，默认为null，默认大小为16的数组，每次扩容时大小总是2的幂次方 nextTable(volatile):扩容时新生成的数组，大小为table的两倍 2.2put函数 123public V put(K key, V value) &#123; return putVal(key, value, false);&#125; 1.putValue函数 首先调用spread函数，计算hash值，之后进入一个自旋循环过程，直到插入或替换成功，才会返回。如果table未被初始化，则调用initTable进行初始化。之后判断hash映射的位置是否为null,如果为null,直接通过CAS自旋操作，插入元素成功，则直接返回，如果映射的位置值为MOVED(-1),则直接去协助扩容，排除以上条件后，尝试对链头Node节点f加锁，加锁成功后，链表通过尾插遍历，进行插入或替换。红黑树通过查询遍历，进行插入或替换。之后如果当前链表节点数量大于阈值，则调用treeifyBin函数，转换为红黑树最后通过调用addCount,执行CAS操作，更新数组大小，并且判断是否需要进行扩容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); //spread函数计算hash值 int hash = spread(key.hashCode()); int binCount = 0; //自旋过程 for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); //判断映射位置节点是否为空 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; &#125; //如果映射位置节点value==MOVED，说明正在进行扩容操作 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //红黑树结构 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; //链表节点数量超过阈值，转为红黑树 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; addCount(1L, binCount); return null;&#125; 2. spread函数 spread函数，计算hash值。key的hash值与其高16位相异或，然后与HASH_BITS将最高位置0 1234static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS; //HASH_BITS=0x7fffffff&#125; 3. tableAt函数 获取最新的tab[i] 4. casTabAt函数 通过CAS操作，将值赋值进tab中对应位置 12345678static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125;static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v);&#125; 5. addCount函数 尝试使用CAS操作，将BASECOUNT加1，操作失败，则说明有其他线程在进行加一操作,发生冲突。之后判断是否需要扩容 123456789101112131415161718192021222324252627282930313233343536373839404142434445private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; //使用CAS操作，将BASECOUNT加1 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123; CounterCell a; long v; int m; //发生冲突 boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; //多线程冲突执行 fullAddCount(x, uncontended); return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; //判断是否需要扩容 大于0.75当前数组大小 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; //判断是否需要帮助扩容 //扩容完成，或者扩容线程达到阈值不需要进行扩容，直接break if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //帮助扩容，扩容线程数+1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; //进行扩容操作 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 2.3 initTable函数 进入一个自旋过程，一旦有线程扩容成功，才break 如果sizeCtl &lt; 0,说明已经有线程正在扩容，所以直接让出线程。 如果sizeCtl&gt;=0,说明当前没有线程扩容，尝试CAS操作，设置sizeCtl为-1 设置sizeCtl为-1成功的线程，进行扩容操作，并且将sc更新为数组负载阈值0.75*n 123456789101112131415161718192021222324private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; //自旋过程 while ((tab = table) == null || tab.length == 0) &#123; if ((sc = sizeCtl) &lt; 0) Thread.yield(); else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; //0.75*n sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; 2.4 统计ConCurrentHashMap中的元素个数 1. mappingCount函数 12345//调用sumCount,获得元素数量public long mappingCount() &#123;long n = sumCount();return (n &lt; 0L) ? 0L : n; // ignore transient negative values&#125; 2. sumCount函数 baseCount+ counterCells各个元素值，就是元素数量 其实baseCount就是记录容器数量的，直接放回baseCount不就可以了吗？为什么sumCount()方法中还要遍历counterCells数组，累加对象的值呢？ 其中：counterCells是个全局的变量，表示的是CounterCell类数组。CounterCell是ConcurrentHashmap的内部类，它就是存储一个值。 JDK1.8中使用一个volatile类型的变量baseCount记录元素的个数，当插入新数据put()或则删除数据remove()时，会通过addCount()方法更新baseCount 初始化时counterCells为空，在并发量很高时，如果存在两个线程同时执行CAS修改baseCount值，则失败的线程会继续执行方法体中的逻辑，执行fullAddCount(x, uncontended)方法，这个方法其实就是初始化counterCells，并将x的值插入到counterCell类中，而x值一般也就是1或-1，这可以从put()方法中得知。 这些对象是因为在CAS更新baseCount值时，由于高并发而导致失败，最终将值保存到CounterCell中，放到counterCells里。这也就是为什么sumCount()中需要遍历counterCells数组，sum累加CounterCell.value值了。 1234567891011final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value; &#125; &#125; return sum;&#125; 3. CounterCell类 只存储一个值 1234static final class CounterCell&#123; volatile long value; CountCell(long x) &#123;value = x;&#125;&#125; 原文链接：https://blog.csdn.net/zycxnanwang/article/details/105424734","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://xiaoyuge5201.github.io/tags/Java/"}]},{"title":"常见sql优化方式","slug":"sql-01","date":"2021-07-01T08:00:57.000Z","updated":"2021-07-01T08:00:57.000Z","comments":true,"path":"sql-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/sql-01/index.html","excerpt":"","text":"对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 123select id from t where num is null -- 可以在num上设置默认值0，确保表中num列没有null值，然后这样查询： select id from t where num=0 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则将引擎放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： 12345select id from t where num=10 or num=20 --可以这样查询： select id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描，如： 123select id from t where num in(1,2,3) --对于连续的数值，能用 between 就不要用 in 了： select id from t where num between 1 and 3 下面的查询也将导致全表扫描： 1select id from t where name like &#x27;%abc%&#x27; 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： 123select id from t where num/2=100 ---应改为: select id from t where num=100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： 123select id from t where substring(name,1,3)=&#x27;abc&#x27;--name以abc开头的id ---应改为: select id from t where name like &#x27;abc%&#x27; 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 不要写一些没有意义的查询，如需要生成一个空表结构： 123select col1,col2 into #t from t where 1=0 --这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： create table #t(...) 很多时候用 exists 代替 in 是一个好的选择： 123select num from a where num in(select num from b) --用下面的语句替换： select num from a where exists(select 1 from b where num=a.num) 并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。 一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。 这是因为引擎在处理查询和连接时会逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 尽可能的使用 varchar 代替 char ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 避免频繁创建和删除临时表，以减少系统表资源的消耗。 临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先create table，然后insert。 如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。 在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 尽量避免大事务操作，提高系统并发能力。 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。","categories":[{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"Linux安装mysql8","slug":"mysql-install","date":"2021-06-29T06:40:08.000Z","updated":"2021-06-29T06:40:08.000Z","comments":true,"path":"mysql-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/mysql-install/index.html","excerpt":"","text":"1. 下载 下载地址：https://downloads.mysql.com/archives/community/ 百度网盘链接: https://pan.baidu.com/s/1BkOuYlz2Ef7KRe9gikDUCg 密码: 0l15 2.卸载mariadb 12345678#查看mariadb 的安装包rpm -qa | grep mariadb#卸载 mariadbrpm -e mariadb-libs-5.5.68-1.el7.x86_64 --nodeps#卸载验证rpm -qa | grep mariadb 3.安装 解压安装包 12345678910111213141516 # 进入下载目录 cd /usr/local/src/#解压， 如果是.tar则用 tar -zxvf解压， 我下载的是.tar.xz包，使用的是tar -xvJf tar -xvJf mysql-8.0.28-linux-glibc2.17-x86_64-minimal.tar.xz# 移动解压后的文件夹至/usr/local mv /usr/local/src/mysql-8.0.28-linux-glibc2.17-x86_64-minimal /usr/local/ cd /usr/local/ # 重命名 mv ./mysql-8.0.28-linux-glibc2.17-x86_64-minimal mysql8 # 创建文件夹data,存储文件; cd /usr/local/mysql8/ mkdir ./data 创建用户以及用户组 1234# 用户组groupadd mysql# 用户 （用户名/密码）useradd -g mysql mysql 授权 1234chown -R mysql.mysql /usr/local/mysql8/ #或chown -R mysql .chgrp -R mysql . 初始化数据库 12345678910# 查看当前所在目录pwd # 若显示/usr/local/mysql-8.0,请继续执行，否则请先进入此目录/usr/local/mysql-8.0# 初始化 注意查看是否存在相关目录,若不存在,请新建# 亲测./bin/mysqld --user=mysql --basedir=/usr/local/mysql8/ --datadir=/usr/local/mysql8/data/ --initialize #或./bin/mysql --user=mysql --basedir=/usr/local/mysql8/ --datadir=/usr/local/mysql8/data/ --initialize ; #如果出现错误：./bin/mysqld: error while loading shared libraries: libaio.so.1: cannot open shared object file: No such file or directoryyum install -y libaio #安装后在初始化就OK了 注意：后面白色高亮选中的是初始密码！！！！ 配置my.cnf 12cp /usr/local/mysql8/support-files/mysql.server /etc/init.d/mysqldvim /etc/my.cnf 在配置中键入如下内容： 12345678910111213141516171819202122232425262728293031323334353637 [mysqld]port=3306# 设置mysql的安装目录basedir=/usr/local/mysql8# 设置mysql数据库的数据的存放目录datadir=/usr/local/mysql8/data# 允许最大连接数max_connections=1000# 允许连接失败的次数。这是为了防止有人从该主机试图攻击数据库系统max_connect_errors=100# 服务端使用的字符集默认为UTF8character-set-server=utf8mb4# 创建新表时将使用的默认存储引擎default-storage-engine=INNODB# 默认使用“mysql_native_password”插件认证default_authentication_plugin=mysql_native_password#是否对sql语句大小写敏感，1表示不敏感lower_case_table_names = 1#MySQL连接闲置超过一定时间后(单位：秒)将会被强行关闭#MySQL默认的wait_timeout 值为8个小时, interactive_timeout参数需要同时配置才能生效interactive_timeout = 1800wait_timeout = 1800#Metadata Lock最大时长（秒）， 一般用于控制 alter操作的最大时长sine mysql5.6#执行 DML操作时除了增加innodb事务锁外还增加Metadata Lock，其他alter（DDL）session将阻塞lock_wait_timeout = 3600#内部内存临时表的最大值。#比如大数据量的group by ,order by时可能用到临时表，#超过了这个值将写入磁盘，系统IO压力增大tmp_table_size = 64Mmax_heap_table_size = 64M[mysql]# 设置mysql客户端默认字符集default-character-set=utf8mb4[client]# 设置mysql客户端连接服务端时默认使用的端口port=3306default-character-set=utf8mb4 建立Mysql服务 1234cp -a ./support-files/mysql.server /etc/init.d/mysqlchmod +x /etc/init.d/mysqlchkconfig --add mysqlchkconfig --list mysql 启动Mysql服务 1234# 启动service mysql start;# 查看启动状态service mysql status; 如果提示： -bash: mysql: command not found 1ln -s /usr/local/mysql8/bin/mysql /usr/bin 登录Mysql 12mysql -uroot -p# 输入&quot;初始化数据库&quot;操作时的&quot;临时密码&quot; 修改密码： 1ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;新密码&#x27;; 远程连接 进入mysql命令行 1234use mysql;update user set host =&#x27;%&#x27; where user=&#x27;root&#x27;;ALTER USER &#x27;root&#x27;@&#x27;%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;root&#x27;;FLUSH PRIVILEGES; 检查端口 12345678#检查3306端口是否开放netstat -nlp|grep 3306#开放3306端口firewall -cmd --permanent --add-prot=3306/tcp#重启防火墙firewall -cmd --reload 4. 常见问题 The server quit without updating PID file 第一，权限的问题，在出这个错误的时候，我所说的权限是mysq.cnf和所定义的mysql数据库存放目录的权限，要保证是mysql用户的权限，如果启动mysql还有问题，那么需要考虑提高权限了。 说人话， 数据库存放目录必须是mysql这个用户的属组(通常的，安装MySQL的时候建立的用户为mysql，不建议使用别的用户），mysql的启动脚本必须有执行权限。赋予权限的命令为：chown -R mysql. /usr/local/mysql,假设我的mysql是安装在 /usr/local/mysql目录，数据库存放目录为 /usr/local/mysql/data/ 第二，进程中有mysql的进程，上次的退出并没有自动结束该pid，导致新的进程无法启动，毕竟，mysql每次启动系统只会给分配一个pid号，再启动，系统也不可能给你分配pid号了。运行命令 ps -ef |grep mysql 找到mysql的进程结束它，然后在启动mysql。 第三，进入mysql数据库的存放目录，如果有mysql-bin.index这样的文件，删除它，在启动mysql，该文件产生的原因不详，不过删除必定没影响，或者另一个binlog.index也删除，两个index后缀的都删除也可以。 第四，my.cnf 这个配置文件内容不对，检查有没有skip-federated这个字段，如果有注释或者删除。 检查是否定义了数据库存放目录，如果没有定义，请立刻定义。 第五，错误日志目录不存在解决方法：使用“chown” “chmod”命令赋予mysql目录所有者及权限。 第六，my.cnf文件内存在lower_case_table_names=1 字段，注释掉它。（这个选项是1表示不区分大小写）。具体原因不详。 总结：mysql说好安装也好安装，说难也难，难点在于权限的配置，给高了不安全，给低了有可能启动出问题，如果启动出问题了，首先第一件事就是检查目录权限，第二就是检查my.cnf这个配置文件，skip-federated，lower_case_table_names=1 ，这样的字段注释掉，第三，清除旧的mysql进程，如果有就清除掉，旧的不去新的不来，就这么一个道理。第四，进入mysql数据库存放目录删除index后缀的文件。下面，给张图，详细的权限慢慢体会 1ls -al /etc/init.d/mysql Can 't connect to local MySQL server through socket '/tmp/mysql.sock 123连接localhost通常通过一个Unix域套接字文件进行，一般是/tmp/mysql.sock。如果套接字文件被删除了，本地客户就不能连接。这可能发生在你的系统运行一个cron任务删除了/tmp下的临时文件。如果你因为丢失套接字文件而不能连接，你可以简单地通过重启服务器重新创建得到它。因为服务器在启动时重新创建它。如果和我一样，重启服务器还是没有任何变化，你可以先执行下面的语句 1mysql -uroot -h 127.0.0.1 -p 解决方式： 12sudo mkdir /var/run/mysqld/sudo ln -s /tmp/mysql.sock /var/run/mysqld/mysqld.sock however file don’t exists. Create writable for user ‘mysql’ 问题描述： 12[root@test2 my57_3307]# /usr/local/mysql57/bin/mysqld_safe --defaults-file=/dbdata/mysql/my57_3307/my57_3307.cnf --ledir=/usr/local/mysql57/bin2018-08-26T14:12:45.459798Z mysqld_safe error: log-error set to &#x27;/var/log/mysqld/my57_3307.log&#x27;, however file don&#x27;t exists. Create writable for user &#x27;mysql&#x27;. 解决方式： 123touch /usr/local/mysql8/log/error.logchown mysql /usr/local/mysql8/log/error.log/usr/local/mysql8/bin/mysqld_safe --defaults-file=/etc/my.cnf --ledir=/usr/local/mysql8/bin cannot open shared object file: No such file or directory 1234#需要安装 libnumayum install libnumayum -y install numactlyum install libaio1 libaio-dev mysqld启动报错Failed to find valid data directory 123456789vim /etc/my.cnf #查看datadir配置项的路径，然后一般是/var/lib/mysql，将这个文件夹删掉，然后重新初始化/usr/local/mysql8/bin/mysqld --initialize --user=mysql#如果mysqld启动服务时提示不能用root启动，则在/var/lib/mysql中加入这行 user=mysql#重启service mysql restart Navicat无法连接：Host is not allowed to connect to this MySQL server 12mysql -uroot -p#输入密码 操作数据库 12use mysql;select host from user where user=&#x27;root&#x27;; 该结果表示当前的root用户限制在当前的IP内访问，需要修改他的访问域 12update user set host=&#x27;%&#x27; where user=&#x27;root&#x27;;flush privileges ; Can’t connect to MySQL server ‘xxxxxx’ on(60) 我买的是阿里云服务器，前往阿里云服务器配置安全组规则即可","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"}]},{"title":"Linux安装JDK以及配置","slug":"jdk-install","date":"2021-06-29T01:02:34.000Z","updated":"2021-06-29T01:02:34.000Z","comments":true,"path":"jdk-install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/jdk-install/index.html","excerpt":"","text":"1. 安装包安装 下载jdk安装包（https://www.oracle.com/cn/java/technologies/javase/downloads/#java8） 上传并解压 在/usr/local/目录下新建文件夹：java 上传介质到/usr/local/java/ 解压：tar -zxvf jdk-8u311-linux-x64.tar.gz 配置环境变量 1234567891011# 编辑配置文件vim /etc/profile# 在末尾追加export JAVA_HOME=/usr/local/java/jdk1.8.0_311export JRE_HOME=/usr/local/java/jdk1.8.0_311/jreexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATHexport PATH=$JAVA_HOME/bin:$PATH# 使配置文件生效source /etc/profile 测试 12345678910111213# 测试版本号java -version# 返回java version &quot;1.8.0_311&quot;Java(TM) SE Runtime Environment (build 1.8.0_311-b10)Java HotSpot(TM) 64-Bit Server VM (build 25.291-b10, mixed mode)# 查询JAVA_HOMEecho $JAVA_HOME# 返回/usr/local/java/jdk1.8.0_311 2. yum源安装 123456789101112131415161718192021222324252627282930313233# 检查是否已经存在java相关命令rpm -qa|grep javarpm -qa|grep jdkrpm -qa|grep gcj# 如果需要卸载rpm -qa | grep java | xargs rpm -e --nodeps# 检索Java1.8源列表yum list java-1.8*# 安装Java1.8yum install java-1.8.0-openjdk* -y# 查询JAVA_HOMEwhich java# 返回/usr/bin/java# ls -l命令ls -l /usr/bin/java# 返回lrwxrwxrwx 1 root root 22 Nov 13 14:37 /usr/bin/java -&gt; /etc/alternatives/java# ls -l命令ls -l /etc/alternatives/java# 返回lrwxrwxrwx 1 root root 73 Nov 13 14:37 /etc/alternatives/java -&gt; /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64/jre/bin/java# 则JAVA_HOME路径为：/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.272.b10-1.el7_9.x86_64","categories":[{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"}],"tags":[{"name":"jdk","slug":"jdk","permalink":"https://xiaoyuge5201.github.io/tags/jdk/"}]},{"title":"面试常见的趣味题","slug":"interest","date":"2021-06-14T02:09:15.000Z","updated":"2021-06-14T02:09:15.000Z","comments":true,"path":"interest/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/interest/index.html","excerpt":"","text":"8升、5升、3升水桶各一个,如何分成两个4升 以面向对象的思想设计长方形和正方形 方式1：设计接口，然后长方形和正方形各自实现这个接口 12345//形状类：结算面积和周长public interface Shape &#123;truepublic double area();truepublic double perimeter();&#125; 1234567891011121314151617//长方形：实现接口并实现方法public class Rectangle implements Shape &#123;trueprivate double width;trueprivate double height;truepublic Rectangle(double width,double height)&#123;truetruethis.width=width;truetruethis.height=height;true&#125; true@Overridetruepublic double area() &#123;truetruereturn this.width*this.height;true&#125;true@Overridetruepublic double perimeter() &#123;truetruereturn 2*(this.width+this.height);true&#125;&#125; 12345678910111213141516//正方形：实现接口并实现方法public class Square implements Shape &#123;trueprivate double side;truepublic Square(double side)&#123;truetruethis.side=side;true&#125;true@Overridetruepublic double area() &#123;truetruereturn side*side;true&#125; true@Overridetruepublic double perimeter() &#123;truetruereturn 4*side;true&#125;&#125; 方式2：使用extents 因为正方形 is a 长方形，所以可以使用继承来设计正方形，然后在构造函数中使用super函数； 123456789101112public class Square extends Rectangle&#123;trueprivate double side;truepublic Square(double side)&#123;truetruesuper(side,side);truetruethis.side=side;true&#125;truepublic static void main(String[] args) &#123;truetrueSquare s=new Square(2.5);truetrueSystem.out.println(s.perimeter());truetrueSystem.out.println(s.area());true&#125;&#125; java使用递归计算1+2+3+…+n之间的和 1234567891011121314public class SumNumber &#123; public static void main(String[] args) &#123; System.out.println(sumN(10)); &#125; //使用递归的方法计算1+2+3+4+....n的和; 切记注意n不能小于1 public static int sumN(int n) &#123; if (n == 1)&#123; return 1; &#125; return n+ sumN(n-1); &#125;&#125; java读取一篇英文文章，并输出其中出现单词次数最多的3个单词以及次数 文件文章中存在,.以及空格 读取文件内容 对文件进行内容匹配 使用map 保存单词、次数 map排序 输出 1234567891011121314151617181920212223242526272829303132333435363738394041public class WordCount &#123; public static void main(String[] args) &#123; try &#123; //1. 使用流读取文件 BufferedReader reader = new BufferedReader(new FileReader(&quot;d:/n.txt&quot;)); StringBuffer sb = new StringBuffer(); String line; while ((line = reader.readLine()) != null) &#123; sb.append(line); &#125; reader.close(); //替换所有的英文逗号和句号 String temp = sb.toString().replaceAll(&quot;/[\\\\w\\\\,\\\\.]+/&quot;, &quot;&quot;); //2.使用正则表达式匹配 Pattern pattern = Pattern.compile(&quot;/[a-zA-Z\\\\w\\\\,\\\\.]+/&quot;); Matcher matcher = pattern.matcher(temp); Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(16); String word; int count; while (matcher.find()) &#123; word = matcher.group(); if (map.containsKey(word)) &#123; count = map.get(word); map.put(word, count + 1); &#125; else &#123; map.put(word, 1); &#125; &#125; //将map的数据根据count排序； List&lt;Map.Entry&lt;String, Integer&gt;&gt; list = new ArrayList&lt;&gt;(map.entrySet()); Collections.sort(list, Comparator.comparing(Map.Entry::getValue)); int last = list.size() - 1; for (int i = last; i &gt; last - 5; i--) &#123; System.out.println(&quot;key=&quot; + list.get(i).getKey() + &quot; value=&quot; + list.get(i).getValue()); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; java 获取字符串第一次出现重复的字符 12345678910public static int findDuplicate(String str)&#123; char[] chars = str.toCharArray(); Set&lt;Character&gt; uniqueChars = new HashSet(chars.length,1); for (int i = 0; i &lt; chars.length; i++) &#123; if (!uniqueChars.add(chars[i]))&#123; return i; &#125; &#125; return -1;&#125;","categories":[{"name":"10 算法","slug":"10-算法","permalink":"https://xiaoyuge5201.github.io/categories/10-%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://xiaoyuge5201.github.io/tags/algorithm/"}]},{"title":"Java内部类初始化","slug":"java-inner-class-01","date":"2021-05-31T16:00:00.000Z","updated":"2021-05-31T16:00:00.000Z","comments":true,"path":"java-inner-class-01/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/java-inner-class-01/index.html","excerpt":"","text":"1. 在同个java文件中，但不是内部类 1234567public class C &#123;&#125;//在同一个Java文件中只能存在一个public类，除内部类外//只允许使用“public”、“abstract”和“final”。class D&#123; &#125; 1234//实例化public static void main(String[] args) &#123; D d = new D();&#125; 2. 常规内部类 要实例化内部类对象，必须先有外部类对象，通过外部类对象.new 内部类();来实例化内部类对象，在其他文件或者其他包内都是这样，只是要能在其他包实例化的话，内部类Inner还得加上修饰符public。 1234567891011121314151617181920212223242526public class Outter &#123; class Inner &#123; &#125; public static void main(String[] args) &#123; Outter out = new Outter(); Outter.Inner in = out.new Inner(); &#125;&#125;//第二种情况：通过提供方法来获取实例对象public class A &#123; public class B&#123; public void test()&#123; System.out.println(111); &#125; &#125; public B getInstance()&#123; return new B(); &#125; public static void main(String[] args) &#123; A a = new A(); B b = a.getInstance(); b.test(); &#125;&#125; 3. 静态内部类 实例化静态内部类和实例化常规内部类有类似的地方，而不同之处在与静态内部类由于是静态的，所以不需要外部类对象就可以实例化，如上例Outter.Inner in = new Outter.Inner(); 在其他Java文件也是这么实例化的 12345678910class Outter &#123; static class Inner &#123;&#125;&#125;public class TestDemo &#123; public static void main(String[] args) &#123; Outter.Inner in = new Outter.Inner(); &#125;&#125; 4. 局部内部类 局部内部类是定义在一个方法或者一个作用域里面的类，它和成员内部类的区别在于局部内部类的访问仅限于方法内或者该作用域内，所以只能在方法或者该作用域内实例化,局部内部类不能有访问说明符,因为它不是外围类的一部分,但是可以访问当前代码块的常量,以及此外围类的所有成员 12345678910111213141516171819public class A &#123; class B &#123; &#125; public void pint() &#123; class C &#123; &#125; new C(); &#125; public void pint(boolean b) &#123; if (b) &#123; class D &#123; &#125; new D(); &#125; &#125;&#125; 5. 匿名内部类 匿名内部类可以继承一个类或实现一个接口，这里的ClassOrInterfaceName是匿名内部类所继承的类名或实现的接口名。但匿名内部类不能同时实现一个接口和继承一个类，也不能实现多个接口。如果实现了一个接口，该类是Object类的直接子类，匿名类继承一个类或实现一个接口，不需要extends和implements关键字 1234567891011ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;() &#123;&#123; add(&quot;A&quot;); add(&quot;B&quot;); add(&quot;C&quot;);&#125;&#125;;new Thread( new Runnable() &#123; public void run() &#123; ... &#125; &#125;).start();","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"内部类","slug":"内部类","permalink":"https://xiaoyuge5201.github.io/tags/%E5%86%85%E9%83%A8%E7%B1%BB/"}]},{"title":"Linux环境下安装Redis","slug":"redis_install","date":"2021-05-13T05:40:44.000Z","updated":"2021-05-13T05:40:44.000Z","comments":true,"path":"redis_install/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/redis_install/index.html","excerpt":"","text":"1. 安装gcc 1yum -y install gcc gcc-c++ 2. 下载安装包 1wget http://download.redis.io/redis-stable.tar.gz 3. 解压 1234tar xvzf redis-stable.tar.gz#移动redis目录，一般都会将redis目录放置到 /usr/local/redis目录mv redis-stable /usr/local/redis 4. 编译 1234567891011cd /usr/local/redismake# 如果执行make命令报错：cc 未找到命令，原因是虚拟机系统中缺少gcc，执行下面命令安装gcc：yum -y install gcc automake autoconf libtool make#如果使用make失败，致命错误:jemalloc/jemalloc.h: 没有那个文件或目录，则需要在make指定分配器为libc make MALLOC=libc//make之后如果出现Hint: To run &#x27;make test&#x27; is a good idea ;//运行make test, 会提示需要安装tcl,执行yum install tcl#执行下面命令安装redis，并指定安装目录make install PREFIX=/usr/local/redis 5. 配置密码以及允许外网ip访问 12345678910#在redis.conf中配置requirepass 密码以及port端口号（非必须）requirepass xxxport 6379 #开启redis允许外网ip访问，在 Linux 中安装了redis 服务，当在客户端通过远程连接的方式连接时，报could not connect错误。错误的原因为：redis采用的安全策略，默认会只准许本地访问。#将所有的bing信息全部屏蔽#bind 192.168.1.100 10.0.0.1#配置redis后台启动，如果不配置的话可以使用hohup启动daemonize yes 6. 启动/关闭redis服务 123456cd ./srcnohup ./redis-server ../redis.conf &amp;#关闭kill -9 pidredis-cli shutdown 7. 查看redis进程 1[root@localhost redis]# pstree 8. 查看安装目录 目录介绍： redis-benchmark: 性能测试工具 redis-check-apf: 修复有问题的AOF文件 redis-check-dump: 修复有问题的dump.rdb文件 redis-sentinel: redis集群使用 redis-server: redis 服务器启动命令 redis-cli: 客户端 9. redis相关知识 默认端口：6379 默认16个数据库，类似数组的下标从0开始，初始默认使用0号库 使用select 来切换数据库，如：select 1 切换到第二个数据库 统一密码管理，所有的库密码相同 dbsize: 查看当前数据库key的数量 flushdb: 清空当前库 flushall: 清空全部库 redis是单线程+多路IO复用技术 多路复用是指使用一个线程来检测多个文件描述符（socket）的就绪状态，比如调用select 和 poll 函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞到超时，得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动使用线程池。 串行 VS 多线程+锁(memcached) VS 单线程+多路复用(redis) 与memcache三不同：支持多数据类型，支持持久化，单线程+多路复用 redis6.0中提供了多线程，命令解析和io数据读写这部分采用了多线程，而命令的执行还是采用的是单线程，多个客户端发送来的命令会在同一个线程去执行，相当于排队执行，效率极高。","categories":[{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"}],"tags":[{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"}]},{"title":"在线修改静态文件","slug":"在线修改静态文件","date":"2021-05-13T05:40:44.000Z","updated":"2021-05-13T05:40:44.000Z","comments":true,"path":"在线修改静态文件/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/%E5%9C%A8%E7%BA%BF%E4%BF%AE%E6%94%B9%E9%9D%99%E6%80%81%E6%96%87%E4%BB%B6/index.html","excerpt":"","text":"项目运行时，如果需要修改某个css、js、html等文件的时候，需要自己连接到服务器然后修改，更有甚者需要连接vpn、堡垒机等等，特别烦！！！！于是弄了一个在线修改静态文件的工具，在此记录一下。 1. 引入pom 12345&lt;dependency&gt; &lt;groupId&gt;com.github.xiaoyuge5201&lt;/groupId&gt; &lt;artifactId&gt;static-file-modify-online&lt;/artifactId&gt; &lt;version&gt;1.2.1&lt;/version&gt;&lt;/dependency&gt; 2. 添加前后台代码 前台使用的是thymeleaf，根据自己项目的实际情况修改！ 12链接: https://pan.baidu.com/s/1oW38vpj74yKOOtbu5xGCOQ 密码: tcmg","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]},{"title":"Java线程的生命周期状态","slug":"thread-status","date":"2020-06-02T08:04:02.000Z","updated":"2020-06-02T08:04:02.000Z","comments":true,"path":"thread-status/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/thread-status/index.html","excerpt":"","text":"1. Java线程分类 在Java中线程分别对应不同的状态，从创建线程的 NEW 到销毁时的 TERMINATED状态， 我们从Thread类中的内部枚举类State中可以看到线程的6种状态； 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public enum State &#123; /** * Thread state for a thread which has not yet started. * 尚未启动的线程的线程状态 */ NEW, /** * Thread state for a runnable thread. A thread in the runnable state is executing in the Java virtual machine but it may * be waiting for other resources from the operating system such as processor. * 可运行线程的线程状态。处于可运行状态的线程正在Java虚拟机中执行，但它可能正在等待来自操作系统的其他资源，例如处理器。 */ RUNNABLE, /** * Thread state for a thread blocked waiting for a monitor lock. A thread in the blocked state is waiting for a monitor lock to enter a synchronized block/method or reenter a synchronized block/method after calling Object.wait() . * 等待监视器锁定的被阻止线程的线程状态。处于阻塞状态的线程正在等待监视器锁进入同步块/方法，或在调用对象后重新进入同步块/方法。 */ BLOCKED, /** * Thread state for a waiting thread. A thread is in the waiting state due to calling one of the following methods: * 1.Object.wait() with no timeout * 2.Thread.join() with no timeout * 3.LockSupport.park() * * A thread in the waiting state is waiting for another thread to perform a particular action. * For example, a thread that has called Object.wait() on an object is waiting for another thread to call * Object.notify or Object.notifyAll() on that object. A thread that has called Thread.join() * is waiting for a specified thread to terminate. */ WAITING, /** * Thread state for a waiting thread with a specified waiting time. A thread is in the timed waiting state due to calling one of * the following methods with a specified positive waiting time: * 1.Thread.sleep() * 2.Object.wait() with timeout * 3.Thread.join() with timeout * 4.LockSupport.parkNanos() * 5.LockSupport.parkUntil() */ TIMED_WAITING, /** * Thread state for a terminated thread. The thread has completed execution. */ TERMINATED;&#125; 1.1 状态1： NEW 当线程被创建出来还没有被调用 start()时候的状态 1官方描述：Thread state for a thread which has not yet started. 示例代码： 1234567public class ThreadStateTest &#123; public static void main(String[] args) &#123; Thread thread = new Thread(&quot;thread1&quot;); System.out.println(thread.getState()); &#125;&#125;//输出： NEW 1.2 状态2： RUNNABLE 当线程被调用start()，且处于等待操作系统分配资源（如CPU）、等待IO连接、正在运行状态，即表示Running状态和Ready状态。 注：调用了start()不一定会立即改变状态，还有一些准备工作，这个时候线程状态是不确定的。 123官方描述：Thread state for a runnable thread. A thread in the runnable state is executing in the Java virtual machine but it maybe waiting for other resources from the operating system such as processor. 示例代码： 123456789public class ThreadStateTest &#123; public static void main(String[] args) &#123; Thread thread = new Thread(&quot;thread1&quot;); thread.start(); System.out.println(thread.getState()); &#125;&#125;//输出： RUNNABLE 1.3 状态3： BLOCKED 等待监视器锁而被阻塞的线程的状态。当进入synchronized块/方法 或者在调用wait()被唤醒/超时之后重新进入synchronized块/方法，但是锁被其他线程占有，这个时候被操作系统挂起，状态为阻塞状态 BLOCKED。 阻塞状态的线程，即使调用interrupt()方法也不会改变其状态 1234官方描述：Thread state for a thread blocked waiting for a monitor lock. A thread in the blocked state is waiting for a monitor lock to enter a synchronized block/method or reenter a synchronized block/method after calling Object.wait() .译文：等待监视器锁定的被阻止线程的线程状态。处于阻塞状态的线程正在等待监视器锁进入同步块/方法，或在调用对象后重新进入同步块/方法。 阻塞(BLOCKED)：阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种： 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。 其他阻塞：运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。 示例代码： 123456789101112131415161718192021222324252627282930313233343536public class BlockedState &#123; static final String lock = &quot;锁&quot;; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; synchronized (lock) &#123; //死循环导致thread1一直持有lock对象锁 while (true) ; &#125; &#125; &#125;; thread1.start(); //休眠1秒，让thread1先启动 TimeUnit.SECONDS.sleep(1); Thread thread2 = new Thread(&quot;thread2&quot;) &#123; @Override public void run() &#123; synchronized (lock) &#123; //@1 System.out.println(&quot;thread2&quot;); &#125; &#125; &#125;; thread2.start(); System.out.println(&quot;thread1.state:&quot; + thread1.getState()); System.out.println(&quot;thread2.state:&quot; + thread2.getState()); //while (true) 死循环导致thread1持有lock对象锁一直没有释放，而thread2也想获取lock对象锁，但是锁一直被thread1持有着，导致thread2被阻塞在@1处， //此时thread2就处于BLOCKED状态 &#125;&#125;//输出： thread1.state:RUNNABLE// thread2.state:BLOCKED 查看2个线程的堆栈信息，包括：线程状态、线程目前执行到哪段代码等 jps命令查看需要打印线程栈的进程号 1jps jstack命令生成java虚拟机当前时刻的线程快照 1jstack 46622 #找到BlockedState线程的 输出： 123456789&quot;thread2&quot; #13 prio=5 os_prio=31 tid=0x00007f81c387e800 nid=0xa703 waiting for monitor entry [0x000070000bbbd000] java.lang.Thread.State: BLOCKED (on object monitor) at org.example.thread.BlockedState$2.run(BlockedState.java:26) - waiting to lock &lt;0x0000000715916c40&gt; (a java.lang.String)&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007f81b3d9b800 nid=0x5503 runnable [0x000070000b9b7000] java.lang.Thread.State: RUNNABLE at org.example.thread.BlockedState$1.run(BlockedState.java:14) - locked &lt;0x0000000715916c40&gt; (a java.lang.String) 输出内容释义： thread1：线程名称 #11：当前线程ID，从 main线程开始，JVM 根据线程创建的顺序为线程编号 prio：是 priority 优先级的缩写，表明了当前线程的优先级，取值范围【1～10】，默认为 5，在虚拟机进行线程调度的时候会参考该优先级为线程分配计算资源，数值越小优先级越高，一般不设置直接使用默认的优先级。 os_prio：线程对应系统的优先级 nid： 本地线程编号， NativeID的缩写，对应JVM虚拟机中线程映射在操作系统中的线程编号，可以通过 top 命令查看进程对应的线程情况进行相关映射 1.4 状态4： WAITING 无条件等待，当线程调用wait()/join()/LockSupport.park()不加超时时间的方法之后所处的状态，如果没有被唤醒或等待的线程没有结束，那么将一直等待，当前状态的线程不会被分配CPU资源和持有锁。 1234567891011官方描述：Thread state for a waiting thread. A thread is in the waiting state due to calling one of the following methods:1.Object.wait with no timeout 2.Thread.join with no timeout 3.LockSupport.parkA thread in the waiting state is waiting for another thread to perform a particular action.For example, a thread that has called Object.wait() on an object is waiting for another thread to callObject.notify or Object.notifyAll() on that object. A thread that has called Thread.join()is waiting for a specified thread to terminate. 方式一：Object.wait() 123456789101112131415161718192021public class WaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;)&#123; @Override public void run() &#123; synchronized (WaitingState.class)&#123; try &#123; WaitingState.class.wait(); &#125;catch (InterruptedException e)&#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; thread1.start(); //模拟休眠1秒，让thread1运行到wait方法处 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread.state:&quot;+thread1.getState()); &#125;&#125;//输出： thread1.state:WAITING 打印线程thread1堆栈信息 1234567&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007fc57b877800 nid=0x5503 in Object.wait() [0x0000700011529000] java.lang.Thread.State: WAITING (on object monitor)true at java.lang.Object.wait(Native Method)true - waiting on &lt;0x0000000715916c40&gt; (a java.lang.Class for org.example.thread.WaitingState)true at java.lang.Object.wait(Object.java:502)true at org.example.thread.WaitingState$1.run(WaitingState.java:12)true - locked &lt;0x0000000715916c40&gt; (a java.lang.Class for org.example.thread.WaitingState) 方式二：Thread.join() 12345678910111213public class WaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;)&#123; @Override public void run() &#123; while (true); &#125; &#125;; thread1.start(); //join方法会让当前主线程等待thread1结束 thread1.join(); &#125;&#125; 上面的代码导致主线程处于WAITING状态，下面是主线程堆栈信息，第二行显示主线程处于WAITING状态，第五行表示因为调用了Thread.join导致线程WAITING 12345678&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fea7b01b800 nid=0xe03 in Object.wait() [0x0000700008b43000] java.lang.Thread.State: WAITING (on object monitor)trueat java.lang.Object.wait(Native Method)true - waiting on &lt;0x000000071591c4c8&gt; (a org.example.thread.WaitingState$1)trueat java.lang.Thread.join(Thread.java:1252)true - locked &lt;0x000000071591c4c8&gt; (a org.example.thread.WaitingState$1)trueat java.lang.Thread.join(Thread.java:1326)trueat org.example.thread.WaitingState.main(WaitingState.java:17) 方式三：LockSupport.park() 1234567891011121314public class WaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;)&#123; @Override public void run() &#123; LockSupport.park(); &#125; &#125;; thread1.start(); TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread.state:&quot;+thread1.getState()); &#125;&#125;//输出： thread.state:WAITING 打印线程thread1的堆栈信息如下： 12345&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007f7baf82d000 nid=0x5503 waiting on condition [0x000070000b2b9000] java.lang.Thread.State: WAITING (parking)trueat sun.misc.Unsafe.park(Native Method)trueat java.util.concurrent.locks.LockSupport.park(LockSupport.java:304)trueat org.example.thread.WaitingState$1.run(WaitingState.java:11) 1.5 状态5： TIMED_WAITING 有条件的等待，区别于上面的WAITING(无条件等待)，当线程调用以下方法之后所处的状态，在指定的时间没有被唤醒或者等待线程没有结束，会被系统自动唤醒，正常退出。 sleep(睡眠时间) wait(等待时间) join(等待时间) LockSupport.parkNanos(等待时间) LockSupport.parkUntil(等待时间) TIMED_WAITING： 有期限的等待 12345678官方描述：Thread state for a waiting thread with a specified waiting time. A thread is in the timed waiting state due to calling one of the following methods with a specified positive waiting time:1.Thread.sleep2.Object.wait with timeout3.Thread.join with timeout4.LockSupport.parkNanos5.LockSupport.parkUntil 方式一：Thread.sleep(seconds) 1234567891011121314151617181920public class TimeWaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; try &#123; //休眠500秒 = 500000毫秒 Thread.sleep(500 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;; thread1.start(); //模拟休眠1秒，让thread1运行到sleep方法处 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread1.state:&quot; + thread1.getState()); &#125;&#125;//输出： thread1.state:TIMED_WAITING 打印线程thread1的堆栈信息，可以看出是线程sleep方法(第三行)导致线程等待 1234&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007fddf71ae800 nid=0x5503 waiting on condition [0x000070000b11c000] java.lang.Thread.State: TIMED_WAITING (sleeping)trueat java.lang.Thread.sleep(Native Method)trueat org.example.thread.TimeWaitingState$1.run(TimeWaitingState.java:12) 方式二：Object.wait(seconds) 123456789101112131415161718192021public class TimeWaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; synchronized (TimeWaitingState.class) &#123; try &#123; TimeWaitingState.class.wait(500 * 100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125;; thread1.start(); //模拟休眠1秒，让thread1运行到sleep方法处 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread1.state:&quot; + thread1.getState()); &#125;&#125;//输出； thread1.state:TIMED_WAITING 打印线程 thread1 堆栈信息，从堆栈信息第三行中可以看出是线程 wait 方法导致线程等待的 123456&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007f80a0129800 nid=0x5503 in Object.wait() [0x000070000856b000] java.lang.Thread.State: TIMED_WAITING (on object monitor)trueat java.lang.Object.wait(Native Method)true- waiting on &lt;0x0000000715916d28&gt; (a java.lang.Class for org.example.thread.TimeWaitingState)trueat org.example.thread.TimeWaitingState$1.run(TimeWaitingState.java:12)true- locked &lt;0x0000000715916d28&gt; (a java.lang.Class for org.example.thread.TimeWaitingState) 方式三：Thread.join(seconds) 1234567891011121314public class TimeWaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; while(true); &#125; &#125;; thread1.start(); //Thread.join 会让当前主线程等待thread1结束，需要等待500s thread1.join(500 * 1000); &#125;&#125; Thread.join 会让当前主线程等待thread1结束，所以我们查看主线程堆栈信息: 1234567&quot;main&quot; #1 prio=5 os_prio=31 tid=0x00007fa6d7009000 nid=0xd03 in Object.wait() [0x000070000feb9000] java.lang.Thread.State: TIMED_WAITING (on object monitor)trueat java.lang.Object.wait(Native Method)true- waiting on &lt;0x000000071591c138&gt; (a org.example.thread.TimeWaitingState$1)trueat java.lang.Thread.join(Thread.java:1260)true- locked &lt;0x000000071591c138&gt; (a org.example.thread.TimeWaitingState$1)trueat org.example.thread.TimeWaitingState.main(TimeWaitingState.java:14) 方式四：LockSupport.parkNanos(seconds) 12345678910111213141516public class TimeWaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; //等待500秒 LockSupport.parkNanos(TimeUnit.SECONDS.toNanos(500)); &#125; &#125;; thread1.start(); //模拟休眠1秒，让thread1运行到parkNanos方法处 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread1.state:&quot; + thread1.getState()); &#125;&#125;//输出： thread1.state:TIMED_WAITING 线程 thread1 堆栈信息 12345&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007fb71b07a000 nid=0x5503 waiting on condition [0x0000700001da8000] java.lang.Thread.State: TIMED_WAITING (parking)trueat sun.misc.Unsafe.park(Native Method)trueat java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:338)trueat org.example.thread.TimeWaitingState$1.run(TimeWaitingState.java:12) 方式五：LockSupport.parkUntil(seconds) 12345678910111213141516public class TimeWaitingState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; //等待500秒 LockSupport.parkUntil(System.currentTimeMillis() + TimeUnit.SECONDS.toMillis(500)); &#125; &#125;; thread1.start(); //模拟休眠1秒，让thread1运行到parkNanos方法处 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread1.state:&quot; + thread1.getState()); &#125;&#125;//输出： thread1.state:TIMED_WAITING 线程 thread1 堆栈信息 12345&quot;thread1&quot; #11 prio=5 os_prio=31 tid=0x00007ff41d076000 nid=0x5503 waiting on condition [0x00007000094df000] java.lang.Thread.State: TIMED_WAITING (parking)trueat sun.misc.Unsafe.park(Native Method)trueat java.util.concurrent.locks.LockSupport.parkUntil(LockSupport.java:372)trueat org.example.thread.TimeWaitingState$1.run(TimeWaitingState.java:12) 1.6 状态6： TERMINATED 执行完了 run()方法。其实这只是 Java 语言级别的一种状态，在操作系统内部可能已经注销了相应的线程，或者将它复用给其他需要使用线程的请求，而在 Java 语言级别只是通过 Java 代码看到的线程状态而已 12官方描述：Thread state for a terminated thread. The thread has completed execution. 示例代码： 12345678910111213141516public class TerminatedState &#123; public static void main(String[] args) throws InterruptedException &#123; Thread thread1 = new Thread(&quot;thread1&quot;) &#123; @Override public void run() &#123; System.out.println(Thread.currentThread()); &#125; &#125;; thread1.start(); //休眠1秒，等待thread1执行完毕 TimeUnit.SECONDS.sleep(1); System.out.println(&quot;thread1 state:&quot; + thread1.getState()); &#125;&#125;//输出：Thread[thread1,5,main]// thread1 state:TERMINATED 2.状态转化","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"}]},{"title":"Java 8学习","slug":"java8特性","date":"2020-04-23T06:04:02.000Z","updated":"2020-04-23T06:04:02.000Z","comments":true,"path":"java8特性/index.html","link":"","permalink":"https://xiaoyuge5201.github.io/java8%E7%89%B9%E6%80%A7/index.html","excerpt":"","text":"1. Optional Optional 类主要解决的问题是臭名昭著的空指针异常（NullPointerException)。 本质上，这是一个包含有可选值的包装类，这意味着 Optional 类既可以含有对象也可以为空 1.1. optional构造方式 Optional.of(T) 该方式的入参不能为null，否则会有NPE，在确定入参不为空时使用该方式。 Optional.ofNullable(T) 该方式的入参可以为null，当入参不确定为非null时使用。 Optional.empty() 这种方式是返回一个空Optional，等效Optional.ofNullable(null) 1.2. 如何正确的使用Optional 尽量避免使用的地方 避免使用Optional.isPresent()来检查实例是否存在，因为这种方式和null != obj没有区别，这样用就没什么意义了。 避免使用Optional.get()方式来获取实例对象，因为使用前需要使用Optional.isPresent()来检查实例是否存在，否则会出现NPE问题。 避免使用Optional作为类或者实例的属性，而应该在返回值中用来包装返回实例对象。 避免使用Optional作为方法的参数，原因同3。 正确使用方式 实例对象存在则返回，否则提供默认值或者通过方法来设置返回值，即使用orElse/orElseGet方式： 12345678910111213141516171819202122//存在则返回User king = new User(1, &quot;king&quot;);Optional&lt;User&gt; userOpt = Optional.of(king);User user = userOpt.orElse(null);System.out.println(user.getName());//不存在提供默认值User user2 = null;Optional&lt;User&gt; userOpt2 = Optional.ofNullable(user2);User user3 = userOpt2.orElse(unknown);System.out.println(user3.getName());//通过方法提供值User user4 = userOpt2.orElseGet(() -&gt; new User(0, &quot;DEFAULT&quot;)); System.out.println(user4.getName()) //不建议下面这种使用if(userOpt.isPresent()) &#123; System.out.println(userOpt.get().getName());&#125; else &#123; //。。。&#125; 使用ifPresent()来进行对象操作，存在则操作，否则不操作。 123//实例存在则操作，否则不操作userOpt.ifPresent(u -&gt; System.out.println(u.getName()));userOpt2.ifPresent(u -&gt; System.out.println(u.getName())); 使用map/flatMap来获取关联数据 1234567891011//使用map方法获取关联数据System.out.println(userOpt.map(u -&gt; u.getName()).orElse(&quot;Unknown&quot;));System.out.println(userOpt2.map(u -&gt; u.getName()).orElse(&quot;Default&quot;));//使用flatMap方法获取关联数据List&lt;String&gt; interests = new ArrayList&lt;String&gt;();interests.add(&quot;a&quot;);interests.add(&quot;b&quot;);interests.add(&quot;c&quot;);user.setInterests(interests);List&lt;String&gt; interests2 = Optional.of(user) .flatMap(u -&gt; Optional.ofNullable(u.getInterests())) .orElse(Collections.emptyList());System.out.println(interests2.isEmpty()); 1.3.Optional判断第三方接口 使用java8的optional可以减少很多的NPE，再也不用当心别人的接口返回值问题了，也不用满屏的if（a != null）这种判断，下面是使用过程中遇到的问题以及如何使用Optional解决。 1.3.1. 接口返回参数问题 在微服务中使用feign调用其他接口，总担心别人返回的参数是否符合标准 参数符合标准后，然后再进行数据判断，先判断是否code为200，然后判断数据存不存在，这样冗余的代码就很多 这是我们期望的返回格式 12345&#123;true&quot;code&quot;: &quot;200&quot;,true&quot;msg&quot;: &quot;调用成功!&quot;,true&quot;data&quot;: []&#125; 12345678//模拟接口调用方法Map&lt;String,Object&gt; map = serviceImpl.queryList();//即使map为空也能正常返回，配合map直接映射数据值return Optional.ofNullable(map).map(r-&gt; r.get(&quot;data&quot;)).orElseGet(ArrayList:: new) //JSONObject 判断是否返回成功，如果成功返回200， 不成功返回400 JSONObject jsonObject = service.updateDate();Optional.ofNullable(jsonObject).map(r-&gt;r.getInteger(&quot;code&quot;)).orElse(400) 1.3.2. 避免判断风暴 对象层层嵌套，为了逻辑严谨必须要进行空判断 1234567891011121314151617181920//对于一个对象里面嵌套对象，那么需要层层去判断非空School school = null;if(school != null)&#123; Clazz clazz = school.getClazz(); if(clazz != null)&#123; Student student = clazz.getStudent(); if(student != null)&#123; String name = student.getName(); if(name == null || &quot;&quot;.equals(name))&#123; name = &quot;学生的姓名为空&quot;; &#125; &#125; &#125;&#125;//使用Optional后 String name = Optional.ofNullable(school) .map(School::getClazz) .map(Clazz::getStudent) .map(Student::getName) .orElse(&quot;学生的姓名为空&quot;); 2. Stream 123//找出某一个字段等于某个值的那一条数据JaponicaRiceCheck1 streamCheck = listItemRice.stream().filter(o -&gt; o.getSYS_PARENTID().equals(check.getSYS_ID())).findAny().orElse(null);","categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"}]}],"categories":[{"name":"01 Java","slug":"01-Java","permalink":"https://xiaoyuge5201.github.io/categories/01-Java/"},{"name":"05 分布式","slug":"05-分布式","permalink":"https://xiaoyuge5201.github.io/categories/05-%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"04 Mybatis","slug":"04-Mybatis","permalink":"https://xiaoyuge5201.github.io/categories/04-Mybatis/"},{"name":"13 开源组件","slug":"13-开源组件","permalink":"https://xiaoyuge5201.github.io/categories/13-%E5%BC%80%E6%BA%90%E7%BB%84%E4%BB%B6/"},{"name":"07 Redis","slug":"07-Redis","permalink":"https://xiaoyuge5201.github.io/categories/07-Redis/"},{"name":"09 Nginx","slug":"09-Nginx","permalink":"https://xiaoyuge5201.github.io/categories/09-Nginx/"},{"name":"02 Spring","slug":"02-Spring","permalink":"https://xiaoyuge5201.github.io/categories/02-Spring/"},{"name":"08 Linux","slug":"08-Linux","permalink":"https://xiaoyuge5201.github.io/categories/08-Linux/"},{"name":"06 数据库","slug":"06-数据库","permalink":"https://xiaoyuge5201.github.io/categories/06-%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"12 前端","slug":"12-前端","permalink":"https://xiaoyuge5201.github.io/categories/12-%E5%89%8D%E7%AB%AF/"},{"name":"10 算法","slug":"10-算法","permalink":"https://xiaoyuge5201.github.io/categories/10-%E7%AE%97%E6%B3%95/"},{"name":"11 其他工具","slug":"11-其他工具","permalink":"https://xiaoyuge5201.github.io/categories/11-%E5%85%B6%E4%BB%96%E5%B7%A5%E5%85%B7/"}],"tags":[{"name":"java","slug":"java","permalink":"https://xiaoyuge5201.github.io/tags/java/"},{"name":"分布式","slug":"分布式","permalink":"https://xiaoyuge5201.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"Java机制","slug":"Java机制","permalink":"https://xiaoyuge5201.github.io/tags/Java%E6%9C%BA%E5%88%B6/"},{"name":"redis专题","slug":"redis专题","permalink":"https://xiaoyuge5201.github.io/tags/redis%E4%B8%93%E9%A2%98/"},{"name":"Nginx专题","slug":"Nginx专题","permalink":"https://xiaoyuge5201.github.io/tags/Nginx%E4%B8%93%E9%A2%98/"},{"name":"spring","slug":"spring","permalink":"https://xiaoyuge5201.github.io/tags/spring/"},{"name":"thread","slug":"thread","permalink":"https://xiaoyuge5201.github.io/tags/thread/"},{"name":"工具","slug":"工具","permalink":"https://xiaoyuge5201.github.io/tags/%E5%B7%A5%E5%85%B7/"},{"name":"设计模式","slug":"设计模式","permalink":"https://xiaoyuge5201.github.io/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"interview","slug":"interview","permalink":"https://xiaoyuge5201.github.io/tags/interview/"},{"name":"springboot","slug":"springboot","permalink":"https://xiaoyuge5201.github.io/tags/springboot/"},{"name":"sql","slug":"sql","permalink":"https://xiaoyuge5201.github.io/tags/sql/"},{"name":"接口设计","slug":"接口设计","permalink":"https://xiaoyuge5201.github.io/tags/%E6%8E%A5%E5%8F%A3%E8%AE%BE%E8%AE%A1/"},{"name":"mysql","slug":"mysql","permalink":"https://xiaoyuge5201.github.io/tags/mysql/"},{"name":"tidb","slug":"tidb","permalink":"https://xiaoyuge5201.github.io/tags/tidb/"},{"name":"kafka","slug":"kafka","permalink":"https://xiaoyuge5201.github.io/tags/kafka/"},{"name":"Canal","slug":"Canal","permalink":"https://xiaoyuge5201.github.io/tags/Canal/"},{"name":"rocketmq","slug":"rocketmq","permalink":"https://xiaoyuge5201.github.io/tags/rocketmq/"},{"name":"linux","slug":"linux","permalink":"https://xiaoyuge5201.github.io/tags/linux/"},{"name":"Apache","slug":"Apache","permalink":"https://xiaoyuge5201.github.io/tags/Apache/"},{"name":"rabbitMQ","slug":"rabbitMQ","permalink":"https://xiaoyuge5201.github.io/tags/rabbitMQ/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"https://xiaoyuge5201.github.io/tags/Zookeeper/"},{"name":"ip","slug":"ip","permalink":"https://xiaoyuge5201.github.io/tags/ip/"},{"name":"vue","slug":"vue","permalink":"https://xiaoyuge5201.github.io/tags/vue/"},{"name":"maven","slug":"maven","permalink":"https://xiaoyuge5201.github.io/tags/maven/"},{"name":"gitlab","slug":"gitlab","permalink":"https://xiaoyuge5201.github.io/tags/gitlab/"},{"name":"swagger","slug":"swagger","permalink":"https://xiaoyuge5201.github.io/tags/swagger/"},{"name":"消息中间件","slug":"消息中间件","permalink":"https://xiaoyuge5201.github.io/tags/%E6%B6%88%E6%81%AF%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"Linux","slug":"Linux","permalink":"https://xiaoyuge5201.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://xiaoyuge5201.github.io/tags/Docker/"},{"name":"docker","slug":"docker","permalink":"https://xiaoyuge5201.github.io/tags/docker/"},{"name":"algorithm","slug":"algorithm","permalink":"https://xiaoyuge5201.github.io/tags/algorithm/"},{"name":"集合","slug":"集合","permalink":"https://xiaoyuge5201.github.io/tags/%E9%9B%86%E5%90%88/"},{"name":"shutdown","slug":"shutdown","permalink":"https://xiaoyuge5201.github.io/tags/shutdown/"},{"name":"hexo","slug":"hexo","permalink":"https://xiaoyuge5201.github.io/tags/hexo/"},{"name":"守护线程","slug":"守护线程","permalink":"https://xiaoyuge5201.github.io/tags/%E5%AE%88%E6%8A%A4%E7%BA%BF%E7%A8%8B/"},{"name":"内存溢出","slug":"内存溢出","permalink":"https://xiaoyuge5201.github.io/tags/%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA/"},{"name":"ClassLoader","slug":"ClassLoader","permalink":"https://xiaoyuge5201.github.io/tags/ClassLoader/"},{"name":"mycat","slug":"mycat","permalink":"https://xiaoyuge5201.github.io/tags/mycat/"},{"name":"lock","slug":"lock","permalink":"https://xiaoyuge5201.github.io/tags/lock/"},{"name":"ELK","slug":"ELK","permalink":"https://xiaoyuge5201.github.io/tags/ELK/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"https://xiaoyuge5201.github.io/tags/SpringCloud/"},{"name":"Java","slug":"Java","permalink":"https://xiaoyuge5201.github.io/tags/Java/"},{"name":"jdk","slug":"jdk","permalink":"https://xiaoyuge5201.github.io/tags/jdk/"},{"name":"内部类","slug":"内部类","permalink":"https://xiaoyuge5201.github.io/tags/%E5%86%85%E9%83%A8%E7%B1%BB/"}]}